{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c8b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiment_utils import get_all_experiments_info, load_best_model\n",
    "import torch\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from notebooks.mnist_classifier.mnist_tiny_cnn import TinyCNN\n",
    "\n",
    "from mixer.mixer import SetMixer\n",
    "from datasets.mnist import MNISTDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "configs = get_all_experiments_info('outputs/', False)\n",
    "cfg = [c for c in configs if 'mnist_multinomial' in c['name'] \n",
    "                    and c['config']['experiment']['batch_size'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde4c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load + prep dataset\n",
    "def prepare_dataset(dataset_cfg):\n",
    "    # probs = np.column_stack((np.linspace(0, 1, num_probs), 1 - np.linspace(0, 1, num_probs)))\n",
    "    dataset = hydra.utils.instantiate(dataset_cfg)\n",
    "    # dataset.probs = probs\n",
    "    # dataset.data, _, _ = dataset.make_sets()\n",
    "    return dataset\n",
    "\n",
    "# load encoder and move to device\n",
    "def load_model(cfg, path, device):\n",
    "    enc = hydra.utils.instantiate(cfg['encoder'])\n",
    "    gen = hydra.utils.instantiate(cfg['generator'])\n",
    "    state = load_best_model(path)\n",
    "    enc.load_state_dict(state['encoder_state_dict'])\n",
    "    gen.model.load_state_dict(state['generator_state_dict'])\n",
    "    enc.eval()\n",
    "    gen.eval()\n",
    "    enc.to(device)\n",
    "    gen.to(device)\n",
    "    return enc, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33995a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c611517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fbe58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyCNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classy = TinyCNN()\n",
    "classy.load_state_dict(torch.load('notebooks/mnist_classifier/mnist_tinycnn.pth'))\n",
    "classy.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84835938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_grid(dim, points_per_dim):\n",
    "    lin = np.linspace(0, 1, points_per_dim)\n",
    "    grid = np.array(list(product(*([lin] * dim))))\n",
    "    grid = grid[np.isclose(grid.sum(axis=1), 1)]  # keep only rows that sum to 1\n",
    "    return grid\n",
    "\n",
    "points_per_dim = 5\n",
    "k = 3\n",
    "\n",
    "set_size = 100\n",
    "\n",
    "mix_probs_labels = simplex_grid(k, points_per_dim)\n",
    "\n",
    "# fr_dist = multinomial_fr(mix_probs)\n",
    "\n",
    "n_sets = len(mix_probs_labels)\n",
    "\n",
    "mix_probs = torch.tensor(np.repeat(mix_probs_labels, n_sets//k, axis=1))\n",
    "\n",
    "dataset = MNISTDataset(n_classes=k, n_sets=n_sets, set_size=5000)\n",
    "mixer = SetMixer(k=k, mixed_set_size=set_size, n_mixed_sets=n_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b399c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Encoder': ['ConvDistributionEncoder'], 'Generation class error': [np.float64(0.026155555555555557)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:01<00:00, 8.17MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 761kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 4.74MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 4.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Encoder': ['ConvDistributionEncoder', 'MNISTPCAEncoder'], 'Generation class error': [np.float64(0.026155555555555557), np.float64(0.007762222222222223)]}\n",
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n",
      "{'Encoder': ['ConvDistributionEncoder', 'MNISTPCAEncoder', 'WormholeEncoder'], 'Generation class error': [np.float64(0.026155555555555557), np.float64(0.007762222222222223), np.float64(0.002288888888888889)]}\n",
      "{'Encoder': ['ConvDistributionEncoder', 'MNISTPCAEncoder', 'WormholeEncoder', 'KMEEncoder'], 'Generation class error': [np.float64(0.026155555555555557), np.float64(0.007762222222222223), np.float64(0.002288888888888889), np.float64(0.10325555555555559)]}\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    \"Encoder\" : [],\n",
    "    \"Generation class error\" : []\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for c in cfg:\n",
    "\n",
    "\n",
    "        encoder, generator = load_model(c['config'], c['dir'], 'cuda')\n",
    "        \n",
    "        rec = generator.sample(encoder(mixed_sets.reshape(n_sets, set_size, 1, 28, 28)), num_samples=100)\n",
    "\n",
    "        preds = classy(rec.reshape(set_size*n_sets, 1, 28, 28)).argmax(dim=1).reshape(n_sets, set_size)\n",
    "\n",
    "        compositions = torch.stack([(set_preds.bincount(minlength=10+1)) for set_preds in preds])\n",
    "\n",
    "        est = compositions[:, :3].cpu().numpy()/100\n",
    "\n",
    "\n",
    "        error = est - mix_probs_labels\n",
    "\n",
    "\n",
    "        d['Encoder'].append(c['encoder'])\n",
    "        d['Generation class error'].append((error**2).mean())\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f717887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Generation class error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConvDistributionEncoder</td>\n",
       "      <td>0.026156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNISTPCAEncoder</td>\n",
       "      <td>0.007762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WormholeEncoder</td>\n",
       "      <td>0.002289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KMEEncoder</td>\n",
       "      <td>0.103256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Encoder  Generation class error\n",
       "0  ConvDistributionEncoder                0.026156\n",
       "1          MNISTPCAEncoder                0.007762\n",
       "2          WormholeEncoder                0.002289\n",
       "3               KMEEncoder                0.103256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf39da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n"
     ]
    }
   ],
   "source": [
    "from utils.eval_utils import compute_encodings_and_resamples, compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e77301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/3: Encoding original samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding samples:   0%|                                                                                                                                                             | 0/10 [00:00<?, ?it/s]/orcd/data/omarabu/001/gokul/DistributionEmbeddings/utils/eval_utils.py:319: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s[:max_encode_samples], dtype=torch.float32)\n",
      "Encoding samples:   0%|                                                                                                                                                             | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m dl_iter = \u001b[38;5;28miter\u001b[39m(dl)\n\u001b[32m      7\u001b[39m samples = [\u001b[38;5;28mnext\u001b[39m(dl_iter)[\u001b[33m'\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[43mcompute_encodings_and_resamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencode_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_encode_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresample_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_resamples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/data/omarabu/001/gokul/DistributionEmbeddings/utils/eval_utils.py:418\u001b[39m, in \u001b[36mcompute_encodings_and_resamples\u001b[39m\u001b[34m(enc, gen, sample_sets, device, max_encode_samples, num_resamples, encode_batch_size, resample_batch_size, reencode_batch_size)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# Step 1: Encode all sample sets in batches\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 1/3: Encoding original samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m latents = \u001b[43mbatch_encode_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_encode_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencode_batch_size\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# Step 2: Generate samples from all latents in batches\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStep 2/3: Generating samples from latents\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/data/omarabu/001/gokul/DistributionEmbeddings/utils/eval_utils.py:325\u001b[39m, in \u001b[36mbatch_encode_samples\u001b[39m\u001b[34m(enc, sample_sets, device, max_encode_samples, batch_size)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# Encode batch\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     batch_latents = \u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tensor\u001b[49m\u001b[43m)\u001b[49m.float()\n\u001b[32m    326\u001b[39m     batch_latents_cpu = batch_latents.cpu().detach().numpy()\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Store latents\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/gokulg/miniforge3/envs/distemb/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/gokulg/miniforge3/envs/distemb/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/data/omarabu/001/gokul/DistributionEmbeddings/encoder/conv_gnn.py:122\u001b[39m, in \u001b[36mConvDistributionEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m        x: Tensor of shape [batch_size, set_size, channels, height, width]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m \u001b[33;03m        - Updated set representations of shape [batch_size, set_size, out_channels, height, width]\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     batch_size, set_size, channels, height, width = x.shape\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# Reshape for initial processing\u001b[39;00m\n\u001b[32m    125\u001b[39m     x_flat = x.view(batch_size * set_size, channels, height, width)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "enc, gen = load_model(cfg[0]['config'], cfg[0]['dir'], device)\n",
    "dataset = prepare_dataset(cfg[0]['config']['dataset'])\n",
    "mixer = hydra.utils.instantiate(cfg[0]['config']['mixer'])\n",
    "\n",
    "dl = DataLoader(dataset, batch_size=3, shuffle=False, collate_fn=mixer.collate_fn)\n",
    "dl_iter = iter(dl)\n",
    "samples = [next(dl_iter)['samples'] for _ in range(100)]\n",
    "\n",
    "results = compute_encodings_and_resamples(\n",
    "    enc, gen, samples, device, \n",
    "    encode_batch_size=10, max_encode_samples=1_000,\n",
    "    resample_batch_size=10, num_resamples=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d52174a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dl_iter)['samples'].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae24b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
