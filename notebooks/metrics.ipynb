{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[0])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiment_utils import get_all_experiments_info, load_best_model\n",
    "\n",
    "device = 'cuda'\n",
    "configs = get_all_experiments_info('../outputs/', False)\n",
    "cfgs = [\n",
    "    c for c in configs if 'gmm_exp' in c['name'] \n",
    "        and c['config']['experiment']['latent_dim'] == 32\n",
    "        and c['config']['experiment']['hidden_dim'] == 128\n",
    "        and c['config']['dataset']['prior_mu'] == [0, 5]\n",
    "        and hasattr(c['config']['encoder'], 'layers')\n",
    "        and c['config']['encoder']['layers'] == 4 \n",
    "]   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = [cfgs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "# load + prep dataset\n",
    "def prepare_dataset_and_mixer(cfg, set_size=None, n_sets=None, n_mixed_sets=None):\n",
    "    # probs = np.column_stack((np.linspace(0, 1, num_probs), 1 - np.linspace(0, 1, num_probs)))\n",
    "    if set_size is not None:\n",
    "        cfg['dataset']['set_size'] = set_size\n",
    "    if n_sets is not None:\n",
    "        cfg['dataset']['n_sets'] = n_sets\n",
    "    if n_mixed_sets is not None:\n",
    "        cfg['mixer']['n_mixed_sets'] = n_mixed_sets\n",
    "    dataset = hydra.utils.instantiate(cfg['dataset'])\n",
    "    mixer = hydra.utils.instantiate(cfg['mixer'])\n",
    "    return dataset, mixer\n",
    "\n",
    "\n",
    "# load encoder and move to device\n",
    "def load_model(cfg, path, device):\n",
    "    enc = hydra.utils.instantiate(cfg['encoder'])\n",
    "    gen = hydra.utils.instantiate(cfg['generator'])\n",
    "    state = load_best_model(path)\n",
    "    enc.load_state_dict(state['encoder_state_dict'])\n",
    "    gen.model.load_state_dict(state['generator_state_dict'])\n",
    "    enc.eval()\n",
    "    gen.eval()\n",
    "    enc.to(device)\n",
    "    gen.to(device)\n",
    "    return enc, gen\n",
    "\n",
    "enc, gen = load_model(cfgs[0]['config'], cfgs[0]['dir'], device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "ds, mx = prepare_dataset_and_mixer(cfgs[0]['config'], set_size=10_000, n_sets=1_000, n_mixed_sets=1)\n",
    "dl = DataLoader(ds, batch_size=3, shuffle=False, collate_fn=mx.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_iter = iter(dl)\n",
    "samples = [next(dl_iter)['samples'].squeeze() for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_utils import compute_encodings_and_resamples, compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/3: Encoding original samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding samples:   0%|          | 0/10 [00:00<?, ?it/s]/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/utils/eval_utils.py:317: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s[:max_encode_samples], dtype=torch.float32)\n",
      "Encoding samples: 100%|██████████| 10/10 [00:00<00:00, 69.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/3: Generating samples from latents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 323\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/utils/eval_utils.py:360: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  batch_tensor = torch.tensor(batch_latents, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 770\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  10%|█         | 1/10 [00:00<00:02,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 650\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  20%|██        | 2/10 [00:00<00:01,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 139\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  30%|███       | 3/10 [00:00<00:01,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 214\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  40%|████      | 4/10 [00:00<00:01,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 400\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  50%|█████     | 5/10 [00:01<00:01,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 260\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  60%|██████    | 6/10 [00:01<00:00,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 980\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  70%|███████   | 7/10 [00:01<00:00,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 169\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  80%|████████  | 8/10 [00:01<00:00,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 100\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  90%|█████████ | 9/10 [00:02<00:00,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep 690\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples: 100%|██████████| 10/10 [00:02<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3/3: Re-encoding generated samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding samples: 100%|██████████| 10/10 [00:00<00:00, 168.00it/s]\n"
     ]
    }
   ],
   "source": [
    "results = compute_encodings_and_resamples(\n",
    "    enc, gen, samples, device, \n",
    "    encode_batch_size=10, max_encode_samples=10_000,\n",
    "    resample_batch_size=10, num_resamples=10_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing distribution metrics:   0%|          | 0/10 [00:00<?, ?it/s]/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/utils/eval_utils.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(pair[0][:sample_batch_size], dtype=torch.float32)\n",
      "Computing distribution metrics: 100%|██████████| 10/10 [00:00<00:00, 17.42it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics(results, batch_size=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latent_recon_error': {'mean': 0.1312226,\n",
       "  'std': 0.0,\n",
       "  'per_set': array(0.1312226, dtype=float32)},\n",
       " 'mmd': {'mean': 0.00199424147605896,\n",
       "  'std': 0.0019327218960941259,\n",
       "  'per_set': array([0.00182664, 0.00090182, 0.00174415, 0.00074124, 0.00010455,\n",
       "         0.00126559, 0.00069761, 0.00090051, 0.00214827, 0.00569415,\n",
       "         0.00032723, 0.00197875, 0.00132   , 0.00300479, 0.00271773,\n",
       "         0.00184464, 0.00023079, 0.00260997, 0.00185871, 0.00460327,\n",
       "         0.00196469, 0.00211346, 0.00195205, 0.00185609, 0.00122714,\n",
       "         0.00149918, 0.00393069, 0.00024259, 0.00025058, 0.00292897,\n",
       "         0.00032818, 0.00159204, 0.00923777, 0.00028932, 0.00025475,\n",
       "         0.00042617, 0.00199354, 0.00306427, 0.00063765, 0.00033736,\n",
       "         0.00066614, 0.00073111, 0.00038934, 0.00092077, 0.00268739,\n",
       "         0.00092649, 0.0018307 , 0.00049198, 0.00210047, 0.00056505,\n",
       "         0.00739872, 0.00458705, 0.00050592, 0.00153506, 0.00102139,\n",
       "         0.00112307, 0.00204301, 0.00173414, 0.00092602, 0.00302899,\n",
       "         0.00067985, 0.00360781, 0.00099432, 0.00024712, 0.00457394,\n",
       "         0.00192893, 0.00298715, 0.00354207, 0.00383151, 0.00743639,\n",
       "         0.00021678, 0.00305283, 0.00050551, 0.00124383, 0.00552166,\n",
       "         0.00036085, 0.00143123, 0.00035024, 0.00101769, 0.00033104,\n",
       "         0.00091517, 0.00779516, 0.00052524, 0.00251734, 0.00494647,\n",
       "         0.00643027, 0.00065464, 0.0063237 , 0.00027001, 0.00589836,\n",
       "         0.001701  , 0.0007019 , 0.00268507, 0.00065416, 0.00055707,\n",
       "         0.00026941, 0.00224543, 0.00041771, 0.00155282, 0.00064278])},\n",
       " 'sinkhorn': {'mean': 0.00870153546333313,\n",
       "  'std': 0.007738095134557856,\n",
       "  'per_set': array([ 0.01651752,  0.00216413,  0.01174444,  0.00293064,  0.00600845,\n",
       "          0.01580566,  0.00394958,  0.00291252,  0.00578487,  0.00285536,\n",
       "          0.00316292,  0.0096541 ,  0.01212132,  0.00503492,  0.01578367,\n",
       "          0.02038455,  0.00576937,  0.01145637,  0.00065488,  0.01640105,\n",
       "          0.00606483, -0.00616479,  0.01822507,  0.00165713,  0.00351208,\n",
       "          0.00706065,  0.00493699,  0.00525838,  0.00406241,  0.00829273,\n",
       "          0.00627887,  0.0079813 ,  0.02290744,  0.0091809 ,  0.00725466,\n",
       "          0.00274128,  0.00579482,  0.01168472,  0.00303119,  0.00826657,\n",
       "          0.00693274,  0.01385713,  0.00530452,  0.00390673,  0.012613  ,\n",
       "          0.00641632,  0.00562167,  0.00312829,  0.0020498 ,  0.00526971,\n",
       "          0.03560758,  0.00681466,  0.00994849,  0.01361889,  0.0096249 ,\n",
       "         -0.00450724,  0.01368213,  0.01362795,  0.0081017 ,  0.00912356,\n",
       "          0.00417036,  0.00085533,  0.00752842,  0.00363147,  0.04078066,\n",
       "          0.01821226,  0.01788276,  0.00542998,  0.02508396,  0.01533306,\n",
       "          0.00338209,  0.00726193,  0.00313711,  0.00980383,  0.01894176,\n",
       "          0.00018394,  0.00336891,  0.00589812,  0.00197399,  0.00301766,\n",
       "          0.00311315,  0.01968735,  0.00334322,  0.00309074,  0.01145184,\n",
       "          0.01300037,  0.00857711,  0.02983451,  0.00985032,  0.01622868,\n",
       "          0.0008015 ,  0.00161606,  0.02960181,  0.0071435 ,  0.00491637,\n",
       "          0.00281513,  0.00531667,  0.00488043,  0.0064171 ,  0.00272399])},\n",
       " 'sliced_wasserstein': {'mean': 0.1714628029987216,\n",
       "  'std': 0.12433473385686591,\n",
       "  'per_set': array([0.23386885, 0.06695809, 0.09685834, 0.0450587 , 0.06846607,\n",
       "         0.22630323, 0.10143901, 0.04309621, 0.2281747 , 0.39820239,\n",
       "         0.07803501, 0.22247493, 0.1615479 , 0.29994696, 0.13775678,\n",
       "         0.1762621 , 0.05909714, 0.13842078, 0.26361108, 0.20115007,\n",
       "         0.06976824, 0.24000433, 0.15256177, 0.19489174, 0.07888231,\n",
       "         0.25095919, 0.10398734, 0.10927048, 0.06871196, 0.09299322,\n",
       "         0.10069363, 0.27076223, 0.36773646, 0.17503938, 0.12819971,\n",
       "         0.03481161, 0.07033955, 0.3291209 , 0.04223869, 0.07872202,\n",
       "         0.07512978, 0.12740462, 0.12208097, 0.18471928, 0.19375297,\n",
       "         0.07387045, 0.06877001, 0.07954052, 0.27782634, 0.09754279,\n",
       "         0.3315517 , 0.06488992, 0.18767379, 0.12186132, 0.11481702,\n",
       "         0.15168904, 0.13920797, 0.12780869, 0.10939554, 0.3277902 ,\n",
       "         0.06103696, 0.52100855, 0.27011034, 0.09345335, 0.34154329,\n",
       "         0.29696512, 0.16042079, 0.55104625, 0.25790566, 0.14389256,\n",
       "         0.10578937, 0.27169353, 0.07856452, 0.12435998, 0.48259687,\n",
       "         0.09917929, 0.05628239, 0.06243035, 0.03675337, 0.16802397,\n",
       "         0.04237602, 0.66054082, 0.04551915, 0.18661018, 0.31147856,\n",
       "         0.47394097, 0.07647972, 0.25793311, 0.13583587, 0.33366707,\n",
       "         0.17505428, 0.06351264, 0.25277886, 0.11524006, 0.05271545,\n",
       "         0.04111852, 0.314024  , 0.06348473, 0.08511955, 0.09204822])}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-types",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
