{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31c8b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiment_utils import get_all_experiments_info, load_best_model\n",
    "import torch\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from notebooks.mnist_classifier.mnist_tiny_cnn import TinyCNN\n",
    "\n",
    "from mixer.mixer import SetMixer\n",
    "from datasets.distribution_datasets import MultinomialDistributionDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import product\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "import ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf5069b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "configs = get_all_experiments_info('outputs/', False)\n",
    "cfg = [c for c in configs if 'multinomial' in c['name'] and 'dna' not in c['name']\n",
    "                    and 'mnist' not in c['name'] and c['config']['dataset']['n_sets'] == 1000000]\n",
    "print(len(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d79c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_multinom_pairwise(ps):\n",
    "    # build pop cov matrix\n",
    "    def pop_cov(x):\n",
    "        n = len(x)\n",
    "        c = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                c[i, j] = x[i]*(1 - x[i]) if i == j else -x[i]*x[j]\n",
    "        return c\n",
    "\n",
    "    ps = np.array(ps)\n",
    "    n = len(ps)\n",
    "    dists = np.zeros((n, n))\n",
    "\n",
    "    sigs = [pop_cov(p) for p in ps]\n",
    "    roots = [sqrtm(s) for s in sigs]\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            mu_diff = np.sum((ps[i] - ps[j])**2)\n",
    "            mid = sqrtm(roots[i] @ sigs[j] @ roots[i])\n",
    "            if np.iscomplexobj(mid):\n",
    "                mid = mid.real\n",
    "            cov_diff = np.trace(sigs[i] + sigs[j] - 2*mid)\n",
    "            dists[i, j] = np.sqrt(mu_diff + cov_diff)\n",
    "\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde4c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load + prep dataset\n",
    "def prepare_dataset(dataset_cfg):\n",
    "    # probs = np.column_stack((np.linspace(0, 1, num_probs), 1 - np.linspace(0, 1, num_probs)))\n",
    "    dataset = hydra.utils.instantiate(dataset_cfg)\n",
    "    # dataset.probs = probs\n",
    "    # dataset.data, _, _ = dataset.make_sets()\n",
    "    return dataset\n",
    "\n",
    "# load encoder and move to device\n",
    "def load_model(cfg, path, device):\n",
    "    enc = hydra.utils.instantiate(cfg['encoder'])\n",
    "    gen = hydra.utils.instantiate(cfg['generator'])\n",
    "    state = load_best_model(path)\n",
    "    enc.load_state_dict(state['encoder_state_dict'])\n",
    "    gen.model.load_state_dict(state['generator_state_dict'])\n",
    "    enc.eval()\n",
    "    gen.eval()\n",
    "    enc.to(device)\n",
    "    gen.to(device)\n",
    "    return enc, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84835938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplex_grid(dim, points_per_dim):\n",
    "    lin = np.linspace(0, 1, points_per_dim)\n",
    "    grid = np.array(list(product(*([lin] * dim))))\n",
    "    grid = grid[np.isclose(grid.sum(axis=1), 1)]  # keep only rows that sum to 1\n",
    "    return grid\n",
    "\n",
    "points_per_dim = 10\n",
    "k = 3\n",
    "set_size = 1000\n",
    "\n",
    "mix_probs_labels = simplex_grid(k, points_per_dim)\n",
    "n_sets = len(mix_probs_labels)\n",
    "\n",
    "dataset = MultinomialDistributionDataset(data_shape=(k,), n_sets=n_sets, set_size=50000,\n",
    "                                            custom_probs=mix_probs_labels)\n",
    "mixed_sets = torch.tensor(dataset.data).to(torch.float).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b399c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {\n",
    "#     \"Encoder\" : [],\n",
    "#     \"Generation class error\" : []\n",
    "# }\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     for c in cfg:\n",
    "\n",
    "\n",
    "#         encoder, generator = load_model(c['config'], c['dir'], 'cuda')\n",
    "        \n",
    "#         rec = generator.sample(encoder(mixed_sets.reshape(n_sets, set_size, 1, 28, 28)), num_samples=100)\n",
    "\n",
    "#         preds = classy(rec.reshape(set_size*n_sets, 1, 28, 28)).argmax(dim=1).reshape(n_sets, set_size)\n",
    "\n",
    "#         compositions = torch.stack([(set_preds.bincount(minlength=10)) for set_preds in preds])\n",
    "\n",
    "#         est = compositions[:, :3].cpu().numpy()/100\n",
    "\n",
    "\n",
    "#         error = est - mix_probs_labels\n",
    "\n",
    "\n",
    "#         d['Encoder'].append(c['encoder'])\n",
    "#         d['Generation class error'].append((error**2).mean())\n",
    "# print(pd.DataFrame(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70f89ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dists(mixed_sets, mix_probs, encoder, batch_size=8, device='cuda'):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        zs = []\n",
    "        for i in range(0, mixed_sets.size(0), batch_size):\n",
    "            x_chunk = mixed_sets[i:i+batch_size].to(device)\n",
    "            z_chunk = encoder(x_chunk).cpu()  # offload!\n",
    "            zs.append(z_chunk)\n",
    "        z = torch.cat(zs, dim=0).to(device)  # full z back on gpu :)\n",
    "\n",
    "    return w2_multinom_pairwise(mix_probs), torch.cdist(z, z, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff758ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ternary(ax, dists, mix_probs, title=None, scale=99):\n",
    "\n",
    "    prob_tuples = [tuple(map(int, np.round(s * scale))) for s in mix_probs]\n",
    "    anchor_idx = prob_tuples.index((scale//3, scale//3, scale//3))\n",
    "    # anchor_idx = 0\n",
    "    colors = dists[anchor_idx]\n",
    "    simplex_d = {prob_tuples[i]: colors[i] for i in range(len(prob_tuples))}\n",
    "\n",
    "    tax = ternary.TernaryAxesSubplot(ax=ax, scale=scale)\n",
    "    tax.heatmap(simplex_d, style=\"h\")\n",
    "    tax.boundary()\n",
    "    tax.clear_matplotlib_ticks()\n",
    "    tax.get_axes().axis('off')\n",
    "    if title:\n",
    "        tax.set_title(title)\n",
    "\n",
    "    tax.scatter([prob_tuples[anchor_idx]], marker='*', color='pink', s=100, zorder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db25c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_953013/2002123837.py:25: RuntimeWarning: invalid value encountered in sqrt\n",
      "  dists[i, j] = np.sqrt(mu_diff + cov_diff)\n",
      "/orcd/home/002/gokulg/miniforge3/envs/distemb/lib/python3.11/site-packages/ternary/plotting.py:148: UserWarning: No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "  ax.scatter(xs, ys, vmin=vmin, vmax=vmax, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  encoder      corr\n",
      "0  DistributionEncoderGNN  0.928639\n"
     ]
    }
   ],
   "source": [
    "corrs = []  # we'll store correlations here :)\n",
    "\n",
    "for idx in range(len(cfg)):\n",
    "    \n",
    "    name = cfg[idx]['encoder']\n",
    "\n",
    "    encoder, _ = load_model(cfg[idx]['config'], cfg[idx]['dir'], 'cuda')\n",
    "\n",
    "    # get distance !\n",
    "    w, l = batched_dists(mixed_sets, mix_probs_labels, encoder)\n",
    "\n",
    "    # plot it  :)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    fig.suptitle(name)\n",
    "    plot_ternary(axs[0], w, mix_probs_labels, scale=9)\n",
    "    plot_ternary(axs[1], l.cpu(), mix_probs_labels, scale=9)\n",
    "    plt.show()\n",
    "\n",
    "    np.save('numerical_results/plain_multinomial_%s_dists.npy'%name, l.cpu().numpy())\n",
    "\n",
    "    # correlate!\n",
    "    w_flat, l_flat = torch.tensor(w.flatten()), l.flatten().cpu()\n",
    "    mask = ~torch.isnan(w_flat) & ~torch.isnan(l_flat)\n",
    "    if mask.sum() > 1:\n",
    "        corr = torch.corrcoef(torch.stack([w_flat[mask], l_flat[mask]]))[0, 1].item()\n",
    "    else:\n",
    "        corr = float('nan')  # if too few points, set as nan\n",
    "    corrs.append({'encoder': name, 'corr': corr})\n",
    "\n",
    "corr_table = pd.DataFrame(corrs)\n",
    "print(corr_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_table.to_csv('numerical_results/multinomial_correlations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fa87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/orcd/home/002/gokulg/miniforge3/envs/distemb/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "[2025-05-13 10:23:14,614][__main__][INFO] - \n",
      "dataset:\n",
      "  _target_: datasets.distribution_datasets.MultinomialDistributionDataset\n",
      "  n_sets: 1000000\n",
      "  set_size: 100\n",
      "  n_per_multinomial: 20\n",
      "  seed: ${seed}\n",
      "  data_shape:\n",
      "  - 3\n",
      "  spike: 1\n",
      "encoder:\n",
      "  _target_: encoder.encoders.DistributionEncoderGNN\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  set_size: ${experiment.set_size}\n",
      "  layers: 2\n",
      "  fc_layers: 2\n",
      "model:\n",
      "  _target_: layers.MLP\n",
      "  in_dims:\n",
      "  - ${dataset.data_shape[0]}\n",
      "  - ${experiment.latent_dim}\n",
      "  - 1\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  out_dim: ${dataset.data_shape[0]}\n",
      "  layers: 4\n",
      "generator:\n",
      "  _target_: generator.ddpm.DDPM\n",
      "  model: ${model}\n",
      "  betas:\n",
      "  - 0.0001\n",
      "  - 0.02\n",
      "  n_T: 400\n",
      "  drop_prob: 0.1\n",
      "  noise_shape: ${dataset.data_shape}\n",
      "optimizer:\n",
      "  _target_: torch.optim.Adam\n",
      "  _partial_: true\n",
      "  lr: ${experiment.lr}\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 0\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.ConstantLR\n",
      "  _partial_: true\n",
      "training:\n",
      "  _target_: training.Trainer\n",
      "  num_epochs: 100\n",
      "  log_interval: 100\n",
      "  save_interval: 20\n",
      "  eval_interval: 100\n",
      "  early_stopping: false\n",
      "  patience: 5\n",
      "  use_tqdm: false\n",
      "mixer:\n",
      "  _target_: types.NoneType\n",
      "wandb:\n",
      "  _target_: wandb_utils.setup_wandb\n",
      "  project: distribution-embeddings\n",
      "  entity: null\n",
      "  name: null\n",
      "  mode: online\n",
      "  tags: []\n",
      "  notes: null\n",
      "  group: null\n",
      "  save_code: true\n",
      "experiment:\n",
      "  name: multinomial\n",
      "  description: Multinomial simplex\n",
      "  latent_dim: 16\n",
      "  hidden_dim: 64\n",
      "  noise_dim: 16\n",
      "  set_size: 100\n",
      "  batch_size: 256\n",
      "  lr: 0.0002\n",
      "loss:\n",
      "  _target_: loss.default.LossManager\n",
      "  mask_context_prob: 0.0\n",
      "experiment_name: ${experiment.name}\n",
      "seed: 42\n",
      "device: cuda\n",
      "\n",
      "[2025-05-13 10:23:14,618][__main__][INFO] - Configuration hash: 069a4b4f71c36569e9352b49f606eff7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mggowri\u001b[0m (\u001b[33mggowri-harvard-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial/2025-05-13_10-23-14/wandb/run-20250513_102315-xme9sjjf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msplendid-bush-539\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/ggowri-harvard-university/distribution-embeddings\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/ggowri-harvard-university/distribution-embeddings/runs/xme9sjjf\u001b[0m\n",
      "/orcd/home/002/gokulg/miniforge3/envs/distemb/lib/python3.11/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "[2025-05-13 10:23:35,775][training][INFO] - Sub epoch interval: 1000, save interval: 20, eval interval: 100\n",
      "[2025-05-13 10:23:36,095][utils.hash_utils][INFO] - Created output directory: /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7\n",
      "[2025-05-13 10:23:36,098][training][INFO] - Using hash-based output directory: /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7\n",
      "[2025-05-13 10:23:36,098][training][INFO] - No checkpoints found. Looking for similar experiments with different epoch counts...\n",
      "similar_experiments []\n",
      "[2025-05-13 10:23:36,142][training][INFO] - Starting training on cuda...\n",
      "/orcd/home/002/gokulg/miniforge3/envs/distemb/lib/python3.11/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "[2025-05-13 10:23:36,452][training][INFO] - Epoch 1, Batch 0/3907, Loss: 1.013049\n",
      "[2025-05-13 10:23:36,853][training][INFO] - Epoch 1, Batch 100/3907, Loss: 0.908150\n",
      "[2025-05-13 10:23:37,244][training][INFO] - Epoch 1, Batch 200/3907, Loss: 0.822350\n",
      "[2025-05-13 10:23:37,631][training][INFO] - Epoch 1, Batch 300/3907, Loss: 0.749470\n",
      "[2025-05-13 10:23:38,023][training][INFO] - Epoch 1, Batch 400/3907, Loss: 0.726934\n",
      "[2025-05-13 10:23:38,413][training][INFO] - Epoch 1, Batch 500/3907, Loss: 0.679314\n",
      "[2025-05-13 10:23:38,809][training][INFO] - Epoch 1, Batch 600/3907, Loss: 0.647471\n",
      "[2025-05-13 10:23:39,204][training][INFO] - Epoch 1, Batch 700/3907, Loss: 0.601017\n",
      "[2025-05-13 10:23:39,595][training][INFO] - Epoch 1, Batch 800/3907, Loss: 0.555928\n",
      "[2025-05-13 10:23:39,982][training][INFO] - Epoch 1, Batch 900/3907, Loss: 0.525817\n",
      "[2025-05-13 10:23:40,376][training][INFO] - Epoch 1, Batch 1000/3907, Loss: 0.518391\n",
      "[2025-05-13 10:23:40,765][training][INFO] - Epoch 1, Batch 1100/3907, Loss: 0.504620\n",
      "[2025-05-13 10:23:41,151][training][INFO] - Epoch 1, Batch 1200/3907, Loss: 0.502270\n",
      "[2025-05-13 10:23:41,537][training][INFO] - Epoch 1, Batch 1300/3907, Loss: 0.505758\n",
      "[2025-05-13 10:23:41,921][training][INFO] - Epoch 1, Batch 1400/3907, Loss: 0.500051\n",
      "[2025-05-13 10:23:42,315][training][INFO] - Epoch 1, Batch 1500/3907, Loss: 0.497112\n",
      "[2025-05-13 10:23:42,706][training][INFO] - Epoch 1, Batch 1600/3907, Loss: 0.486782\n",
      "[2025-05-13 10:23:43,095][training][INFO] - Epoch 1, Batch 1700/3907, Loss: 0.490653\n",
      "[2025-05-13 10:23:43,489][training][INFO] - Epoch 1, Batch 1800/3907, Loss: 0.485867\n",
      "[2025-05-13 10:23:43,888][training][INFO] - Epoch 1, Batch 1900/3907, Loss: 0.485235\n",
      "[2025-05-13 10:23:44,278][training][INFO] - Epoch 1, Batch 2000/3907, Loss: 0.481594\n",
      "[2025-05-13 10:23:44,665][training][INFO] - Epoch 1, Batch 2100/3907, Loss: 0.475302\n",
      "[2025-05-13 10:23:45,050][training][INFO] - Epoch 1, Batch 2200/3907, Loss: 0.480411\n",
      "[2025-05-13 10:23:45,441][training][INFO] - Epoch 1, Batch 2300/3907, Loss: 0.476988\n",
      "[2025-05-13 10:23:45,835][training][INFO] - Epoch 1, Batch 2400/3907, Loss: 0.471446\n",
      "[2025-05-13 10:23:46,226][training][INFO] - Epoch 1, Batch 2500/3907, Loss: 0.471596\n",
      "[2025-05-13 10:23:46,616][training][INFO] - Epoch 1, Batch 2600/3907, Loss: 0.468746\n",
      "[2025-05-13 10:23:47,002][training][INFO] - Epoch 1, Batch 2700/3907, Loss: 0.472790\n",
      "[2025-05-13 10:23:47,399][training][INFO] - Epoch 1, Batch 2800/3907, Loss: 0.470292\n",
      "[2025-05-13 10:23:47,785][training][INFO] - Epoch 1, Batch 2900/3907, Loss: 0.467417\n",
      "[2025-05-13 10:23:48,173][training][INFO] - Epoch 1, Batch 3000/3907, Loss: 0.464771\n",
      "[2025-05-13 10:23:48,559][training][INFO] - Epoch 1, Batch 3100/3907, Loss: 0.461870\n",
      "[2025-05-13 10:23:48,951][training][INFO] - Epoch 1, Batch 3200/3907, Loss: 0.462494\n",
      "[2025-05-13 10:23:49,340][training][INFO] - Epoch 1, Batch 3300/3907, Loss: 0.459935\n",
      "[2025-05-13 10:23:49,731][training][INFO] - Epoch 1, Batch 3400/3907, Loss: 0.462993\n",
      "[2025-05-13 10:23:50,125][training][INFO] - Epoch 1, Batch 3500/3907, Loss: 0.456295\n",
      "[2025-05-13 10:23:50,518][training][INFO] - Epoch 1, Batch 3600/3907, Loss: 0.453279\n",
      "[2025-05-13 10:23:50,910][training][INFO] - Epoch 1, Batch 3700/3907, Loss: 0.450939\n",
      "[2025-05-13 10:23:51,304][training][INFO] - Epoch 1, Batch 3800/3907, Loss: 0.459012\n",
      "[2025-05-13 10:23:51,718][training][INFO] - Epoch 1, Batch 3900/3907, Loss: 0.461242\n",
      "[2025-05-13 10:23:51,735][training][INFO] - Learning rate: 0.000067\n",
      "[2025-05-13 10:23:51,736][training][INFO] - Epoch 1 complete. Avg Loss: 0.533915\n",
      "[2025-05-13 10:23:51,789][training][INFO] - Epoch 2, Batch 0/3907, Loss: 0.456503\n",
      "[2025-05-13 10:23:52,176][training][INFO] - Epoch 2, Batch 100/3907, Loss: 0.451585\n",
      "[2025-05-13 10:23:52,567][training][INFO] - Epoch 2, Batch 200/3907, Loss: 0.448872\n",
      "[2025-05-13 10:23:52,952][training][INFO] - Epoch 2, Batch 300/3907, Loss: 0.446223\n",
      "[2025-05-13 10:23:53,335][training][INFO] - Epoch 2, Batch 400/3907, Loss: 0.451680\n",
      "[2025-05-13 10:23:53,718][training][INFO] - Epoch 2, Batch 500/3907, Loss: 0.455433\n",
      "[2025-05-13 10:23:54,109][training][INFO] - Epoch 2, Batch 600/3907, Loss: 0.457704\n",
      "[2025-05-13 10:23:54,494][training][INFO] - Epoch 2, Batch 700/3907, Loss: 0.449329\n",
      "[2025-05-13 10:23:54,883][training][INFO] - Epoch 2, Batch 800/3907, Loss: 0.445426\n",
      "[2025-05-13 10:23:55,271][training][INFO] - Epoch 2, Batch 900/3907, Loss: 0.454551\n",
      "[2025-05-13 10:23:55,658][training][INFO] - Epoch 2, Batch 1000/3907, Loss: 0.449219\n",
      "[2025-05-13 10:23:56,047][training][INFO] - Epoch 2, Batch 1100/3907, Loss: 0.454594\n",
      "[2025-05-13 10:23:56,438][training][INFO] - Epoch 2, Batch 1200/3907, Loss: 0.446945\n",
      "[2025-05-13 10:23:56,823][training][INFO] - Epoch 2, Batch 1300/3907, Loss: 0.443090\n",
      "[2025-05-13 10:23:57,211][training][INFO] - Epoch 2, Batch 1400/3907, Loss: 0.447922\n",
      "[2025-05-13 10:23:57,602][training][INFO] - Epoch 2, Batch 1500/3907, Loss: 0.443394\n",
      "[2025-05-13 10:23:57,992][training][INFO] - Epoch 2, Batch 1600/3907, Loss: 0.437305\n",
      "[2025-05-13 10:23:58,380][training][INFO] - Epoch 2, Batch 1700/3907, Loss: 0.441712\n",
      "[2025-05-13 10:23:58,769][training][INFO] - Epoch 2, Batch 1800/3907, Loss: 0.439502\n",
      "[2025-05-13 10:23:59,158][training][INFO] - Epoch 2, Batch 1900/3907, Loss: 0.450749\n",
      "[2025-05-13 10:23:59,551][training][INFO] - Epoch 2, Batch 2000/3907, Loss: 0.437217\n",
      "[2025-05-13 10:23:59,940][training][INFO] - Epoch 2, Batch 2100/3907, Loss: 0.439344\n",
      "[2025-05-13 10:24:00,331][training][INFO] - Epoch 2, Batch 2200/3907, Loss: 0.443141\n",
      "[2025-05-13 10:24:00,719][training][INFO] - Epoch 2, Batch 2300/3907, Loss: 0.428929\n",
      "[2025-05-13 10:24:01,113][training][INFO] - Epoch 2, Batch 2400/3907, Loss: 0.443383\n",
      "[2025-05-13 10:24:01,502][training][INFO] - Epoch 2, Batch 2500/3907, Loss: 0.435529\n",
      "[2025-05-13 10:24:01,892][training][INFO] - Epoch 2, Batch 2600/3907, Loss: 0.437621\n",
      "[2025-05-13 10:24:02,282][training][INFO] - Epoch 2, Batch 2700/3907, Loss: 0.433118\n",
      "[2025-05-13 10:24:02,674][training][INFO] - Epoch 2, Batch 2800/3907, Loss: 0.436736\n",
      "[2025-05-13 10:24:03,062][training][INFO] - Epoch 2, Batch 2900/3907, Loss: 0.433333\n",
      "[2025-05-13 10:24:03,449][training][INFO] - Epoch 2, Batch 3000/3907, Loss: 0.432698\n",
      "[2025-05-13 10:24:03,842][training][INFO] - Epoch 2, Batch 3100/3907, Loss: 0.430795\n",
      "[2025-05-13 10:24:04,241][training][INFO] - Epoch 2, Batch 3200/3907, Loss: 0.430372\n",
      "[2025-05-13 10:24:04,639][training][INFO] - Epoch 2, Batch 3300/3907, Loss: 0.433640\n",
      "[2025-05-13 10:24:05,031][training][INFO] - Epoch 2, Batch 3400/3907, Loss: 0.440512\n",
      "[2025-05-13 10:24:05,426][training][INFO] - Epoch 2, Batch 3500/3907, Loss: 0.430075\n",
      "[2025-05-13 10:24:05,814][training][INFO] - Epoch 2, Batch 3600/3907, Loss: 0.434358\n",
      "[2025-05-13 10:24:06,207][training][INFO] - Epoch 2, Batch 3700/3907, Loss: 0.428913\n",
      "[2025-05-13 10:24:06,594][training][INFO] - Epoch 2, Batch 3800/3907, Loss: 0.432746\n",
      "[2025-05-13 10:24:07,010][training][INFO] - Epoch 2, Batch 3900/3907, Loss: 0.424463\n",
      "[2025-05-13 10:24:07,027][training][INFO] - Learning rate: 0.000067\n",
      "[2025-05-13 10:24:07,028][training][INFO] - Epoch 2 complete. Avg Loss: 0.440871\n",
      "[2025-05-13 10:24:07,076][training][INFO] - Epoch 3, Batch 0/3907, Loss: 0.427398\n",
      "[2025-05-13 10:24:07,467][training][INFO] - Epoch 3, Batch 100/3907, Loss: 0.435073\n",
      "[2025-05-13 10:24:07,860][training][INFO] - Epoch 3, Batch 200/3907, Loss: 0.422903\n",
      "[2025-05-13 10:24:08,247][training][INFO] - Epoch 3, Batch 300/3907, Loss: 0.424783\n",
      "[2025-05-13 10:24:08,635][training][INFO] - Epoch 3, Batch 400/3907, Loss: 0.421889\n",
      "[2025-05-13 10:24:09,022][training][INFO] - Epoch 3, Batch 500/3907, Loss: 0.420787\n",
      "[2025-05-13 10:24:09,412][training][INFO] - Epoch 3, Batch 600/3907, Loss: 0.420813\n",
      "[2025-05-13 10:24:09,801][training][INFO] - Epoch 3, Batch 700/3907, Loss: 0.426777\n",
      "[2025-05-13 10:24:10,190][training][INFO] - Epoch 3, Batch 800/3907, Loss: 0.426714\n",
      "[2025-05-13 10:24:10,583][training][INFO] - Epoch 3, Batch 900/3907, Loss: 0.421640\n",
      "[2025-05-13 10:24:10,972][training][INFO] - Epoch 3, Batch 1000/3907, Loss: 0.421212\n",
      "[2025-05-13 10:24:11,369][training][INFO] - Epoch 3, Batch 1100/3907, Loss: 0.421589\n",
      "[2025-05-13 10:24:11,756][training][INFO] - Epoch 3, Batch 1200/3907, Loss: 0.425806\n",
      "[2025-05-13 10:24:12,143][training][INFO] - Epoch 3, Batch 1300/3907, Loss: 0.421185\n",
      "[2025-05-13 10:24:12,531][training][INFO] - Epoch 3, Batch 1400/3907, Loss: 0.425447\n",
      "[2025-05-13 10:24:12,923][training][INFO] - Epoch 3, Batch 1500/3907, Loss: 0.427561\n",
      "[2025-05-13 10:24:13,314][training][INFO] - Epoch 3, Batch 1600/3907, Loss: 0.428839\n",
      "[2025-05-13 10:24:13,702][training][INFO] - Epoch 3, Batch 1700/3907, Loss: 0.424546\n",
      "[2025-05-13 10:24:14,088][training][INFO] - Epoch 3, Batch 1800/3907, Loss: 0.419594\n",
      "[2025-05-13 10:24:14,481][training][INFO] - Epoch 3, Batch 1900/3907, Loss: 0.416890\n",
      "[2025-05-13 10:24:14,872][training][INFO] - Epoch 3, Batch 2000/3907, Loss: 0.413279\n",
      "[2025-05-13 10:24:15,260][training][INFO] - Epoch 3, Batch 2100/3907, Loss: 0.429447\n",
      "[2025-05-13 10:24:15,652][training][INFO] - Epoch 3, Batch 2200/3907, Loss: 0.427962\n",
      "[2025-05-13 10:24:16,045][training][INFO] - Epoch 3, Batch 2300/3907, Loss: 0.422461\n",
      "[2025-05-13 10:24:16,442][training][INFO] - Epoch 3, Batch 2400/3907, Loss: 0.416771\n",
      "[2025-05-13 10:24:16,828][training][INFO] - Epoch 3, Batch 2500/3907, Loss: 0.419112\n",
      "[2025-05-13 10:24:17,220][training][INFO] - Epoch 3, Batch 2600/3907, Loss: 0.421883\n",
      "[2025-05-13 10:24:17,606][training][INFO] - Epoch 3, Batch 2700/3907, Loss: 0.412280\n",
      "[2025-05-13 10:24:17,999][training][INFO] - Epoch 3, Batch 2800/3907, Loss: 0.414486\n",
      "[2025-05-13 10:24:18,387][training][INFO] - Epoch 3, Batch 2900/3907, Loss: 0.426141\n",
      "[2025-05-13 10:24:18,772][training][INFO] - Epoch 3, Batch 3000/3907, Loss: 0.415744\n",
      "[2025-05-13 10:24:19,165][training][INFO] - Epoch 3, Batch 3100/3907, Loss: 0.423926\n",
      "[2025-05-13 10:24:19,555][training][INFO] - Epoch 3, Batch 3200/3907, Loss: 0.417063\n",
      "[2025-05-13 10:24:19,946][training][INFO] - Epoch 3, Batch 3300/3907, Loss: 0.427827\n",
      "[2025-05-13 10:24:20,335][training][INFO] - Epoch 3, Batch 3400/3907, Loss: 0.418455\n",
      "[2025-05-13 10:24:20,725][training][INFO] - Epoch 3, Batch 3500/3907, Loss: 0.416011\n",
      "[2025-05-13 10:24:21,111][training][INFO] - Epoch 3, Batch 3600/3907, Loss: 0.422029\n",
      "[2025-05-13 10:24:21,508][training][INFO] - Epoch 3, Batch 3700/3907, Loss: 0.420469\n",
      "[2025-05-13 10:24:21,896][training][INFO] - Epoch 3, Batch 3800/3907, Loss: 0.422467\n",
      "[2025-05-13 10:24:22,320][training][INFO] - Epoch 3, Batch 3900/3907, Loss: 0.417470\n",
      "[2025-05-13 10:24:22,336][training][INFO] - Learning rate: 0.000067\n",
      "[2025-05-13 10:24:22,336][training][INFO] - Epoch 3 complete. Avg Loss: 0.422832\n",
      "[2025-05-13 10:24:22,389][training][INFO] - Epoch 4, Batch 0/3907, Loss: 0.408913\n",
      "[2025-05-13 10:24:22,781][training][INFO] - Epoch 4, Batch 100/3907, Loss: 0.419000\n",
      "[2025-05-13 10:24:23,177][training][INFO] - Epoch 4, Batch 200/3907, Loss: 0.417990\n",
      "[2025-05-13 10:24:23,567][training][INFO] - Epoch 4, Batch 300/3907, Loss: 0.415643\n",
      "[2025-05-13 10:24:23,959][training][INFO] - Epoch 4, Batch 400/3907, Loss: 0.414449\n",
      "[2025-05-13 10:24:24,353][training][INFO] - Epoch 4, Batch 500/3907, Loss: 0.415188\n",
      "[2025-05-13 10:24:24,749][training][INFO] - Epoch 4, Batch 600/3907, Loss: 0.414953\n",
      "[2025-05-13 10:24:25,142][training][INFO] - Epoch 4, Batch 700/3907, Loss: 0.410190\n",
      "[2025-05-13 10:24:25,529][training][INFO] - Epoch 4, Batch 800/3907, Loss: 0.420423\n",
      "[2025-05-13 10:24:25,914][training][INFO] - Epoch 4, Batch 900/3907, Loss: 0.425137\n",
      "[2025-05-13 10:24:26,302][training][INFO] - Epoch 4, Batch 1000/3907, Loss: 0.413873\n",
      "[2025-05-13 10:24:26,701][training][INFO] - Epoch 4, Batch 1100/3907, Loss: 0.416254\n",
      "[2025-05-13 10:24:27,091][training][INFO] - Epoch 4, Batch 1200/3907, Loss: 0.417215\n",
      "[2025-05-13 10:24:27,482][training][INFO] - Epoch 4, Batch 1300/3907, Loss: 0.416159\n",
      "[2025-05-13 10:24:27,869][training][INFO] - Epoch 4, Batch 1400/3907, Loss: 0.424822\n",
      "[2025-05-13 10:24:28,258][training][INFO] - Epoch 4, Batch 1500/3907, Loss: 0.414675\n",
      "[2025-05-13 10:24:28,648][training][INFO] - Epoch 4, Batch 1600/3907, Loss: 0.411765\n",
      "[2025-05-13 10:24:29,033][training][INFO] - Epoch 4, Batch 1700/3907, Loss: 0.416391\n",
      "[2025-05-13 10:24:29,423][training][INFO] - Epoch 4, Batch 1800/3907, Loss: 0.416421\n",
      "[2025-05-13 10:24:29,812][training][INFO] - Epoch 4, Batch 1900/3907, Loss: 0.416228\n",
      "[2025-05-13 10:24:30,208][training][INFO] - Epoch 4, Batch 2000/3907, Loss: 0.421389\n",
      "[2025-05-13 10:24:30,603][training][INFO] - Epoch 4, Batch 2100/3907, Loss: 0.418283\n",
      "[2025-05-13 10:24:30,997][training][INFO] - Epoch 4, Batch 2200/3907, Loss: 0.428444\n",
      "[2025-05-13 10:24:31,388][training][INFO] - Epoch 4, Batch 2300/3907, Loss: 0.416087\n",
      "[2025-05-13 10:24:31,785][training][INFO] - Epoch 4, Batch 2400/3907, Loss: 0.414499\n",
      "[2025-05-13 10:24:32,173][training][INFO] - Epoch 4, Batch 2500/3907, Loss: 0.412100\n",
      "[2025-05-13 10:24:32,559][training][INFO] - Epoch 4, Batch 2600/3907, Loss: 0.417198\n",
      "[2025-05-13 10:24:32,946][training][INFO] - Epoch 4, Batch 2700/3907, Loss: 0.406016\n",
      "[2025-05-13 10:24:33,336][training][INFO] - Epoch 4, Batch 2800/3907, Loss: 0.419313\n",
      "[2025-05-13 10:24:33,727][training][INFO] - Epoch 4, Batch 2900/3907, Loss: 0.407801\n",
      "[2025-05-13 10:24:34,113][training][INFO] - Epoch 4, Batch 3000/3907, Loss: 0.413564\n",
      "[2025-05-13 10:24:34,501][training][INFO] - Epoch 4, Batch 3100/3907, Loss: 0.410021\n",
      "[2025-05-13 10:24:34,890][training][INFO] - Epoch 4, Batch 3200/3907, Loss: 0.413813\n",
      "[2025-05-13 10:24:35,284][training][INFO] - Epoch 4, Batch 3300/3907, Loss: 0.416380\n",
      "[2025-05-13 10:24:35,672][training][INFO] - Epoch 4, Batch 3400/3907, Loss: 0.411477\n",
      "[2025-05-13 10:24:36,062][training][INFO] - Epoch 4, Batch 3500/3907, Loss: 0.416962\n",
      "[2025-05-13 10:24:36,449][training][INFO] - Epoch 4, Batch 3600/3907, Loss: 0.407615\n",
      "[2025-05-13 10:24:36,842][training][INFO] - Epoch 4, Batch 3700/3907, Loss: 0.420979\n",
      "[2025-05-13 10:24:37,232][training][INFO] - Epoch 4, Batch 3800/3907, Loss: 0.414254\n",
      "[2025-05-13 10:24:37,646][training][INFO] - Epoch 4, Batch 3900/3907, Loss: 0.414590\n",
      "[2025-05-13 10:24:37,662][training][INFO] - Learning rate: 0.000067\n",
      "[2025-05-13 10:24:37,663][training][INFO] - Epoch 4 complete. Avg Loss: 0.415935\n",
      "[2025-05-13 10:24:37,715][training][INFO] - Epoch 5, Batch 0/3907, Loss: 0.410658\n",
      "[2025-05-13 10:24:38,100][training][INFO] - Epoch 5, Batch 100/3907, Loss: 0.411609\n",
      "[2025-05-13 10:24:38,491][training][INFO] - Epoch 5, Batch 200/3907, Loss: 0.410213\n",
      "[2025-05-13 10:24:38,879][training][INFO] - Epoch 5, Batch 300/3907, Loss: 0.413475\n",
      "[2025-05-13 10:24:39,264][training][INFO] - Epoch 5, Batch 400/3907, Loss: 0.418035\n",
      "[2025-05-13 10:24:39,654][training][INFO] - Epoch 5, Batch 500/3907, Loss: 0.411636\n",
      "[2025-05-13 10:24:40,039][training][INFO] - Epoch 5, Batch 600/3907, Loss: 0.414698\n",
      "[2025-05-13 10:24:40,434][training][INFO] - Epoch 5, Batch 700/3907, Loss: 0.412980\n",
      "[2025-05-13 10:24:40,821][training][INFO] - Epoch 5, Batch 800/3907, Loss: 0.416517\n",
      "[2025-05-13 10:24:41,208][training][INFO] - Epoch 5, Batch 900/3907, Loss: 0.409152\n",
      "[2025-05-13 10:24:41,597][training][INFO] - Epoch 5, Batch 1000/3907, Loss: 0.415915\n",
      "[2025-05-13 10:24:41,987][training][INFO] - Epoch 5, Batch 1100/3907, Loss: 0.418209\n",
      "[2025-05-13 10:24:42,392][training][INFO] - Epoch 5, Batch 1200/3907, Loss: 0.420113\n",
      "[2025-05-13 10:24:42,787][training][INFO] - Epoch 5, Batch 1300/3907, Loss: 0.416888\n",
      "[2025-05-13 10:24:43,221][training][INFO] - Epoch 5, Batch 1400/3907, Loss: 0.418104\n",
      "[2025-05-13 10:24:44,699][training][INFO] - Epoch 5, Batch 1500/3907, Loss: 0.410997\n",
      "[2025-05-13 10:24:45,101][training][INFO] - Epoch 5, Batch 1600/3907, Loss: 0.404131\n",
      "[2025-05-13 10:24:45,506][training][INFO] - Epoch 5, Batch 1700/3907, Loss: 0.417838\n",
      "[2025-05-13 10:24:45,913][training][INFO] - Epoch 5, Batch 1800/3907, Loss: 0.413362\n",
      "[2025-05-13 10:24:46,321][training][INFO] - Epoch 5, Batch 1900/3907, Loss: 0.407630\n",
      "[2025-05-13 10:24:46,710][training][INFO] - Epoch 5, Batch 2000/3907, Loss: 0.410253\n",
      "[2025-05-13 10:24:47,100][training][INFO] - Epoch 5, Batch 2100/3907, Loss: 0.410904\n",
      "[2025-05-13 10:24:47,490][training][INFO] - Epoch 5, Batch 2200/3907, Loss: 0.417686\n",
      "[2025-05-13 10:24:47,878][training][INFO] - Epoch 5, Batch 2300/3907, Loss: 0.416203\n",
      "[2025-05-13 10:24:48,270][training][INFO] - Epoch 5, Batch 2400/3907, Loss: 0.419845\n",
      "[2025-05-13 10:24:48,658][training][INFO] - Epoch 5, Batch 2500/3907, Loss: 0.410874\n",
      "[2025-05-13 10:24:49,044][training][INFO] - Epoch 5, Batch 2600/3907, Loss: 0.414607\n",
      "[2025-05-13 10:24:49,437][training][INFO] - Epoch 5, Batch 2700/3907, Loss: 0.409355\n",
      "[2025-05-13 10:24:49,838][training][INFO] - Epoch 5, Batch 2800/3907, Loss: 0.414507\n",
      "[2025-05-13 10:24:50,237][training][INFO] - Epoch 5, Batch 2900/3907, Loss: 0.415592\n",
      "[2025-05-13 10:24:50,639][training][INFO] - Epoch 5, Batch 3000/3907, Loss: 0.410232\n",
      "[2025-05-13 10:24:51,034][training][INFO] - Epoch 5, Batch 3100/3907, Loss: 0.408833\n",
      "[2025-05-13 10:24:51,436][training][INFO] - Epoch 5, Batch 3200/3907, Loss: 0.412854\n",
      "[2025-05-13 10:24:51,835][training][INFO] - Epoch 5, Batch 3300/3907, Loss: 0.411246\n",
      "[2025-05-13 10:24:52,234][training][INFO] - Epoch 5, Batch 3400/3907, Loss: 0.409629\n",
      "[2025-05-13 10:24:52,642][training][INFO] - Epoch 5, Batch 3500/3907, Loss: 0.410628\n",
      "[2025-05-13 10:24:53,040][training][INFO] - Epoch 5, Batch 3600/3907, Loss: 0.416392\n",
      "[2025-05-13 10:24:53,437][training][INFO] - Epoch 5, Batch 3700/3907, Loss: 0.409822\n",
      "[2025-05-13 10:24:53,830][training][INFO] - Epoch 5, Batch 3800/3907, Loss: 0.411100\n",
      "[2025-05-13 10:24:54,249][training][INFO] - Epoch 5, Batch 3900/3907, Loss: 0.403056\n",
      "[2025-05-13 10:24:54,265][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:24:54,265][training][INFO] - Epoch 5 complete. Avg Loss: 0.412257\n",
      "[2025-05-13 10:24:54,319][training][INFO] - Epoch 6, Batch 0/3907, Loss: 0.403495\n",
      "[2025-05-13 10:24:54,711][training][INFO] - Epoch 6, Batch 100/3907, Loss: 0.489756\n",
      "[2025-05-13 10:24:55,102][training][INFO] - Epoch 6, Batch 200/3907, Loss: 0.443886\n",
      "[2025-05-13 10:24:55,489][training][INFO] - Epoch 6, Batch 300/3907, Loss: 0.427339\n",
      "[2025-05-13 10:24:55,882][training][INFO] - Epoch 6, Batch 400/3907, Loss: 0.415233\n",
      "[2025-05-13 10:24:56,272][training][INFO] - Epoch 6, Batch 500/3907, Loss: 0.412473\n",
      "[2025-05-13 10:24:56,671][training][INFO] - Epoch 6, Batch 600/3907, Loss: 0.412216\n",
      "[2025-05-13 10:24:57,058][training][INFO] - Epoch 6, Batch 700/3907, Loss: 0.419329\n",
      "[2025-05-13 10:24:57,450][training][INFO] - Epoch 6, Batch 800/3907, Loss: 0.409100\n",
      "[2025-05-13 10:24:57,841][training][INFO] - Epoch 6, Batch 900/3907, Loss: 0.417747\n",
      "[2025-05-13 10:24:58,236][training][INFO] - Epoch 6, Batch 1000/3907, Loss: 0.402983\n",
      "[2025-05-13 10:24:58,627][training][INFO] - Epoch 6, Batch 1100/3907, Loss: 0.412642\n",
      "[2025-05-13 10:24:59,014][training][INFO] - Epoch 6, Batch 1200/3907, Loss: 0.403677\n",
      "[2025-05-13 10:24:59,402][training][INFO] - Epoch 6, Batch 1300/3907, Loss: 0.408302\n",
      "[2025-05-13 10:24:59,791][training][INFO] - Epoch 6, Batch 1400/3907, Loss: 0.409397\n",
      "[2025-05-13 10:25:00,188][training][INFO] - Epoch 6, Batch 1500/3907, Loss: 0.411987\n",
      "[2025-05-13 10:25:00,577][training][INFO] - Epoch 6, Batch 1600/3907, Loss: 0.408742\n",
      "[2025-05-13 10:25:00,977][training][INFO] - Epoch 6, Batch 1700/3907, Loss: 0.409364\n",
      "[2025-05-13 10:25:01,370][training][INFO] - Epoch 6, Batch 1800/3907, Loss: 0.419073\n",
      "[2025-05-13 10:25:01,767][training][INFO] - Epoch 6, Batch 1900/3907, Loss: 0.414483\n",
      "[2025-05-13 10:25:02,160][training][INFO] - Epoch 6, Batch 2000/3907, Loss: 0.411063\n",
      "[2025-05-13 10:25:02,549][training][INFO] - Epoch 6, Batch 2100/3907, Loss: 0.413232\n",
      "[2025-05-13 10:25:02,941][training][INFO] - Epoch 6, Batch 2200/3907, Loss: 0.408567\n",
      "[2025-05-13 10:25:03,331][training][INFO] - Epoch 6, Batch 2300/3907, Loss: 0.408016\n",
      "[2025-05-13 10:25:03,720][training][INFO] - Epoch 6, Batch 2400/3907, Loss: 0.406776\n",
      "[2025-05-13 10:25:04,109][training][INFO] - Epoch 6, Batch 2500/3907, Loss: 0.410793\n",
      "[2025-05-13 10:25:04,498][training][INFO] - Epoch 6, Batch 2600/3907, Loss: 0.405876\n",
      "[2025-05-13 10:25:04,885][training][INFO] - Epoch 6, Batch 2700/3907, Loss: 0.411304\n",
      "[2025-05-13 10:25:05,282][training][INFO] - Epoch 6, Batch 2800/3907, Loss: 0.411027\n",
      "[2025-05-13 10:25:05,672][training][INFO] - Epoch 6, Batch 2900/3907, Loss: 0.406586\n",
      "[2025-05-13 10:25:06,064][training][INFO] - Epoch 6, Batch 3000/3907, Loss: 0.404985\n",
      "[2025-05-13 10:25:06,450][training][INFO] - Epoch 6, Batch 3100/3907, Loss: 0.414165\n",
      "[2025-05-13 10:25:06,846][training][INFO] - Epoch 6, Batch 3200/3907, Loss: 0.410521\n",
      "[2025-05-13 10:25:07,233][training][INFO] - Epoch 6, Batch 3300/3907, Loss: 0.411142\n",
      "[2025-05-13 10:25:07,625][training][INFO] - Epoch 6, Batch 3400/3907, Loss: 0.417969\n",
      "[2025-05-13 10:25:08,012][training][INFO] - Epoch 6, Batch 3500/3907, Loss: 0.405448\n",
      "[2025-05-13 10:25:08,399][training][INFO] - Epoch 6, Batch 3600/3907, Loss: 0.410452\n",
      "[2025-05-13 10:25:08,791][training][INFO] - Epoch 6, Batch 3700/3907, Loss: 0.410693\n",
      "[2025-05-13 10:25:09,181][training][INFO] - Epoch 6, Batch 3800/3907, Loss: 0.398783\n",
      "[2025-05-13 10:25:09,595][training][INFO] - Epoch 6, Batch 3900/3907, Loss: 0.405852\n",
      "[2025-05-13 10:25:09,611][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:25:09,611][training][INFO] - Epoch 6 complete. Avg Loss: 0.420801\n",
      "[2025-05-13 10:25:09,665][training][INFO] - Epoch 7, Batch 0/3907, Loss: 0.408802\n",
      "[2025-05-13 10:25:10,053][training][INFO] - Epoch 7, Batch 100/3907, Loss: 0.399480\n",
      "[2025-05-13 10:25:10,448][training][INFO] - Epoch 7, Batch 200/3907, Loss: 0.410184\n",
      "[2025-05-13 10:25:10,840][training][INFO] - Epoch 7, Batch 300/3907, Loss: 0.412514\n",
      "[2025-05-13 10:25:11,226][training][INFO] - Epoch 7, Batch 400/3907, Loss: 0.417253\n",
      "[2025-05-13 10:25:11,615][training][INFO] - Epoch 7, Batch 500/3907, Loss: 0.396327\n",
      "[2025-05-13 10:25:12,009][training][INFO] - Epoch 7, Batch 600/3907, Loss: 0.406762\n",
      "[2025-05-13 10:25:12,397][training][INFO] - Epoch 7, Batch 700/3907, Loss: 0.408684\n",
      "[2025-05-13 10:25:12,784][training][INFO] - Epoch 7, Batch 800/3907, Loss: 0.408809\n",
      "[2025-05-13 10:25:13,173][training][INFO] - Epoch 7, Batch 900/3907, Loss: 0.408427\n",
      "[2025-05-13 10:25:13,564][training][INFO] - Epoch 7, Batch 1000/3907, Loss: 0.406143\n",
      "[2025-05-13 10:25:13,957][training][INFO] - Epoch 7, Batch 1100/3907, Loss: 0.408276\n",
      "[2025-05-13 10:25:14,343][training][INFO] - Epoch 7, Batch 1200/3907, Loss: 0.410739\n",
      "[2025-05-13 10:25:14,725][training][INFO] - Epoch 7, Batch 1300/3907, Loss: 0.410875\n",
      "[2025-05-13 10:25:15,110][training][INFO] - Epoch 7, Batch 1400/3907, Loss: 0.413617\n",
      "[2025-05-13 10:25:15,502][training][INFO] - Epoch 7, Batch 1500/3907, Loss: 0.403311\n",
      "[2025-05-13 10:25:15,895][training][INFO] - Epoch 7, Batch 1600/3907, Loss: 0.418543\n",
      "[2025-05-13 10:25:16,287][training][INFO] - Epoch 7, Batch 1700/3907, Loss: 0.415086\n",
      "[2025-05-13 10:25:16,675][training][INFO] - Epoch 7, Batch 1800/3907, Loss: 0.411107\n",
      "[2025-05-13 10:25:17,068][training][INFO] - Epoch 7, Batch 1900/3907, Loss: 0.405966\n",
      "[2025-05-13 10:25:17,458][training][INFO] - Epoch 7, Batch 2000/3907, Loss: 0.413650\n",
      "[2025-05-13 10:25:17,846][training][INFO] - Epoch 7, Batch 2100/3907, Loss: 0.409294\n",
      "[2025-05-13 10:25:18,233][training][INFO] - Epoch 7, Batch 2200/3907, Loss: 0.403119\n",
      "[2025-05-13 10:25:18,620][training][INFO] - Epoch 7, Batch 2300/3907, Loss: 0.408725\n",
      "[2025-05-13 10:25:19,012][training][INFO] - Epoch 7, Batch 2400/3907, Loss: 0.417613\n",
      "[2025-05-13 10:25:19,399][training][INFO] - Epoch 7, Batch 2500/3907, Loss: 0.404028\n",
      "[2025-05-13 10:25:19,801][training][INFO] - Epoch 7, Batch 2600/3907, Loss: 0.408102\n",
      "[2025-05-13 10:25:20,186][training][INFO] - Epoch 7, Batch 2700/3907, Loss: 0.401241\n",
      "[2025-05-13 10:25:20,581][training][INFO] - Epoch 7, Batch 2800/3907, Loss: 0.404866\n",
      "[2025-05-13 10:25:20,966][training][INFO] - Epoch 7, Batch 2900/3907, Loss: 0.405298\n",
      "[2025-05-13 10:25:21,356][training][INFO] - Epoch 7, Batch 3000/3907, Loss: 0.408990\n",
      "[2025-05-13 10:25:21,745][training][INFO] - Epoch 7, Batch 3100/3907, Loss: 0.411729\n",
      "[2025-05-13 10:25:22,141][training][INFO] - Epoch 7, Batch 3200/3907, Loss: 0.413376\n",
      "[2025-05-13 10:25:22,530][training][INFO] - Epoch 7, Batch 3300/3907, Loss: 0.405321\n",
      "[2025-05-13 10:25:22,917][training][INFO] - Epoch 7, Batch 3400/3907, Loss: 0.403802\n",
      "[2025-05-13 10:25:23,302][training][INFO] - Epoch 7, Batch 3500/3907, Loss: 0.417986\n",
      "[2025-05-13 10:25:23,687][training][INFO] - Epoch 7, Batch 3600/3907, Loss: 0.406183\n",
      "[2025-05-13 10:25:24,082][training][INFO] - Epoch 7, Batch 3700/3907, Loss: 0.411372\n",
      "[2025-05-13 10:25:24,467][training][INFO] - Epoch 7, Batch 3800/3907, Loss: 0.417040\n",
      "[2025-05-13 10:25:24,881][training][INFO] - Epoch 7, Batch 3900/3907, Loss: 0.402218\n",
      "[2025-05-13 10:25:24,898][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:25:24,898][training][INFO] - Epoch 7 complete. Avg Loss: 0.409356\n",
      "[2025-05-13 10:25:24,948][training][INFO] - Epoch 8, Batch 0/3907, Loss: 0.408303\n",
      "[2025-05-13 10:25:25,339][training][INFO] - Epoch 8, Batch 100/3907, Loss: 0.399292\n",
      "[2025-05-13 10:25:25,736][training][INFO] - Epoch 8, Batch 200/3907, Loss: 0.408038\n",
      "[2025-05-13 10:25:26,127][training][INFO] - Epoch 8, Batch 300/3907, Loss: 0.409395\n",
      "[2025-05-13 10:25:26,514][training][INFO] - Epoch 8, Batch 400/3907, Loss: 0.409717\n",
      "[2025-05-13 10:25:26,900][training][INFO] - Epoch 8, Batch 500/3907, Loss: 0.410140\n",
      "[2025-05-13 10:25:27,291][training][INFO] - Epoch 8, Batch 600/3907, Loss: 0.407126\n",
      "[2025-05-13 10:25:27,683][training][INFO] - Epoch 8, Batch 700/3907, Loss: 0.414111\n",
      "[2025-05-13 10:25:28,071][training][INFO] - Epoch 8, Batch 800/3907, Loss: 0.407513\n",
      "[2025-05-13 10:25:28,462][training][INFO] - Epoch 8, Batch 900/3907, Loss: 0.400734\n",
      "[2025-05-13 10:25:28,853][training][INFO] - Epoch 8, Batch 1000/3907, Loss: 0.405655\n",
      "[2025-05-13 10:25:29,249][training][INFO] - Epoch 8, Batch 1100/3907, Loss: 0.412385\n",
      "[2025-05-13 10:25:29,636][training][INFO] - Epoch 8, Batch 1200/3907, Loss: 0.407029\n",
      "[2025-05-13 10:25:30,022][training][INFO] - Epoch 8, Batch 1300/3907, Loss: 0.405855\n",
      "[2025-05-13 10:25:30,414][training][INFO] - Epoch 8, Batch 1400/3907, Loss: 0.406687\n",
      "[2025-05-13 10:25:30,811][training][INFO] - Epoch 8, Batch 1500/3907, Loss: 0.402191\n",
      "[2025-05-13 10:25:31,203][training][INFO] - Epoch 8, Batch 1600/3907, Loss: 0.403158\n",
      "[2025-05-13 10:25:31,599][training][INFO] - Epoch 8, Batch 1700/3907, Loss: 0.401437\n",
      "[2025-05-13 10:25:31,990][training][INFO] - Epoch 8, Batch 1800/3907, Loss: 0.403266\n",
      "[2025-05-13 10:25:32,385][training][INFO] - Epoch 8, Batch 1900/3907, Loss: 0.412261\n",
      "[2025-05-13 10:25:32,773][training][INFO] - Epoch 8, Batch 2000/3907, Loss: 0.410697\n",
      "[2025-05-13 10:25:33,163][training][INFO] - Epoch 8, Batch 2100/3907, Loss: 0.408786\n",
      "[2025-05-13 10:25:33,553][training][INFO] - Epoch 8, Batch 2200/3907, Loss: 0.410075\n",
      "[2025-05-13 10:25:33,946][training][INFO] - Epoch 8, Batch 2300/3907, Loss: 0.398934\n",
      "[2025-05-13 10:25:34,343][training][INFO] - Epoch 8, Batch 2400/3907, Loss: 0.412235\n",
      "[2025-05-13 10:25:34,740][training][INFO] - Epoch 8, Batch 2500/3907, Loss: 0.402751\n",
      "[2025-05-13 10:25:35,128][training][INFO] - Epoch 8, Batch 2600/3907, Loss: 0.406158\n",
      "[2025-05-13 10:25:35,519][training][INFO] - Epoch 8, Batch 2700/3907, Loss: 0.401792\n",
      "[2025-05-13 10:25:35,915][training][INFO] - Epoch 8, Batch 2800/3907, Loss: 0.415186\n",
      "[2025-05-13 10:25:36,305][training][INFO] - Epoch 8, Batch 2900/3907, Loss: 0.406475\n",
      "[2025-05-13 10:25:36,697][training][INFO] - Epoch 8, Batch 3000/3907, Loss: 0.421678\n",
      "[2025-05-13 10:25:37,085][training][INFO] - Epoch 8, Batch 3100/3907, Loss: 0.403899\n",
      "[2025-05-13 10:25:37,478][training][INFO] - Epoch 8, Batch 3200/3907, Loss: 0.401824\n",
      "[2025-05-13 10:25:37,871][training][INFO] - Epoch 8, Batch 3300/3907, Loss: 0.410485\n",
      "[2025-05-13 10:25:38,261][training][INFO] - Epoch 8, Batch 3400/3907, Loss: 0.412886\n",
      "[2025-05-13 10:25:38,648][training][INFO] - Epoch 8, Batch 3500/3907, Loss: 0.410098\n",
      "[2025-05-13 10:25:39,041][training][INFO] - Epoch 8, Batch 3600/3907, Loss: 0.415030\n",
      "[2025-05-13 10:25:39,435][training][INFO] - Epoch 8, Batch 3700/3907, Loss: 0.407662\n",
      "[2025-05-13 10:25:39,821][training][INFO] - Epoch 8, Batch 3800/3907, Loss: 0.409602\n",
      "[2025-05-13 10:25:40,234][training][INFO] - Epoch 8, Batch 3900/3907, Loss: 0.408609\n",
      "[2025-05-13 10:25:40,251][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:25:40,251][training][INFO] - Epoch 8 complete. Avg Loss: 0.407190\n",
      "[2025-05-13 10:25:40,289][training][INFO] - Epoch 9, Batch 0/3907, Loss: 0.407601\n",
      "[2025-05-13 10:25:40,687][training][INFO] - Epoch 9, Batch 100/3907, Loss: 0.408134\n",
      "[2025-05-13 10:25:41,081][training][INFO] - Epoch 9, Batch 200/3907, Loss: 0.408431\n",
      "[2025-05-13 10:25:41,472][training][INFO] - Epoch 9, Batch 300/3907, Loss: 0.410103\n",
      "[2025-05-13 10:25:41,863][training][INFO] - Epoch 9, Batch 400/3907, Loss: 0.405129\n",
      "[2025-05-13 10:25:42,249][training][INFO] - Epoch 9, Batch 500/3907, Loss: 0.404242\n",
      "[2025-05-13 10:25:42,643][training][INFO] - Epoch 9, Batch 600/3907, Loss: 0.403104\n",
      "[2025-05-13 10:25:43,034][training][INFO] - Epoch 9, Batch 700/3907, Loss: 0.400358\n",
      "[2025-05-13 10:25:43,422][training][INFO] - Epoch 9, Batch 800/3907, Loss: 0.405569\n",
      "[2025-05-13 10:25:43,812][training][INFO] - Epoch 9, Batch 900/3907, Loss: 0.406529\n",
      "[2025-05-13 10:25:44,202][training][INFO] - Epoch 9, Batch 1000/3907, Loss: 0.409563\n",
      "[2025-05-13 10:25:44,598][training][INFO] - Epoch 9, Batch 1100/3907, Loss: 0.411833\n",
      "[2025-05-13 10:25:44,988][training][INFO] - Epoch 9, Batch 1200/3907, Loss: 0.401801\n",
      "[2025-05-13 10:25:45,376][training][INFO] - Epoch 9, Batch 1300/3907, Loss: 0.399352\n",
      "[2025-05-13 10:25:45,766][training][INFO] - Epoch 9, Batch 1400/3907, Loss: 0.401335\n",
      "[2025-05-13 10:25:46,164][training][INFO] - Epoch 9, Batch 1500/3907, Loss: 0.418870\n",
      "[2025-05-13 10:25:46,553][training][INFO] - Epoch 9, Batch 1600/3907, Loss: 0.407894\n",
      "[2025-05-13 10:25:46,938][training][INFO] - Epoch 9, Batch 1700/3907, Loss: 0.400207\n",
      "[2025-05-13 10:25:47,326][training][INFO] - Epoch 9, Batch 1800/3907, Loss: 0.405584\n",
      "[2025-05-13 10:25:47,714][training][INFO] - Epoch 9, Batch 1900/3907, Loss: 0.403002\n",
      "[2025-05-13 10:25:48,109][training][INFO] - Epoch 9, Batch 2000/3907, Loss: 0.401640\n",
      "[2025-05-13 10:25:48,496][training][INFO] - Epoch 9, Batch 2100/3907, Loss: 0.405950\n",
      "[2025-05-13 10:25:48,880][training][INFO] - Epoch 9, Batch 2200/3907, Loss: 0.406952\n",
      "[2025-05-13 10:25:49,268][training][INFO] - Epoch 9, Batch 2300/3907, Loss: 0.399460\n",
      "[2025-05-13 10:25:49,668][training][INFO] - Epoch 9, Batch 2400/3907, Loss: 0.406484\n",
      "[2025-05-13 10:25:50,057][training][INFO] - Epoch 9, Batch 2500/3907, Loss: 0.407470\n",
      "[2025-05-13 10:25:50,447][training][INFO] - Epoch 9, Batch 2600/3907, Loss: 0.410922\n",
      "[2025-05-13 10:25:50,849][training][INFO] - Epoch 9, Batch 2700/3907, Loss: 0.407800\n",
      "[2025-05-13 10:25:51,251][training][INFO] - Epoch 9, Batch 2800/3907, Loss: 0.405860\n",
      "[2025-05-13 10:25:51,656][training][INFO] - Epoch 9, Batch 2900/3907, Loss: 0.398862\n",
      "[2025-05-13 10:25:52,056][training][INFO] - Epoch 9, Batch 3000/3907, Loss: 0.408556\n",
      "[2025-05-13 10:25:52,460][training][INFO] - Epoch 9, Batch 3100/3907, Loss: 0.407008\n",
      "[2025-05-13 10:25:52,864][training][INFO] - Epoch 9, Batch 3200/3907, Loss: 0.397372\n",
      "[2025-05-13 10:25:53,267][training][INFO] - Epoch 9, Batch 3300/3907, Loss: 0.408667\n",
      "[2025-05-13 10:25:53,671][training][INFO] - Epoch 9, Batch 3400/3907, Loss: 0.406683\n",
      "[2025-05-13 10:25:54,077][training][INFO] - Epoch 9, Batch 3500/3907, Loss: 0.398428\n",
      "[2025-05-13 10:25:54,477][training][INFO] - Epoch 9, Batch 3600/3907, Loss: 0.403499\n",
      "[2025-05-13 10:25:54,882][training][INFO] - Epoch 9, Batch 3700/3907, Loss: 0.401469\n",
      "[2025-05-13 10:25:55,284][training][INFO] - Epoch 9, Batch 3800/3907, Loss: 0.403739\n",
      "[2025-05-13 10:25:55,715][training][INFO] - Epoch 9, Batch 3900/3907, Loss: 0.407225\n",
      "[2025-05-13 10:25:55,732][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:25:55,732][training][INFO] - Epoch 9 complete. Avg Loss: 0.405465\n",
      "[2025-05-13 10:25:55,778][training][INFO] - Epoch 10, Batch 0/3907, Loss: 0.400466\n",
      "[2025-05-13 10:25:56,194][training][INFO] - Epoch 10, Batch 100/3907, Loss: 0.399998\n",
      "[2025-05-13 10:25:56,602][training][INFO] - Epoch 10, Batch 200/3907, Loss: 0.401315\n",
      "[2025-05-13 10:25:57,009][training][INFO] - Epoch 10, Batch 300/3907, Loss: 0.404828\n",
      "[2025-05-13 10:25:57,412][training][INFO] - Epoch 10, Batch 400/3907, Loss: 0.410695\n",
      "[2025-05-13 10:25:57,815][training][INFO] - Epoch 10, Batch 500/3907, Loss: 0.406718\n",
      "[2025-05-13 10:25:58,220][training][INFO] - Epoch 10, Batch 600/3907, Loss: 0.402396\n",
      "[2025-05-13 10:25:58,622][training][INFO] - Epoch 10, Batch 700/3907, Loss: 0.409843\n",
      "[2025-05-13 10:25:59,023][training][INFO] - Epoch 10, Batch 800/3907, Loss: 0.400244\n",
      "[2025-05-13 10:25:59,426][training][INFO] - Epoch 10, Batch 900/3907, Loss: 0.396511\n",
      "[2025-05-13 10:25:59,833][training][INFO] - Epoch 10, Batch 1000/3907, Loss: 0.409917\n",
      "[2025-05-13 10:26:00,236][training][INFO] - Epoch 10, Batch 1100/3907, Loss: 0.401404\n",
      "[2025-05-13 10:26:00,644][training][INFO] - Epoch 10, Batch 1200/3907, Loss: 0.405629\n",
      "[2025-05-13 10:26:01,048][training][INFO] - Epoch 10, Batch 1300/3907, Loss: 0.400685\n",
      "[2025-05-13 10:26:01,456][training][INFO] - Epoch 10, Batch 1400/3907, Loss: 0.402613\n",
      "[2025-05-13 10:26:01,860][training][INFO] - Epoch 10, Batch 1500/3907, Loss: 0.402194\n",
      "[2025-05-13 10:26:02,262][training][INFO] - Epoch 10, Batch 1600/3907, Loss: 0.403336\n",
      "[2025-05-13 10:26:02,667][training][INFO] - Epoch 10, Batch 1700/3907, Loss: 0.403291\n",
      "[2025-05-13 10:26:03,072][training][INFO] - Epoch 10, Batch 1800/3907, Loss: 0.401120\n",
      "[2025-05-13 10:26:03,473][training][INFO] - Epoch 10, Batch 1900/3907, Loss: 0.421204\n",
      "[2025-05-13 10:26:03,874][training][INFO] - Epoch 10, Batch 2000/3907, Loss: 0.407867\n",
      "[2025-05-13 10:26:04,274][training][INFO] - Epoch 10, Batch 2100/3907, Loss: 0.402021\n",
      "[2025-05-13 10:26:04,673][training][INFO] - Epoch 10, Batch 2200/3907, Loss: 0.397981\n",
      "[2025-05-13 10:26:05,076][training][INFO] - Epoch 10, Batch 2300/3907, Loss: 0.398105\n",
      "[2025-05-13 10:26:05,479][training][INFO] - Epoch 10, Batch 2400/3907, Loss: 0.404004\n",
      "[2025-05-13 10:26:05,884][training][INFO] - Epoch 10, Batch 2500/3907, Loss: 0.405114\n",
      "[2025-05-13 10:26:06,288][training][INFO] - Epoch 10, Batch 2600/3907, Loss: 0.409287\n",
      "[2025-05-13 10:26:06,696][training][INFO] - Epoch 10, Batch 2700/3907, Loss: 0.402693\n",
      "[2025-05-13 10:26:07,098][training][INFO] - Epoch 10, Batch 2800/3907, Loss: 0.395079\n",
      "[2025-05-13 10:26:07,500][training][INFO] - Epoch 10, Batch 2900/3907, Loss: 0.400700\n",
      "[2025-05-13 10:26:07,903][training][INFO] - Epoch 10, Batch 3000/3907, Loss: 0.408471\n",
      "[2025-05-13 10:26:08,313][training][INFO] - Epoch 10, Batch 3100/3907, Loss: 0.405433\n",
      "[2025-05-13 10:26:08,719][training][INFO] - Epoch 10, Batch 3200/3907, Loss: 0.403608\n",
      "[2025-05-13 10:26:09,126][training][INFO] - Epoch 10, Batch 3300/3907, Loss: 0.405376\n",
      "[2025-05-13 10:26:09,528][training][INFO] - Epoch 10, Batch 3400/3907, Loss: 0.401572\n",
      "[2025-05-13 10:26:09,932][training][INFO] - Epoch 10, Batch 3500/3907, Loss: 0.409788\n",
      "[2025-05-13 10:26:10,334][training][INFO] - Epoch 10, Batch 3600/3907, Loss: 0.398498\n",
      "[2025-05-13 10:26:10,736][training][INFO] - Epoch 10, Batch 3700/3907, Loss: 0.399025\n",
      "[2025-05-13 10:26:11,139][training][INFO] - Epoch 10, Batch 3800/3907, Loss: 0.398705\n",
      "[2025-05-13 10:26:11,570][training][INFO] - Epoch 10, Batch 3900/3907, Loss: 0.405897\n",
      "[2025-05-13 10:26:11,587][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:26:11,587][training][INFO] - Epoch 10 complete. Avg Loss: 0.403864\n",
      "[2025-05-13 10:26:11,629][training][INFO] - Epoch 11, Batch 0/3907, Loss: 0.405624\n",
      "[2025-05-13 10:26:12,053][training][INFO] - Epoch 11, Batch 100/3907, Loss: 0.403552\n",
      "[2025-05-13 10:26:12,456][training][INFO] - Epoch 11, Batch 200/3907, Loss: 0.401085\n",
      "[2025-05-13 10:26:12,859][training][INFO] - Epoch 11, Batch 300/3907, Loss: 0.404589\n",
      "[2025-05-13 10:26:13,263][training][INFO] - Epoch 11, Batch 400/3907, Loss: 0.408220\n",
      "[2025-05-13 10:26:13,666][training][INFO] - Epoch 11, Batch 500/3907, Loss: 0.402644\n",
      "[2025-05-13 10:26:14,068][training][INFO] - Epoch 11, Batch 600/3907, Loss: 0.397558\n",
      "[2025-05-13 10:26:14,474][training][INFO] - Epoch 11, Batch 700/3907, Loss: 0.399138\n",
      "[2025-05-13 10:26:14,880][training][INFO] - Epoch 11, Batch 800/3907, Loss: 0.403084\n",
      "[2025-05-13 10:26:15,287][training][INFO] - Epoch 11, Batch 900/3907, Loss: 0.401785\n",
      "[2025-05-13 10:26:15,691][training][INFO] - Epoch 11, Batch 1000/3907, Loss: 0.397575\n",
      "[2025-05-13 10:26:16,096][training][INFO] - Epoch 11, Batch 1100/3907, Loss: 0.402441\n",
      "[2025-05-13 10:26:16,499][training][INFO] - Epoch 11, Batch 1200/3907, Loss: 0.398702\n",
      "[2025-05-13 10:26:16,903][training][INFO] - Epoch 11, Batch 1300/3907, Loss: 0.406841\n",
      "[2025-05-13 10:26:17,304][training][INFO] - Epoch 11, Batch 1400/3907, Loss: 0.404619\n",
      "[2025-05-13 10:26:17,708][training][INFO] - Epoch 11, Batch 1500/3907, Loss: 0.395056\n",
      "[2025-05-13 10:26:18,117][training][INFO] - Epoch 11, Batch 1600/3907, Loss: 0.404073\n",
      "[2025-05-13 10:26:18,523][training][INFO] - Epoch 11, Batch 1700/3907, Loss: 0.402433\n",
      "[2025-05-13 10:26:18,925][training][INFO] - Epoch 11, Batch 1800/3907, Loss: 0.402584\n",
      "[2025-05-13 10:26:19,329][training][INFO] - Epoch 11, Batch 1900/3907, Loss: 0.391216\n",
      "[2025-05-13 10:26:19,729][training][INFO] - Epoch 11, Batch 2000/3907, Loss: 0.402516\n",
      "[2025-05-13 10:26:20,128][training][INFO] - Epoch 11, Batch 2100/3907, Loss: 0.400145\n",
      "[2025-05-13 10:26:20,533][training][INFO] - Epoch 11, Batch 2200/3907, Loss: 0.409027\n",
      "[2025-05-13 10:26:20,934][training][INFO] - Epoch 11, Batch 2300/3907, Loss: 0.402704\n",
      "[2025-05-13 10:26:21,341][training][INFO] - Epoch 11, Batch 2400/3907, Loss: 0.397170\n",
      "[2025-05-13 10:26:21,746][training][INFO] - Epoch 11, Batch 2500/3907, Loss: 0.401237\n",
      "[2025-05-13 10:26:22,159][training][INFO] - Epoch 11, Batch 2600/3907, Loss: 0.401950\n",
      "[2025-05-13 10:26:22,569][training][INFO] - Epoch 11, Batch 2700/3907, Loss: 0.410740\n",
      "[2025-05-13 10:26:22,976][training][INFO] - Epoch 11, Batch 2800/3907, Loss: 0.399307\n",
      "[2025-05-13 10:26:23,382][training][INFO] - Epoch 11, Batch 2900/3907, Loss: 0.400530\n",
      "[2025-05-13 10:26:23,793][training][INFO] - Epoch 11, Batch 3000/3907, Loss: 0.404319\n",
      "[2025-05-13 10:26:24,198][training][INFO] - Epoch 11, Batch 3100/3907, Loss: 0.407719\n",
      "[2025-05-13 10:26:24,599][training][INFO] - Epoch 11, Batch 3200/3907, Loss: 0.403289\n",
      "[2025-05-13 10:26:25,002][training][INFO] - Epoch 11, Batch 3300/3907, Loss: 0.397652\n",
      "[2025-05-13 10:26:25,407][training][INFO] - Epoch 11, Batch 3400/3907, Loss: 0.400089\n",
      "[2025-05-13 10:26:25,809][training][INFO] - Epoch 11, Batch 3500/3907, Loss: 0.400135\n",
      "[2025-05-13 10:26:26,214][training][INFO] - Epoch 11, Batch 3600/3907, Loss: 0.408058\n",
      "[2025-05-13 10:26:26,617][training][INFO] - Epoch 11, Batch 3700/3907, Loss: 0.402113\n",
      "[2025-05-13 10:26:27,020][training][INFO] - Epoch 11, Batch 3800/3907, Loss: 0.406368\n",
      "[2025-05-13 10:26:27,450][training][INFO] - Epoch 11, Batch 3900/3907, Loss: 0.402752\n",
      "[2025-05-13 10:26:27,467][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:26:27,467][training][INFO] - Epoch 11 complete. Avg Loss: 0.402416\n",
      "[2025-05-13 10:26:27,522][training][INFO] - Epoch 12, Batch 0/3907, Loss: 0.408368\n",
      "[2025-05-13 10:26:27,926][training][INFO] - Epoch 12, Batch 100/3907, Loss: 0.401746\n",
      "[2025-05-13 10:26:28,327][training][INFO] - Epoch 12, Batch 200/3907, Loss: 0.398982\n",
      "[2025-05-13 10:26:28,732][training][INFO] - Epoch 12, Batch 300/3907, Loss: 0.403950\n",
      "[2025-05-13 10:26:29,137][training][INFO] - Epoch 12, Batch 400/3907, Loss: 0.400724\n",
      "[2025-05-13 10:26:29,540][training][INFO] - Epoch 12, Batch 500/3907, Loss: 0.402770\n",
      "[2025-05-13 10:26:29,940][training][INFO] - Epoch 12, Batch 600/3907, Loss: 0.396680\n",
      "[2025-05-13 10:26:30,343][training][INFO] - Epoch 12, Batch 700/3907, Loss: 0.404643\n",
      "[2025-05-13 10:26:30,750][training][INFO] - Epoch 12, Batch 800/3907, Loss: 0.395499\n",
      "[2025-05-13 10:26:31,155][training][INFO] - Epoch 12, Batch 900/3907, Loss: 0.397640\n",
      "[2025-05-13 10:26:31,557][training][INFO] - Epoch 12, Batch 1000/3907, Loss: 0.398528\n",
      "[2025-05-13 10:26:31,961][training][INFO] - Epoch 12, Batch 1100/3907, Loss: 0.405209\n",
      "[2025-05-13 10:26:32,367][training][INFO] - Epoch 12, Batch 1200/3907, Loss: 0.411639\n",
      "[2025-05-13 10:26:32,771][training][INFO] - Epoch 12, Batch 1300/3907, Loss: 0.401489\n",
      "[2025-05-13 10:26:33,171][training][INFO] - Epoch 12, Batch 1400/3907, Loss: 0.397050\n",
      "[2025-05-13 10:26:33,572][training][INFO] - Epoch 12, Batch 1500/3907, Loss: 0.403683\n",
      "[2025-05-13 10:26:33,976][training][INFO] - Epoch 12, Batch 1600/3907, Loss: 0.403026\n",
      "[2025-05-13 10:26:34,377][training][INFO] - Epoch 12, Batch 1700/3907, Loss: 0.396467\n",
      "[2025-05-13 10:26:34,781][training][INFO] - Epoch 12, Batch 1800/3907, Loss: 0.407019\n",
      "[2025-05-13 10:26:35,185][training][INFO] - Epoch 12, Batch 1900/3907, Loss: 0.396336\n",
      "[2025-05-13 10:26:35,591][training][INFO] - Epoch 12, Batch 2000/3907, Loss: 0.404522\n",
      "[2025-05-13 10:26:35,993][training][INFO] - Epoch 12, Batch 2100/3907, Loss: 0.403029\n",
      "[2025-05-13 10:26:36,393][training][INFO] - Epoch 12, Batch 2200/3907, Loss: 0.399355\n",
      "[2025-05-13 10:26:36,792][training][INFO] - Epoch 12, Batch 2300/3907, Loss: 0.403827\n",
      "[2025-05-13 10:26:37,192][training][INFO] - Epoch 12, Batch 2400/3907, Loss: 0.392837\n",
      "[2025-05-13 10:26:37,597][training][INFO] - Epoch 12, Batch 2500/3907, Loss: 0.403249\n",
      "[2025-05-13 10:26:37,999][training][INFO] - Epoch 12, Batch 2600/3907, Loss: 0.394560\n",
      "[2025-05-13 10:26:38,403][training][INFO] - Epoch 12, Batch 2700/3907, Loss: 0.413924\n",
      "[2025-05-13 10:26:38,805][training][INFO] - Epoch 12, Batch 2800/3907, Loss: 0.406403\n",
      "[2025-05-13 10:26:39,213][training][INFO] - Epoch 12, Batch 2900/3907, Loss: 0.403003\n",
      "[2025-05-13 10:26:39,617][training][INFO] - Epoch 12, Batch 3000/3907, Loss: 0.386908\n",
      "[2025-05-13 10:26:40,020][training][INFO] - Epoch 12, Batch 3100/3907, Loss: 0.402313\n",
      "[2025-05-13 10:26:40,423][training][INFO] - Epoch 12, Batch 3200/3907, Loss: 0.400634\n",
      "[2025-05-13 10:26:40,828][training][INFO] - Epoch 12, Batch 3300/3907, Loss: 0.402957\n",
      "[2025-05-13 10:26:41,234][training][INFO] - Epoch 12, Batch 3400/3907, Loss: 0.394313\n",
      "[2025-05-13 10:26:41,642][training][INFO] - Epoch 12, Batch 3500/3907, Loss: 0.386705\n",
      "[2025-05-13 10:26:42,046][training][INFO] - Epoch 12, Batch 3600/3907, Loss: 0.401269\n",
      "[2025-05-13 10:26:42,450][training][INFO] - Epoch 12, Batch 3700/3907, Loss: 0.399839\n",
      "[2025-05-13 10:26:42,852][training][INFO] - Epoch 12, Batch 3800/3907, Loss: 0.406809\n",
      "[2025-05-13 10:26:43,276][training][INFO] - Epoch 12, Batch 3900/3907, Loss: 0.391461\n",
      "[2025-05-13 10:26:43,293][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:26:43,293][training][INFO] - Epoch 12 complete. Avg Loss: 0.401239\n",
      "[2025-05-13 10:26:43,347][training][INFO] - Epoch 13, Batch 0/3907, Loss: 0.401176\n",
      "[2025-05-13 10:26:43,750][training][INFO] - Epoch 13, Batch 100/3907, Loss: 0.394612\n",
      "[2025-05-13 10:26:44,156][training][INFO] - Epoch 13, Batch 200/3907, Loss: 0.407434\n",
      "[2025-05-13 10:26:44,560][training][INFO] - Epoch 13, Batch 300/3907, Loss: 0.394682\n",
      "[2025-05-13 10:26:44,960][training][INFO] - Epoch 13, Batch 400/3907, Loss: 0.398138\n",
      "[2025-05-13 10:26:45,361][training][INFO] - Epoch 13, Batch 500/3907, Loss: 0.396159\n",
      "[2025-05-13 10:26:45,768][training][INFO] - Epoch 13, Batch 600/3907, Loss: 0.400519\n",
      "[2025-05-13 10:26:46,177][training][INFO] - Epoch 13, Batch 700/3907, Loss: 0.399706\n",
      "[2025-05-13 10:26:46,579][training][INFO] - Epoch 13, Batch 800/3907, Loss: 0.408576\n",
      "[2025-05-13 10:26:46,982][training][INFO] - Epoch 13, Batch 900/3907, Loss: 0.398690\n",
      "[2025-05-13 10:26:47,371][training][INFO] - Epoch 13, Batch 1000/3907, Loss: 0.404909\n",
      "[2025-05-13 10:26:47,769][training][INFO] - Epoch 13, Batch 1100/3907, Loss: 0.407399\n",
      "[2025-05-13 10:26:48,159][training][INFO] - Epoch 13, Batch 1200/3907, Loss: 0.402298\n",
      "[2025-05-13 10:26:48,545][training][INFO] - Epoch 13, Batch 1300/3907, Loss: 0.397398\n",
      "[2025-05-13 10:26:48,928][training][INFO] - Epoch 13, Batch 1400/3907, Loss: 0.389455\n",
      "[2025-05-13 10:26:49,322][training][INFO] - Epoch 13, Batch 1500/3907, Loss: 0.398620\n",
      "[2025-05-13 10:26:49,710][training][INFO] - Epoch 13, Batch 1600/3907, Loss: 0.402776\n",
      "[2025-05-13 10:26:50,095][training][INFO] - Epoch 13, Batch 1700/3907, Loss: 0.398618\n",
      "[2025-05-13 10:26:50,483][training][INFO] - Epoch 13, Batch 1800/3907, Loss: 0.402068\n",
      "[2025-05-13 10:26:50,870][training][INFO] - Epoch 13, Batch 1900/3907, Loss: 0.392991\n",
      "[2025-05-13 10:26:51,262][training][INFO] - Epoch 13, Batch 2000/3907, Loss: 0.410209\n",
      "[2025-05-13 10:26:51,650][training][INFO] - Epoch 13, Batch 2100/3907, Loss: 0.400296\n",
      "[2025-05-13 10:26:52,037][training][INFO] - Epoch 13, Batch 2200/3907, Loss: 0.405607\n",
      "[2025-05-13 10:26:52,423][training][INFO] - Epoch 13, Batch 2300/3907, Loss: 0.396397\n",
      "[2025-05-13 10:26:52,815][training][INFO] - Epoch 13, Batch 2400/3907, Loss: 0.402981\n",
      "[2025-05-13 10:26:53,204][training][INFO] - Epoch 13, Batch 2500/3907, Loss: 0.403033\n",
      "[2025-05-13 10:26:53,590][training][INFO] - Epoch 13, Batch 2600/3907, Loss: 0.403909\n",
      "[2025-05-13 10:26:53,981][training][INFO] - Epoch 13, Batch 2700/3907, Loss: 0.393265\n",
      "[2025-05-13 10:26:54,371][training][INFO] - Epoch 13, Batch 2800/3907, Loss: 0.405503\n",
      "[2025-05-13 10:26:54,764][training][INFO] - Epoch 13, Batch 2900/3907, Loss: 0.389648\n",
      "[2025-05-13 10:26:55,153][training][INFO] - Epoch 13, Batch 3000/3907, Loss: 0.403816\n",
      "[2025-05-13 10:26:55,536][training][INFO] - Epoch 13, Batch 3100/3907, Loss: 0.394931\n",
      "[2025-05-13 10:26:55,930][training][INFO] - Epoch 13, Batch 3200/3907, Loss: 0.405554\n",
      "[2025-05-13 10:26:56,327][training][INFO] - Epoch 13, Batch 3300/3907, Loss: 0.400180\n",
      "[2025-05-13 10:26:56,717][training][INFO] - Epoch 13, Batch 3400/3907, Loss: 0.395223\n",
      "[2025-05-13 10:26:57,104][training][INFO] - Epoch 13, Batch 3500/3907, Loss: 0.402271\n",
      "[2025-05-13 10:26:57,489][training][INFO] - Epoch 13, Batch 3600/3907, Loss: 0.404534\n",
      "[2025-05-13 10:26:57,882][training][INFO] - Epoch 13, Batch 3700/3907, Loss: 0.403636\n",
      "[2025-05-13 10:26:58,273][training][INFO] - Epoch 13, Batch 3800/3907, Loss: 0.394935\n",
      "[2025-05-13 10:26:58,685][training][INFO] - Epoch 13, Batch 3900/3907, Loss: 0.400049\n",
      "[2025-05-13 10:26:58,701][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:26:58,701][training][INFO] - Epoch 13 complete. Avg Loss: 0.399935\n",
      "[2025-05-13 10:26:58,748][training][INFO] - Epoch 14, Batch 0/3907, Loss: 0.402233\n",
      "[2025-05-13 10:26:59,136][training][INFO] - Epoch 14, Batch 100/3907, Loss: 0.402509\n",
      "[2025-05-13 10:26:59,527][training][INFO] - Epoch 14, Batch 200/3907, Loss: 0.399187\n",
      "[2025-05-13 10:26:59,921][training][INFO] - Epoch 14, Batch 300/3907, Loss: 0.394129\n",
      "[2025-05-13 10:27:00,307][training][INFO] - Epoch 14, Batch 400/3907, Loss: 0.405626\n",
      "[2025-05-13 10:27:00,696][training][INFO] - Epoch 14, Batch 500/3907, Loss: 0.406216\n",
      "[2025-05-13 10:27:01,084][training][INFO] - Epoch 14, Batch 600/3907, Loss: 0.399033\n",
      "[2025-05-13 10:27:01,475][training][INFO] - Epoch 14, Batch 700/3907, Loss: 0.401655\n",
      "[2025-05-13 10:27:01,861][training][INFO] - Epoch 14, Batch 800/3907, Loss: 0.396973\n",
      "[2025-05-13 10:27:02,249][training][INFO] - Epoch 14, Batch 900/3907, Loss: 0.402738\n",
      "[2025-05-13 10:27:02,635][training][INFO] - Epoch 14, Batch 1000/3907, Loss: 0.405430\n",
      "[2025-05-13 10:27:03,027][training][INFO] - Epoch 14, Batch 1100/3907, Loss: 0.394733\n",
      "[2025-05-13 10:27:03,421][training][INFO] - Epoch 14, Batch 1200/3907, Loss: 0.392631\n",
      "[2025-05-13 10:27:03,810][training][INFO] - Epoch 14, Batch 1300/3907, Loss: 0.404072\n",
      "[2025-05-13 10:27:04,200][training][INFO] - Epoch 14, Batch 1400/3907, Loss: 0.393092\n",
      "[2025-05-13 10:27:04,587][training][INFO] - Epoch 14, Batch 1500/3907, Loss: 0.402616\n",
      "[2025-05-13 10:27:04,981][training][INFO] - Epoch 14, Batch 1600/3907, Loss: 0.396614\n",
      "[2025-05-13 10:27:05,369][training][INFO] - Epoch 14, Batch 1700/3907, Loss: 0.404270\n",
      "[2025-05-13 10:27:05,756][training][INFO] - Epoch 14, Batch 1800/3907, Loss: 0.404467\n",
      "[2025-05-13 10:27:06,142][training][INFO] - Epoch 14, Batch 1900/3907, Loss: 0.391809\n",
      "[2025-05-13 10:27:06,532][training][INFO] - Epoch 14, Batch 2000/3907, Loss: 0.393242\n",
      "[2025-05-13 10:27:06,919][training][INFO] - Epoch 14, Batch 2100/3907, Loss: 0.399864\n",
      "[2025-05-13 10:27:07,307][training][INFO] - Epoch 14, Batch 2200/3907, Loss: 0.397879\n",
      "[2025-05-13 10:27:07,692][training][INFO] - Epoch 14, Batch 2300/3907, Loss: 0.400960\n",
      "[2025-05-13 10:27:08,085][training][INFO] - Epoch 14, Batch 2400/3907, Loss: 0.406816\n",
      "[2025-05-13 10:27:08,476][training][INFO] - Epoch 14, Batch 2500/3907, Loss: 0.388472\n",
      "[2025-05-13 10:27:08,862][training][INFO] - Epoch 14, Batch 2600/3907, Loss: 0.400377\n",
      "[2025-05-13 10:27:09,247][training][INFO] - Epoch 14, Batch 2700/3907, Loss: 0.403611\n",
      "[2025-05-13 10:27:09,634][training][INFO] - Epoch 14, Batch 2800/3907, Loss: 0.399599\n",
      "[2025-05-13 10:27:10,023][training][INFO] - Epoch 14, Batch 2900/3907, Loss: 0.401199\n",
      "[2025-05-13 10:27:10,410][training][INFO] - Epoch 14, Batch 3000/3907, Loss: 0.394356\n",
      "[2025-05-13 10:27:10,796][training][INFO] - Epoch 14, Batch 3100/3907, Loss: 0.392500\n",
      "[2025-05-13 10:27:11,182][training][INFO] - Epoch 14, Batch 3200/3907, Loss: 0.404796\n",
      "[2025-05-13 10:27:11,574][training][INFO] - Epoch 14, Batch 3300/3907, Loss: 0.397767\n",
      "[2025-05-13 10:27:11,963][training][INFO] - Epoch 14, Batch 3400/3907, Loss: 0.401311\n",
      "[2025-05-13 10:27:12,348][training][INFO] - Epoch 14, Batch 3500/3907, Loss: 0.392279\n",
      "[2025-05-13 10:27:12,734][training][INFO] - Epoch 14, Batch 3600/3907, Loss: 0.403046\n",
      "[2025-05-13 10:27:13,121][training][INFO] - Epoch 14, Batch 3700/3907, Loss: 0.405038\n",
      "[2025-05-13 10:27:13,515][training][INFO] - Epoch 14, Batch 3800/3907, Loss: 0.397558\n",
      "[2025-05-13 10:27:13,924][training][INFO] - Epoch 14, Batch 3900/3907, Loss: 0.395077\n",
      "[2025-05-13 10:27:13,941][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:27:13,941][training][INFO] - Epoch 14 complete. Avg Loss: 0.398863\n",
      "[2025-05-13 10:27:13,985][training][INFO] - Epoch 15, Batch 0/3907, Loss: 0.401242\n",
      "[2025-05-13 10:27:14,379][training][INFO] - Epoch 15, Batch 100/3907, Loss: 0.394788\n",
      "[2025-05-13 10:27:14,766][training][INFO] - Epoch 15, Batch 200/3907, Loss: 0.400219\n",
      "[2025-05-13 10:27:15,159][training][INFO] - Epoch 15, Batch 300/3907, Loss: 0.388958\n",
      "[2025-05-13 10:27:15,545][training][INFO] - Epoch 15, Batch 400/3907, Loss: 0.402638\n",
      "[2025-05-13 10:27:15,935][training][INFO] - Epoch 15, Batch 500/3907, Loss: 0.399869\n",
      "[2025-05-13 10:27:16,320][training][INFO] - Epoch 15, Batch 600/3907, Loss: 0.395118\n",
      "[2025-05-13 10:27:16,713][training][INFO] - Epoch 15, Batch 700/3907, Loss: 0.395028\n",
      "[2025-05-13 10:27:17,098][training][INFO] - Epoch 15, Batch 800/3907, Loss: 0.402292\n",
      "[2025-05-13 10:27:17,484][training][INFO] - Epoch 15, Batch 900/3907, Loss: 0.395804\n",
      "[2025-05-13 10:27:17,869][training][INFO] - Epoch 15, Batch 1000/3907, Loss: 0.396399\n",
      "[2025-05-13 10:27:18,259][training][INFO] - Epoch 15, Batch 1100/3907, Loss: 0.394390\n",
      "[2025-05-13 10:27:18,653][training][INFO] - Epoch 15, Batch 1200/3907, Loss: 0.389891\n",
      "[2025-05-13 10:27:19,040][training][INFO] - Epoch 15, Batch 1300/3907, Loss: 0.393639\n",
      "[2025-05-13 10:27:19,428][training][INFO] - Epoch 15, Batch 1400/3907, Loss: 0.401863\n",
      "[2025-05-13 10:27:19,813][training][INFO] - Epoch 15, Batch 1500/3907, Loss: 0.392771\n",
      "[2025-05-13 10:27:20,206][training][INFO] - Epoch 15, Batch 1600/3907, Loss: 0.387731\n",
      "[2025-05-13 10:27:20,593][training][INFO] - Epoch 15, Batch 1700/3907, Loss: 0.399803\n",
      "[2025-05-13 10:27:20,980][training][INFO] - Epoch 15, Batch 1800/3907, Loss: 0.392723\n",
      "[2025-05-13 10:27:21,365][training][INFO] - Epoch 15, Batch 1900/3907, Loss: 0.396968\n",
      "[2025-05-13 10:27:21,755][training][INFO] - Epoch 15, Batch 2000/3907, Loss: 0.395512\n",
      "[2025-05-13 10:27:22,144][training][INFO] - Epoch 15, Batch 2100/3907, Loss: 0.392370\n",
      "[2025-05-13 10:27:22,533][training][INFO] - Epoch 15, Batch 2200/3907, Loss: 0.400467\n",
      "[2025-05-13 10:27:22,919][training][INFO] - Epoch 15, Batch 2300/3907, Loss: 0.392599\n",
      "[2025-05-13 10:27:23,306][training][INFO] - Epoch 15, Batch 2400/3907, Loss: 0.403119\n",
      "[2025-05-13 10:27:23,700][training][INFO] - Epoch 15, Batch 2500/3907, Loss: 0.402774\n",
      "[2025-05-13 10:27:24,089][training][INFO] - Epoch 15, Batch 2600/3907, Loss: 0.396007\n",
      "[2025-05-13 10:27:24,483][training][INFO] - Epoch 15, Batch 2700/3907, Loss: 0.388908\n",
      "[2025-05-13 10:27:24,875][training][INFO] - Epoch 15, Batch 2800/3907, Loss: 0.394919\n",
      "[2025-05-13 10:27:25,274][training][INFO] - Epoch 15, Batch 2900/3907, Loss: 0.392820\n",
      "[2025-05-13 10:27:25,666][training][INFO] - Epoch 15, Batch 3000/3907, Loss: 0.397811\n",
      "[2025-05-13 10:27:26,058][training][INFO] - Epoch 15, Batch 3100/3907, Loss: 0.398174\n",
      "[2025-05-13 10:27:26,454][training][INFO] - Epoch 15, Batch 3200/3907, Loss: 0.395725\n",
      "[2025-05-13 10:27:26,848][training][INFO] - Epoch 15, Batch 3300/3907, Loss: 0.394249\n",
      "[2025-05-13 10:27:27,245][training][INFO] - Epoch 15, Batch 3400/3907, Loss: 0.402997\n",
      "[2025-05-13 10:27:27,639][training][INFO] - Epoch 15, Batch 3500/3907, Loss: 0.401733\n",
      "[2025-05-13 10:27:28,028][training][INFO] - Epoch 15, Batch 3600/3907, Loss: 0.400590\n",
      "[2025-05-13 10:27:28,416][training][INFO] - Epoch 15, Batch 3700/3907, Loss: 0.390521\n",
      "[2025-05-13 10:27:28,815][training][INFO] - Epoch 15, Batch 3800/3907, Loss: 0.399789\n",
      "[2025-05-13 10:27:29,227][training][INFO] - Epoch 15, Batch 3900/3907, Loss: 0.406614\n",
      "[2025-05-13 10:27:29,244][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:27:29,244][training][INFO] - Epoch 15 complete. Avg Loss: 0.397883\n",
      "[2025-05-13 10:27:29,294][training][INFO] - Epoch 16, Batch 0/3907, Loss: 0.394440\n",
      "[2025-05-13 10:27:29,683][training][INFO] - Epoch 16, Batch 100/3907, Loss: 0.398977\n",
      "[2025-05-13 10:27:30,071][training][INFO] - Epoch 16, Batch 200/3907, Loss: 0.401507\n",
      "[2025-05-13 10:27:30,463][training][INFO] - Epoch 16, Batch 300/3907, Loss: 0.395918\n",
      "[2025-05-13 10:27:30,859][training][INFO] - Epoch 16, Batch 400/3907, Loss: 0.391218\n",
      "[2025-05-13 10:27:31,246][training][INFO] - Epoch 16, Batch 500/3907, Loss: 0.394550\n",
      "[2025-05-13 10:27:31,631][training][INFO] - Epoch 16, Batch 600/3907, Loss: 0.386649\n",
      "[2025-05-13 10:27:32,019][training][INFO] - Epoch 16, Batch 700/3907, Loss: 0.395669\n",
      "[2025-05-13 10:27:32,410][training][INFO] - Epoch 16, Batch 800/3907, Loss: 0.387640\n",
      "[2025-05-13 10:27:32,800][training][INFO] - Epoch 16, Batch 900/3907, Loss: 0.403191\n",
      "[2025-05-13 10:27:33,187][training][INFO] - Epoch 16, Batch 1000/3907, Loss: 0.397657\n",
      "[2025-05-13 10:27:33,577][training][INFO] - Epoch 16, Batch 1100/3907, Loss: 0.390693\n",
      "[2025-05-13 10:27:33,971][training][INFO] - Epoch 16, Batch 1200/3907, Loss: 0.388195\n",
      "[2025-05-13 10:27:34,359][training][INFO] - Epoch 16, Batch 1300/3907, Loss: 0.399120\n",
      "[2025-05-13 10:27:34,743][training][INFO] - Epoch 16, Batch 1400/3907, Loss: 0.394840\n",
      "[2025-05-13 10:27:35,128][training][INFO] - Epoch 16, Batch 1500/3907, Loss: 0.402655\n",
      "[2025-05-13 10:27:35,519][training][INFO] - Epoch 16, Batch 1600/3907, Loss: 0.395576\n",
      "[2025-05-13 10:27:35,905][training][INFO] - Epoch 16, Batch 1700/3907, Loss: 0.406625\n",
      "[2025-05-13 10:27:36,290][training][INFO] - Epoch 16, Batch 1800/3907, Loss: 0.394756\n",
      "[2025-05-13 10:27:36,676][training][INFO] - Epoch 16, Batch 1900/3907, Loss: 0.396505\n",
      "[2025-05-13 10:27:37,060][training][INFO] - Epoch 16, Batch 2000/3907, Loss: 0.396517\n",
      "[2025-05-13 10:27:37,453][training][INFO] - Epoch 16, Batch 2100/3907, Loss: 0.396859\n",
      "[2025-05-13 10:27:37,836][training][INFO] - Epoch 16, Batch 2200/3907, Loss: 0.403275\n",
      "[2025-05-13 10:27:38,225][training][INFO] - Epoch 16, Batch 2300/3907, Loss: 0.393997\n",
      "[2025-05-13 10:27:38,612][training][INFO] - Epoch 16, Batch 2400/3907, Loss: 0.395803\n",
      "[2025-05-13 10:27:39,005][training][INFO] - Epoch 16, Batch 2500/3907, Loss: 0.394729\n",
      "[2025-05-13 10:27:39,395][training][INFO] - Epoch 16, Batch 2600/3907, Loss: 0.395880\n",
      "[2025-05-13 10:27:39,783][training][INFO] - Epoch 16, Batch 2700/3907, Loss: 0.394396\n",
      "[2025-05-13 10:27:40,167][training][INFO] - Epoch 16, Batch 2800/3907, Loss: 0.394133\n",
      "[2025-05-13 10:27:40,557][training][INFO] - Epoch 16, Batch 2900/3907, Loss: 0.397527\n",
      "[2025-05-13 10:27:40,954][training][INFO] - Epoch 16, Batch 3000/3907, Loss: 0.393162\n",
      "[2025-05-13 10:27:41,341][training][INFO] - Epoch 16, Batch 3100/3907, Loss: 0.393928\n",
      "[2025-05-13 10:27:41,725][training][INFO] - Epoch 16, Batch 3200/3907, Loss: 0.402280\n",
      "[2025-05-13 10:27:42,112][training][INFO] - Epoch 16, Batch 3300/3907, Loss: 0.404423\n",
      "[2025-05-13 10:27:42,506][training][INFO] - Epoch 16, Batch 3400/3907, Loss: 0.387659\n",
      "[2025-05-13 10:27:42,892][training][INFO] - Epoch 16, Batch 3500/3907, Loss: 0.387091\n",
      "[2025-05-13 10:27:43,281][training][INFO] - Epoch 16, Batch 3600/3907, Loss: 0.391903\n",
      "[2025-05-13 10:27:43,665][training][INFO] - Epoch 16, Batch 3700/3907, Loss: 0.400723\n",
      "[2025-05-13 10:27:44,064][training][INFO] - Epoch 16, Batch 3800/3907, Loss: 0.395826\n",
      "[2025-05-13 10:27:44,474][training][INFO] - Epoch 16, Batch 3900/3907, Loss: 0.393337\n",
      "[2025-05-13 10:27:44,491][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:27:44,491][training][INFO] - Epoch 16 complete. Avg Loss: 0.397008\n",
      "[2025-05-13 10:27:44,542][training][INFO] - Epoch 17, Batch 0/3907, Loss: 0.394937\n",
      "[2025-05-13 10:27:44,927][training][INFO] - Epoch 17, Batch 100/3907, Loss: 0.406584\n",
      "[2025-05-13 10:27:45,313][training][INFO] - Epoch 17, Batch 200/3907, Loss: 0.398852\n",
      "[2025-05-13 10:27:45,711][training][INFO] - Epoch 17, Batch 300/3907, Loss: 0.394958\n",
      "[2025-05-13 10:27:46,105][training][INFO] - Epoch 17, Batch 400/3907, Loss: 0.399654\n",
      "[2025-05-13 10:27:46,492][training][INFO] - Epoch 17, Batch 500/3907, Loss: 0.394830\n",
      "[2025-05-13 10:27:46,876][training][INFO] - Epoch 17, Batch 600/3907, Loss: 0.389434\n",
      "[2025-05-13 10:27:47,266][training][INFO] - Epoch 17, Batch 700/3907, Loss: 0.388280\n",
      "[2025-05-13 10:27:47,659][training][INFO] - Epoch 17, Batch 800/3907, Loss: 0.393214\n",
      "[2025-05-13 10:27:48,048][training][INFO] - Epoch 17, Batch 900/3907, Loss: 0.390440\n",
      "[2025-05-13 10:27:48,437][training][INFO] - Epoch 17, Batch 1000/3907, Loss: 0.386230\n",
      "[2025-05-13 10:27:48,825][training][INFO] - Epoch 17, Batch 1100/3907, Loss: 0.389943\n",
      "[2025-05-13 10:27:49,222][training][INFO] - Epoch 17, Batch 1200/3907, Loss: 0.395038\n",
      "[2025-05-13 10:27:49,612][training][INFO] - Epoch 17, Batch 1300/3907, Loss: 0.393220\n",
      "[2025-05-13 10:27:50,003][training][INFO] - Epoch 17, Batch 1400/3907, Loss: 0.394644\n",
      "[2025-05-13 10:27:50,397][training][INFO] - Epoch 17, Batch 1500/3907, Loss: 0.386777\n",
      "[2025-05-13 10:27:50,789][training][INFO] - Epoch 17, Batch 1600/3907, Loss: 0.390039\n",
      "[2025-05-13 10:27:51,181][training][INFO] - Epoch 17, Batch 1700/3907, Loss: 0.395402\n",
      "[2025-05-13 10:27:51,566][training][INFO] - Epoch 17, Batch 1800/3907, Loss: 0.404078\n",
      "[2025-05-13 10:27:51,952][training][INFO] - Epoch 17, Batch 1900/3907, Loss: 0.400716\n",
      "[2025-05-13 10:27:52,342][training][INFO] - Epoch 17, Batch 2000/3907, Loss: 0.394279\n",
      "[2025-05-13 10:27:52,737][training][INFO] - Epoch 17, Batch 2100/3907, Loss: 0.397541\n",
      "[2025-05-13 10:27:53,122][training][INFO] - Epoch 17, Batch 2200/3907, Loss: 0.398154\n",
      "[2025-05-13 10:27:53,511][training][INFO] - Epoch 17, Batch 2300/3907, Loss: 0.390723\n",
      "[2025-05-13 10:27:53,897][training][INFO] - Epoch 17, Batch 2400/3907, Loss: 0.391003\n",
      "[2025-05-13 10:27:54,293][training][INFO] - Epoch 17, Batch 2500/3907, Loss: 0.392387\n",
      "[2025-05-13 10:27:54,686][training][INFO] - Epoch 17, Batch 2600/3907, Loss: 0.394064\n",
      "[2025-05-13 10:27:55,075][training][INFO] - Epoch 17, Batch 2700/3907, Loss: 0.396723\n",
      "[2025-05-13 10:27:55,462][training][INFO] - Epoch 17, Batch 2800/3907, Loss: 0.398284\n",
      "[2025-05-13 10:27:55,850][training][INFO] - Epoch 17, Batch 2900/3907, Loss: 0.399524\n",
      "[2025-05-13 10:27:56,244][training][INFO] - Epoch 17, Batch 3000/3907, Loss: 0.403087\n",
      "[2025-05-13 10:27:56,629][training][INFO] - Epoch 17, Batch 3100/3907, Loss: 0.406835\n",
      "[2025-05-13 10:27:57,017][training][INFO] - Epoch 17, Batch 3200/3907, Loss: 0.393067\n",
      "[2025-05-13 10:27:57,402][training][INFO] - Epoch 17, Batch 3300/3907, Loss: 0.396714\n",
      "[2025-05-13 10:27:57,795][training][INFO] - Epoch 17, Batch 3400/3907, Loss: 0.397472\n",
      "[2025-05-13 10:27:58,181][training][INFO] - Epoch 17, Batch 3500/3907, Loss: 0.396708\n",
      "[2025-05-13 10:27:58,568][training][INFO] - Epoch 17, Batch 3600/3907, Loss: 0.394159\n",
      "[2025-05-13 10:27:58,958][training][INFO] - Epoch 17, Batch 3700/3907, Loss: 0.403548\n",
      "[2025-05-13 10:27:59,355][training][INFO] - Epoch 17, Batch 3800/3907, Loss: 0.405762\n",
      "[2025-05-13 10:27:59,770][training][INFO] - Epoch 17, Batch 3900/3907, Loss: 0.396196\n",
      "[2025-05-13 10:27:59,786][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:27:59,786][training][INFO] - Epoch 17 complete. Avg Loss: 0.396188\n",
      "[2025-05-13 10:27:59,832][training][INFO] - Epoch 18, Batch 0/3907, Loss: 0.399920\n",
      "[2025-05-13 10:28:00,226][training][INFO] - Epoch 18, Batch 100/3907, Loss: 0.390757\n",
      "[2025-05-13 10:28:00,617][training][INFO] - Epoch 18, Batch 200/3907, Loss: 0.384804\n",
      "[2025-05-13 10:28:01,008][training][INFO] - Epoch 18, Batch 300/3907, Loss: 0.400845\n",
      "[2025-05-13 10:28:01,401][training][INFO] - Epoch 18, Batch 400/3907, Loss: 0.396365\n",
      "[2025-05-13 10:28:01,788][training][INFO] - Epoch 18, Batch 500/3907, Loss: 0.393224\n",
      "[2025-05-13 10:28:02,175][training][INFO] - Epoch 18, Batch 600/3907, Loss: 0.395227\n",
      "[2025-05-13 10:28:02,561][training][INFO] - Epoch 18, Batch 700/3907, Loss: 0.402851\n",
      "[2025-05-13 10:28:02,958][training][INFO] - Epoch 18, Batch 800/3907, Loss: 0.396984\n",
      "[2025-05-13 10:28:03,343][training][INFO] - Epoch 18, Batch 900/3907, Loss: 0.386728\n",
      "[2025-05-13 10:28:03,727][training][INFO] - Epoch 18, Batch 1000/3907, Loss: 0.391916\n",
      "[2025-05-13 10:28:04,113][training][INFO] - Epoch 18, Batch 1100/3907, Loss: 0.385012\n",
      "[2025-05-13 10:28:04,508][training][INFO] - Epoch 18, Batch 1200/3907, Loss: 0.394549\n",
      "[2025-05-13 10:28:04,894][training][INFO] - Epoch 18, Batch 1300/3907, Loss: 0.388281\n",
      "[2025-05-13 10:28:05,277][training][INFO] - Epoch 18, Batch 1400/3907, Loss: 0.393599\n",
      "[2025-05-13 10:28:05,666][training][INFO] - Epoch 18, Batch 1500/3907, Loss: 0.378701\n",
      "[2025-05-13 10:28:06,052][training][INFO] - Epoch 18, Batch 1600/3907, Loss: 0.397588\n",
      "[2025-05-13 10:28:06,445][training][INFO] - Epoch 18, Batch 1700/3907, Loss: 0.388966\n",
      "[2025-05-13 10:28:06,828][training][INFO] - Epoch 18, Batch 1800/3907, Loss: 0.404274\n",
      "[2025-05-13 10:28:07,215][training][INFO] - Epoch 18, Batch 1900/3907, Loss: 0.400915\n",
      "[2025-05-13 10:28:07,599][training][INFO] - Epoch 18, Batch 2000/3907, Loss: 0.393630\n",
      "[2025-05-13 10:28:07,989][training][INFO] - Epoch 18, Batch 2100/3907, Loss: 0.399014\n",
      "[2025-05-13 10:28:08,377][training][INFO] - Epoch 18, Batch 2200/3907, Loss: 0.401751\n",
      "[2025-05-13 10:28:08,763][training][INFO] - Epoch 18, Batch 2300/3907, Loss: 0.394378\n",
      "[2025-05-13 10:28:09,150][training][INFO] - Epoch 18, Batch 2400/3907, Loss: 0.396691\n",
      "[2025-05-13 10:28:09,539][training][INFO] - Epoch 18, Batch 2500/3907, Loss: 0.402196\n",
      "[2025-05-13 10:28:09,934][training][INFO] - Epoch 18, Batch 2600/3907, Loss: 0.404860\n",
      "[2025-05-13 10:28:10,322][training][INFO] - Epoch 18, Batch 2700/3907, Loss: 0.394515\n",
      "[2025-05-13 10:28:10,709][training][INFO] - Epoch 18, Batch 2800/3907, Loss: 0.399343\n",
      "[2025-05-13 10:28:11,094][training][INFO] - Epoch 18, Batch 2900/3907, Loss: 0.398024\n",
      "[2025-05-13 10:28:11,487][training][INFO] - Epoch 18, Batch 3000/3907, Loss: 0.400992\n",
      "[2025-05-13 10:28:11,876][training][INFO] - Epoch 18, Batch 3100/3907, Loss: 0.392369\n",
      "[2025-05-13 10:28:12,265][training][INFO] - Epoch 18, Batch 3200/3907, Loss: 0.383142\n",
      "[2025-05-13 10:28:12,652][training][INFO] - Epoch 18, Batch 3300/3907, Loss: 0.396478\n",
      "[2025-05-13 10:28:13,045][training][INFO] - Epoch 18, Batch 3400/3907, Loss: 0.401800\n",
      "[2025-05-13 10:28:13,437][training][INFO] - Epoch 18, Batch 3500/3907, Loss: 0.395743\n",
      "[2025-05-13 10:28:13,823][training][INFO] - Epoch 18, Batch 3600/3907, Loss: 0.388068\n",
      "[2025-05-13 10:28:14,211][training][INFO] - Epoch 18, Batch 3700/3907, Loss: 0.400700\n",
      "[2025-05-13 10:28:14,601][training][INFO] - Epoch 18, Batch 3800/3907, Loss: 0.397453\n",
      "[2025-05-13 10:28:15,019][training][INFO] - Epoch 18, Batch 3900/3907, Loss: 0.400871\n",
      "[2025-05-13 10:28:15,036][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:28:15,036][training][INFO] - Epoch 18 complete. Avg Loss: 0.395435\n",
      "[2025-05-13 10:28:15,070][training][INFO] - Epoch 19, Batch 0/3907, Loss: 0.395108\n",
      "[2025-05-13 10:28:15,474][training][INFO] - Epoch 19, Batch 100/3907, Loss: 0.391415\n",
      "[2025-05-13 10:28:15,866][training][INFO] - Epoch 19, Batch 200/3907, Loss: 0.397604\n",
      "[2025-05-13 10:28:16,255][training][INFO] - Epoch 19, Batch 300/3907, Loss: 0.400193\n",
      "[2025-05-13 10:28:16,647][training][INFO] - Epoch 19, Batch 400/3907, Loss: 0.389771\n",
      "[2025-05-13 10:28:17,033][training][INFO] - Epoch 19, Batch 500/3907, Loss: 0.392569\n",
      "[2025-05-13 10:28:17,420][training][INFO] - Epoch 19, Batch 600/3907, Loss: 0.393323\n",
      "[2025-05-13 10:28:17,804][training][INFO] - Epoch 19, Batch 700/3907, Loss: 0.391261\n",
      "[2025-05-13 10:28:18,195][training][INFO] - Epoch 19, Batch 800/3907, Loss: 0.395170\n",
      "[2025-05-13 10:28:18,582][training][INFO] - Epoch 19, Batch 900/3907, Loss: 0.394327\n",
      "[2025-05-13 10:28:18,967][training][INFO] - Epoch 19, Batch 1000/3907, Loss: 0.391363\n",
      "[2025-05-13 10:28:19,353][training][INFO] - Epoch 19, Batch 1100/3907, Loss: 0.400448\n",
      "[2025-05-13 10:28:19,738][training][INFO] - Epoch 19, Batch 1200/3907, Loss: 0.394405\n",
      "[2025-05-13 10:28:20,141][training][INFO] - Epoch 19, Batch 1300/3907, Loss: 0.400194\n",
      "[2025-05-13 10:28:20,533][training][INFO] - Epoch 19, Batch 1400/3907, Loss: 0.397613\n",
      "[2025-05-13 10:28:20,916][training][INFO] - Epoch 19, Batch 1500/3907, Loss: 0.392853\n",
      "[2025-05-13 10:28:21,300][training][INFO] - Epoch 19, Batch 1600/3907, Loss: 0.395984\n",
      "[2025-05-13 10:28:21,690][training][INFO] - Epoch 19, Batch 1700/3907, Loss: 0.398214\n",
      "[2025-05-13 10:28:22,081][training][INFO] - Epoch 19, Batch 1800/3907, Loss: 0.390366\n",
      "[2025-05-13 10:28:22,465][training][INFO] - Epoch 19, Batch 1900/3907, Loss: 0.391334\n",
      "[2025-05-13 10:28:22,850][training][INFO] - Epoch 19, Batch 2000/3907, Loss: 0.391650\n",
      "[2025-05-13 10:28:23,235][training][INFO] - Epoch 19, Batch 2100/3907, Loss: 0.401696\n",
      "[2025-05-13 10:28:23,627][training][INFO] - Epoch 19, Batch 2200/3907, Loss: 0.390923\n",
      "[2025-05-13 10:28:24,015][training][INFO] - Epoch 19, Batch 2300/3907, Loss: 0.400249\n",
      "[2025-05-13 10:28:24,399][training][INFO] - Epoch 19, Batch 2400/3907, Loss: 0.401503\n",
      "[2025-05-13 10:28:24,788][training][INFO] - Epoch 19, Batch 2500/3907, Loss: 0.396716\n",
      "[2025-05-13 10:28:25,183][training][INFO] - Epoch 19, Batch 2600/3907, Loss: 0.398224\n",
      "[2025-05-13 10:28:25,569][training][INFO] - Epoch 19, Batch 2700/3907, Loss: 0.392807\n",
      "[2025-05-13 10:28:25,957][training][INFO] - Epoch 19, Batch 2800/3907, Loss: 0.401766\n",
      "[2025-05-13 10:28:26,341][training][INFO] - Epoch 19, Batch 2900/3907, Loss: 0.395247\n",
      "[2025-05-13 10:28:26,731][training][INFO] - Epoch 19, Batch 3000/3907, Loss: 0.400677\n",
      "[2025-05-13 10:28:27,120][training][INFO] - Epoch 19, Batch 3100/3907, Loss: 0.400914\n",
      "[2025-05-13 10:28:27,506][training][INFO] - Epoch 19, Batch 3200/3907, Loss: 0.400048\n",
      "[2025-05-13 10:28:27,888][training][INFO] - Epoch 19, Batch 3300/3907, Loss: 0.398152\n",
      "[2025-05-13 10:28:28,271][training][INFO] - Epoch 19, Batch 3400/3907, Loss: 0.391877\n",
      "[2025-05-13 10:28:28,662][training][INFO] - Epoch 19, Batch 3500/3907, Loss: 0.398066\n",
      "[2025-05-13 10:28:29,045][training][INFO] - Epoch 19, Batch 3600/3907, Loss: 0.395992\n",
      "[2025-05-13 10:28:29,431][training][INFO] - Epoch 19, Batch 3700/3907, Loss: 0.393395\n",
      "[2025-05-13 10:28:29,817][training][INFO] - Epoch 19, Batch 3800/3907, Loss: 0.404438\n",
      "[2025-05-13 10:28:30,230][training][INFO] - Epoch 19, Batch 3900/3907, Loss: 0.397309\n",
      "[2025-05-13 10:28:30,247][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:28:30,247][training][INFO] - Epoch 19 complete. Avg Loss: 0.394671\n",
      "[2025-05-13 10:28:30,296][training][INFO] - Epoch 20, Batch 0/3907, Loss: 0.397707\n",
      "[2025-05-13 10:28:30,684][training][INFO] - Epoch 20, Batch 100/3907, Loss: 0.400397\n",
      "[2025-05-13 10:28:31,069][training][INFO] - Epoch 20, Batch 200/3907, Loss: 0.393043\n",
      "[2025-05-13 10:28:31,457][training][INFO] - Epoch 20, Batch 300/3907, Loss: 0.389272\n",
      "[2025-05-13 10:28:31,848][training][INFO] - Epoch 20, Batch 400/3907, Loss: 0.386332\n",
      "[2025-05-13 10:28:32,235][training][INFO] - Epoch 20, Batch 500/3907, Loss: 0.398365\n",
      "[2025-05-13 10:28:32,620][training][INFO] - Epoch 20, Batch 600/3907, Loss: 0.402249\n",
      "[2025-05-13 10:28:33,003][training][INFO] - Epoch 20, Batch 700/3907, Loss: 0.404850\n",
      "[2025-05-13 10:28:33,386][training][INFO] - Epoch 20, Batch 800/3907, Loss: 0.387133\n",
      "[2025-05-13 10:28:33,778][training][INFO] - Epoch 20, Batch 900/3907, Loss: 0.397475\n",
      "[2025-05-13 10:28:34,163][training][INFO] - Epoch 20, Batch 1000/3907, Loss: 0.393274\n",
      "[2025-05-13 10:28:34,547][training][INFO] - Epoch 20, Batch 1100/3907, Loss: 0.391416\n",
      "[2025-05-13 10:28:34,931][training][INFO] - Epoch 20, Batch 1200/3907, Loss: 0.385478\n",
      "[2025-05-13 10:28:35,323][training][INFO] - Epoch 20, Batch 1300/3907, Loss: 0.397367\n",
      "[2025-05-13 10:28:35,708][training][INFO] - Epoch 20, Batch 1400/3907, Loss: 0.396672\n",
      "[2025-05-13 10:28:36,100][training][INFO] - Epoch 20, Batch 1500/3907, Loss: 0.396497\n",
      "[2025-05-13 10:28:36,488][training][INFO] - Epoch 20, Batch 1600/3907, Loss: 0.400073\n",
      "[2025-05-13 10:28:36,874][training][INFO] - Epoch 20, Batch 1700/3907, Loss: 0.389989\n",
      "[2025-05-13 10:28:37,265][training][INFO] - Epoch 20, Batch 1800/3907, Loss: 0.392757\n",
      "[2025-05-13 10:28:37,650][training][INFO] - Epoch 20, Batch 1900/3907, Loss: 0.400044\n",
      "[2025-05-13 10:28:38,030][training][INFO] - Epoch 20, Batch 2000/3907, Loss: 0.395625\n",
      "[2025-05-13 10:28:38,413][training][INFO] - Epoch 20, Batch 2100/3907, Loss: 0.398729\n",
      "[2025-05-13 10:28:38,804][training][INFO] - Epoch 20, Batch 2200/3907, Loss: 0.380986\n",
      "[2025-05-13 10:28:39,192][training][INFO] - Epoch 20, Batch 2300/3907, Loss: 0.394594\n",
      "[2025-05-13 10:28:39,577][training][INFO] - Epoch 20, Batch 2400/3907, Loss: 0.392409\n",
      "[2025-05-13 10:28:39,963][training][INFO] - Epoch 20, Batch 2500/3907, Loss: 0.388730\n",
      "[2025-05-13 10:28:40,356][training][INFO] - Epoch 20, Batch 2600/3907, Loss: 0.397173\n",
      "[2025-05-13 10:28:40,747][training][INFO] - Epoch 20, Batch 2700/3907, Loss: 0.393511\n",
      "[2025-05-13 10:28:41,133][training][INFO] - Epoch 20, Batch 2800/3907, Loss: 0.401856\n",
      "[2025-05-13 10:28:41,524][training][INFO] - Epoch 20, Batch 2900/3907, Loss: 0.394452\n",
      "[2025-05-13 10:28:41,907][training][INFO] - Epoch 20, Batch 3000/3907, Loss: 0.390035\n",
      "[2025-05-13 10:28:42,299][training][INFO] - Epoch 20, Batch 3100/3907, Loss: 0.400134\n",
      "[2025-05-13 10:28:42,686][training][INFO] - Epoch 20, Batch 3200/3907, Loss: 0.394836\n",
      "[2025-05-13 10:28:43,072][training][INFO] - Epoch 20, Batch 3300/3907, Loss: 0.389383\n",
      "[2025-05-13 10:28:43,454][training][INFO] - Epoch 20, Batch 3400/3907, Loss: 0.390563\n",
      "[2025-05-13 10:28:43,844][training][INFO] - Epoch 20, Batch 3500/3907, Loss: 0.415140\n",
      "[2025-05-13 10:28:44,231][training][INFO] - Epoch 20, Batch 3600/3907, Loss: 0.398882\n",
      "[2025-05-13 10:28:44,618][training][INFO] - Epoch 20, Batch 3700/3907, Loss: 0.405499\n",
      "[2025-05-13 10:28:45,009][training][INFO] - Epoch 20, Batch 3800/3907, Loss: 0.389731\n",
      "[2025-05-13 10:28:45,424][training][INFO] - Epoch 20, Batch 3900/3907, Loss: 0.397592\n",
      "[2025-05-13 10:28:45,441][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:28:45,441][training][INFO] - Epoch 20 complete. Avg Loss: 0.394269\n",
      "[2025-05-13 10:28:45,463][training][INFO] - Saved checkpoint to /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7/checkpoint_epoch_20.pt\n",
      "[2025-05-13 10:28:45,516][training][INFO] - Epoch 21, Batch 0/3907, Loss: 0.394856\n",
      "[2025-05-13 10:28:45,911][training][INFO] - Epoch 21, Batch 100/3907, Loss: 0.397340\n",
      "[2025-05-13 10:28:46,299][training][INFO] - Epoch 21, Batch 200/3907, Loss: 0.392987\n",
      "[2025-05-13 10:28:46,682][training][INFO] - Epoch 21, Batch 300/3907, Loss: 0.395104\n",
      "[2025-05-13 10:28:47,067][training][INFO] - Epoch 21, Batch 400/3907, Loss: 0.388280\n",
      "[2025-05-13 10:28:47,460][training][INFO] - Epoch 21, Batch 500/3907, Loss: 0.394628\n",
      "[2025-05-13 10:28:47,847][training][INFO] - Epoch 21, Batch 600/3907, Loss: 0.379898\n",
      "[2025-05-13 10:28:48,233][training][INFO] - Epoch 21, Batch 700/3907, Loss: 0.389979\n",
      "[2025-05-13 10:28:48,617][training][INFO] - Epoch 21, Batch 800/3907, Loss: 0.395407\n",
      "[2025-05-13 10:28:49,010][training][INFO] - Epoch 21, Batch 900/3907, Loss: 0.387497\n",
      "[2025-05-13 10:28:49,396][training][INFO] - Epoch 21, Batch 1000/3907, Loss: 0.394973\n",
      "[2025-05-13 10:28:49,783][training][INFO] - Epoch 21, Batch 1100/3907, Loss: 0.389289\n",
      "[2025-05-13 10:28:50,169][training][INFO] - Epoch 21, Batch 1200/3907, Loss: 0.397650\n",
      "[2025-05-13 10:28:50,561][training][INFO] - Epoch 21, Batch 1300/3907, Loss: 0.389708\n",
      "[2025-05-13 10:28:50,951][training][INFO] - Epoch 21, Batch 1400/3907, Loss: 0.394811\n",
      "[2025-05-13 10:28:51,336][training][INFO] - Epoch 21, Batch 1500/3907, Loss: 0.401106\n",
      "[2025-05-13 10:28:51,720][training][INFO] - Epoch 21, Batch 1600/3907, Loss: 0.401871\n",
      "[2025-05-13 10:28:52,103][training][INFO] - Epoch 21, Batch 1700/3907, Loss: 0.402512\n",
      "[2025-05-13 10:28:52,499][training][INFO] - Epoch 21, Batch 1800/3907, Loss: 0.393980\n",
      "[2025-05-13 10:28:52,884][training][INFO] - Epoch 21, Batch 1900/3907, Loss: 0.395429\n",
      "[2025-05-13 10:28:53,270][training][INFO] - Epoch 21, Batch 2000/3907, Loss: 0.391868\n",
      "[2025-05-13 10:28:53,659][training][INFO] - Epoch 21, Batch 2100/3907, Loss: 0.394983\n",
      "[2025-05-13 10:28:54,050][training][INFO] - Epoch 21, Batch 2200/3907, Loss: 0.388572\n",
      "[2025-05-13 10:28:54,439][training][INFO] - Epoch 21, Batch 2300/3907, Loss: 0.391221\n",
      "[2025-05-13 10:28:54,825][training][INFO] - Epoch 21, Batch 2400/3907, Loss: 0.397625\n",
      "[2025-05-13 10:28:55,213][training][INFO] - Epoch 21, Batch 2500/3907, Loss: 0.396327\n",
      "[2025-05-13 10:28:55,600][training][INFO] - Epoch 21, Batch 2600/3907, Loss: 0.392460\n",
      "[2025-05-13 10:28:55,991][training][INFO] - Epoch 21, Batch 2700/3907, Loss: 0.395638\n",
      "[2025-05-13 10:28:56,380][training][INFO] - Epoch 21, Batch 2800/3907, Loss: 0.391743\n",
      "[2025-05-13 10:28:56,769][training][INFO] - Epoch 21, Batch 2900/3907, Loss: 0.390224\n",
      "[2025-05-13 10:28:57,152][training][INFO] - Epoch 21, Batch 3000/3907, Loss: 0.392170\n",
      "[2025-05-13 10:28:57,541][training][INFO] - Epoch 21, Batch 3100/3907, Loss: 0.397507\n",
      "[2025-05-13 10:28:57,928][training][INFO] - Epoch 21, Batch 3200/3907, Loss: 0.390358\n",
      "[2025-05-13 10:28:58,315][training][INFO] - Epoch 21, Batch 3300/3907, Loss: 0.393727\n",
      "[2025-05-13 10:28:58,702][training][INFO] - Epoch 21, Batch 3400/3907, Loss: 0.406432\n",
      "[2025-05-13 10:28:59,085][training][INFO] - Epoch 21, Batch 3500/3907, Loss: 0.392997\n",
      "[2025-05-13 10:28:59,482][training][INFO] - Epoch 21, Batch 3600/3907, Loss: 0.389250\n",
      "[2025-05-13 10:28:59,869][training][INFO] - Epoch 21, Batch 3700/3907, Loss: 0.391740\n",
      "[2025-05-13 10:29:00,257][training][INFO] - Epoch 21, Batch 3800/3907, Loss: 0.401040\n",
      "[2025-05-13 10:29:00,669][training][INFO] - Epoch 21, Batch 3900/3907, Loss: 0.396012\n",
      "[2025-05-13 10:29:00,686][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:29:00,686][training][INFO] - Epoch 21 complete. Avg Loss: 0.393702\n",
      "[2025-05-13 10:29:00,734][training][INFO] - Epoch 22, Batch 0/3907, Loss: 0.393056\n",
      "[2025-05-13 10:29:01,130][training][INFO] - Epoch 22, Batch 100/3907, Loss: 0.389965\n",
      "[2025-05-13 10:29:01,518][training][INFO] - Epoch 22, Batch 200/3907, Loss: 0.386876\n",
      "[2025-05-13 10:29:01,906][training][INFO] - Epoch 22, Batch 300/3907, Loss: 0.396841\n",
      "[2025-05-13 10:29:02,294][training][INFO] - Epoch 22, Batch 400/3907, Loss: 0.391732\n",
      "[2025-05-13 10:29:02,685][training][INFO] - Epoch 22, Batch 500/3907, Loss: 0.385391\n",
      "[2025-05-13 10:29:03,071][training][INFO] - Epoch 22, Batch 600/3907, Loss: 0.386583\n",
      "[2025-05-13 10:29:03,458][training][INFO] - Epoch 22, Batch 700/3907, Loss: 0.393463\n",
      "[2025-05-13 10:29:03,843][training][INFO] - Epoch 22, Batch 800/3907, Loss: 0.389801\n",
      "[2025-05-13 10:29:04,228][training][INFO] - Epoch 22, Batch 900/3907, Loss: 0.405827\n",
      "[2025-05-13 10:29:04,623][training][INFO] - Epoch 22, Batch 1000/3907, Loss: 0.406461\n",
      "[2025-05-13 10:29:05,009][training][INFO] - Epoch 22, Batch 1100/3907, Loss: 0.396522\n",
      "[2025-05-13 10:29:05,395][training][INFO] - Epoch 22, Batch 1200/3907, Loss: 0.390397\n",
      "[2025-05-13 10:29:05,779][training][INFO] - Epoch 22, Batch 1300/3907, Loss: 0.389845\n",
      "[2025-05-13 10:29:06,170][training][INFO] - Epoch 22, Batch 1400/3907, Loss: 0.387080\n",
      "[2025-05-13 10:29:06,556][training][INFO] - Epoch 22, Batch 1500/3907, Loss: 0.394467\n",
      "[2025-05-13 10:29:06,945][training][INFO] - Epoch 22, Batch 1600/3907, Loss: 0.392344\n",
      "[2025-05-13 10:29:07,330][training][INFO] - Epoch 22, Batch 1700/3907, Loss: 0.391238\n",
      "[2025-05-13 10:29:07,720][training][INFO] - Epoch 22, Batch 1800/3907, Loss: 0.397438\n",
      "[2025-05-13 10:29:08,110][training][INFO] - Epoch 22, Batch 1900/3907, Loss: 0.398284\n",
      "[2025-05-13 10:29:08,496][training][INFO] - Epoch 22, Batch 2000/3907, Loss: 0.395273\n",
      "[2025-05-13 10:29:08,884][training][INFO] - Epoch 22, Batch 2100/3907, Loss: 0.389296\n",
      "[2025-05-13 10:29:09,273][training][INFO] - Epoch 22, Batch 2200/3907, Loss: 0.389838\n",
      "[2025-05-13 10:29:09,667][training][INFO] - Epoch 22, Batch 2300/3907, Loss: 0.391181\n",
      "[2025-05-13 10:29:10,054][training][INFO] - Epoch 22, Batch 2400/3907, Loss: 0.394262\n",
      "[2025-05-13 10:29:10,443][training][INFO] - Epoch 22, Batch 2500/3907, Loss: 0.391095\n",
      "[2025-05-13 10:29:10,829][training][INFO] - Epoch 22, Batch 2600/3907, Loss: 0.394626\n",
      "[2025-05-13 10:29:11,222][training][INFO] - Epoch 22, Batch 2700/3907, Loss: 0.393579\n",
      "[2025-05-13 10:29:11,612][training][INFO] - Epoch 22, Batch 2800/3907, Loss: 0.398399\n",
      "[2025-05-13 10:29:12,000][training][INFO] - Epoch 22, Batch 2900/3907, Loss: 0.394122\n",
      "[2025-05-13 10:29:12,390][training][INFO] - Epoch 22, Batch 3000/3907, Loss: 0.402552\n",
      "[2025-05-13 10:29:12,777][training][INFO] - Epoch 22, Batch 3100/3907, Loss: 0.390201\n",
      "[2025-05-13 10:29:13,169][training][INFO] - Epoch 22, Batch 3200/3907, Loss: 0.398181\n",
      "[2025-05-13 10:29:13,556][training][INFO] - Epoch 22, Batch 3300/3907, Loss: 0.394538\n",
      "[2025-05-13 10:29:13,944][training][INFO] - Epoch 22, Batch 3400/3907, Loss: 0.396370\n",
      "[2025-05-13 10:29:14,333][training][INFO] - Epoch 22, Batch 3500/3907, Loss: 0.400211\n",
      "[2025-05-13 10:29:14,724][training][INFO] - Epoch 22, Batch 3600/3907, Loss: 0.395913\n",
      "[2025-05-13 10:29:15,112][training][INFO] - Epoch 22, Batch 3700/3907, Loss: 0.395552\n",
      "[2025-05-13 10:29:15,498][training][INFO] - Epoch 22, Batch 3800/3907, Loss: 0.395386\n",
      "[2025-05-13 10:29:15,918][training][INFO] - Epoch 22, Batch 3900/3907, Loss: 0.391548\n",
      "[2025-05-13 10:29:15,935][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:29:15,935][training][INFO] - Epoch 22 complete. Avg Loss: 0.393194\n",
      "[2025-05-13 10:29:15,987][training][INFO] - Epoch 23, Batch 0/3907, Loss: 0.391286\n",
      "[2025-05-13 10:29:16,380][training][INFO] - Epoch 23, Batch 100/3907, Loss: 0.398846\n",
      "[2025-05-13 10:29:16,769][training][INFO] - Epoch 23, Batch 200/3907, Loss: 0.392951\n",
      "[2025-05-13 10:29:17,156][training][INFO] - Epoch 23, Batch 300/3907, Loss: 0.386330\n",
      "[2025-05-13 10:29:17,542][training][INFO] - Epoch 23, Batch 400/3907, Loss: 0.389163\n",
      "[2025-05-13 10:29:17,930][training][INFO] - Epoch 23, Batch 500/3907, Loss: 0.387195\n",
      "[2025-05-13 10:29:18,323][training][INFO] - Epoch 23, Batch 600/3907, Loss: 0.387195\n",
      "[2025-05-13 10:29:18,712][training][INFO] - Epoch 23, Batch 700/3907, Loss: 0.386302\n",
      "[2025-05-13 10:29:19,097][training][INFO] - Epoch 23, Batch 800/3907, Loss: 0.400516\n",
      "[2025-05-13 10:29:19,483][training][INFO] - Epoch 23, Batch 900/3907, Loss: 0.397979\n",
      "[2025-05-13 10:29:19,876][training][INFO] - Epoch 23, Batch 1000/3907, Loss: 0.399454\n",
      "[2025-05-13 10:29:20,262][training][INFO] - Epoch 23, Batch 1100/3907, Loss: 0.388452\n",
      "[2025-05-13 10:29:20,650][training][INFO] - Epoch 23, Batch 1200/3907, Loss: 0.399685\n",
      "[2025-05-13 10:29:21,036][training][INFO] - Epoch 23, Batch 1300/3907, Loss: 0.396208\n",
      "[2025-05-13 10:29:21,427][training][INFO] - Epoch 23, Batch 1400/3907, Loss: 0.382764\n",
      "[2025-05-13 10:29:21,816][training][INFO] - Epoch 23, Batch 1500/3907, Loss: 0.395680\n",
      "[2025-05-13 10:29:22,198][training][INFO] - Epoch 23, Batch 1600/3907, Loss: 0.395532\n",
      "[2025-05-13 10:29:22,588][training][INFO] - Epoch 23, Batch 1700/3907, Loss: 0.393503\n",
      "[2025-05-13 10:29:22,974][training][INFO] - Epoch 23, Batch 1800/3907, Loss: 0.393968\n",
      "[2025-05-13 10:29:23,369][training][INFO] - Epoch 23, Batch 1900/3907, Loss: 0.400540\n",
      "[2025-05-13 10:29:23,758][training][INFO] - Epoch 23, Batch 2000/3907, Loss: 0.399087\n",
      "[2025-05-13 10:29:24,146][training][INFO] - Epoch 23, Batch 2100/3907, Loss: 0.395179\n",
      "[2025-05-13 10:29:24,533][training][INFO] - Epoch 23, Batch 2200/3907, Loss: 0.392889\n",
      "[2025-05-13 10:29:24,925][training][INFO] - Epoch 23, Batch 2300/3907, Loss: 0.394909\n",
      "[2025-05-13 10:29:25,313][training][INFO] - Epoch 23, Batch 2400/3907, Loss: 0.386095\n",
      "[2025-05-13 10:29:25,699][training][INFO] - Epoch 23, Batch 2500/3907, Loss: 0.383631\n",
      "[2025-05-13 10:29:26,090][training][INFO] - Epoch 23, Batch 2600/3907, Loss: 0.396125\n",
      "[2025-05-13 10:29:26,480][training][INFO] - Epoch 23, Batch 2700/3907, Loss: 0.381481\n",
      "[2025-05-13 10:29:26,868][training][INFO] - Epoch 23, Batch 2800/3907, Loss: 0.392144\n",
      "[2025-05-13 10:29:27,255][training][INFO] - Epoch 23, Batch 2900/3907, Loss: 0.398618\n",
      "[2025-05-13 10:29:27,640][training][INFO] - Epoch 23, Batch 3000/3907, Loss: 0.395644\n",
      "[2025-05-13 10:29:28,025][training][INFO] - Epoch 23, Batch 3100/3907, Loss: 0.394506\n",
      "[2025-05-13 10:29:28,417][training][INFO] - Epoch 23, Batch 3200/3907, Loss: 0.399943\n",
      "[2025-05-13 10:29:28,804][training][INFO] - Epoch 23, Batch 3300/3907, Loss: 0.396843\n",
      "[2025-05-13 10:29:29,190][training][INFO] - Epoch 23, Batch 3400/3907, Loss: 0.387298\n",
      "[2025-05-13 10:29:29,575][training][INFO] - Epoch 23, Batch 3500/3907, Loss: 0.385308\n",
      "[2025-05-13 10:29:29,965][training][INFO] - Epoch 23, Batch 3600/3907, Loss: 0.392526\n",
      "[2025-05-13 10:29:30,354][training][INFO] - Epoch 23, Batch 3700/3907, Loss: 0.397885\n",
      "[2025-05-13 10:29:30,748][training][INFO] - Epoch 23, Batch 3800/3907, Loss: 0.401102\n",
      "[2025-05-13 10:29:31,162][training][INFO] - Epoch 23, Batch 3900/3907, Loss: 0.394802\n",
      "[2025-05-13 10:29:31,179][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:29:31,179][training][INFO] - Epoch 23 complete. Avg Loss: 0.392878\n",
      "[2025-05-13 10:29:31,213][training][INFO] - Epoch 24, Batch 0/3907, Loss: 0.390889\n",
      "[2025-05-13 10:29:31,612][training][INFO] - Epoch 24, Batch 100/3907, Loss: 0.388410\n",
      "[2025-05-13 10:29:32,003][training][INFO] - Epoch 24, Batch 200/3907, Loss: 0.396529\n",
      "[2025-05-13 10:29:32,389][training][INFO] - Epoch 24, Batch 300/3907, Loss: 0.385319\n",
      "[2025-05-13 10:29:32,775][training][INFO] - Epoch 24, Batch 400/3907, Loss: 0.391697\n",
      "[2025-05-13 10:29:33,163][training][INFO] - Epoch 24, Batch 500/3907, Loss: 0.392634\n",
      "[2025-05-13 10:29:33,553][training][INFO] - Epoch 24, Batch 600/3907, Loss: 0.396234\n",
      "[2025-05-13 10:29:33,938][training][INFO] - Epoch 24, Batch 700/3907, Loss: 0.389686\n",
      "[2025-05-13 10:29:34,325][training][INFO] - Epoch 24, Batch 800/3907, Loss: 0.390406\n",
      "[2025-05-13 10:29:34,710][training][INFO] - Epoch 24, Batch 900/3907, Loss: 0.382420\n",
      "[2025-05-13 10:29:35,104][training][INFO] - Epoch 24, Batch 1000/3907, Loss: 0.386535\n",
      "[2025-05-13 10:29:35,493][training][INFO] - Epoch 24, Batch 1100/3907, Loss: 0.387950\n",
      "[2025-05-13 10:29:35,882][training][INFO] - Epoch 24, Batch 1200/3907, Loss: 0.399220\n",
      "[2025-05-13 10:29:36,274][training][INFO] - Epoch 24, Batch 1300/3907, Loss: 0.394324\n",
      "[2025-05-13 10:29:36,662][training][INFO] - Epoch 24, Batch 1400/3907, Loss: 0.397046\n",
      "[2025-05-13 10:29:37,055][training][INFO] - Epoch 24, Batch 1500/3907, Loss: 0.389807\n",
      "[2025-05-13 10:29:37,439][training][INFO] - Epoch 24, Batch 1600/3907, Loss: 0.395566\n",
      "[2025-05-13 10:29:37,827][training][INFO] - Epoch 24, Batch 1700/3907, Loss: 0.389959\n",
      "[2025-05-13 10:29:38,217][training][INFO] - Epoch 24, Batch 1800/3907, Loss: 0.395896\n",
      "[2025-05-13 10:29:38,608][training][INFO] - Epoch 24, Batch 1900/3907, Loss: 0.385105\n",
      "[2025-05-13 10:29:38,999][training][INFO] - Epoch 24, Batch 2000/3907, Loss: 0.389721\n",
      "[2025-05-13 10:29:39,387][training][INFO] - Epoch 24, Batch 2100/3907, Loss: 0.392829\n",
      "[2025-05-13 10:29:39,775][training][INFO] - Epoch 24, Batch 2200/3907, Loss: 0.390579\n",
      "[2025-05-13 10:29:40,165][training][INFO] - Epoch 24, Batch 2300/3907, Loss: 0.390437\n",
      "[2025-05-13 10:29:40,555][training][INFO] - Epoch 24, Batch 2400/3907, Loss: 0.392731\n",
      "[2025-05-13 10:29:40,946][training][INFO] - Epoch 24, Batch 2500/3907, Loss: 0.395635\n",
      "[2025-05-13 10:29:41,335][training][INFO] - Epoch 24, Batch 2600/3907, Loss: 0.387931\n",
      "[2025-05-13 10:29:41,722][training][INFO] - Epoch 24, Batch 2700/3907, Loss: 0.394162\n",
      "[2025-05-13 10:29:42,114][training][INFO] - Epoch 24, Batch 2800/3907, Loss: 0.388681\n",
      "[2025-05-13 10:29:42,502][training][INFO] - Epoch 24, Batch 2900/3907, Loss: 0.389007\n",
      "[2025-05-13 10:29:42,893][training][INFO] - Epoch 24, Batch 3000/3907, Loss: 0.403349\n",
      "[2025-05-13 10:29:43,280][training][INFO] - Epoch 24, Batch 3100/3907, Loss: 0.389780\n",
      "[2025-05-13 10:29:43,679][training][INFO] - Epoch 24, Batch 3200/3907, Loss: 0.391868\n",
      "[2025-05-13 10:29:44,072][training][INFO] - Epoch 24, Batch 3300/3907, Loss: 0.392430\n",
      "[2025-05-13 10:29:44,462][training][INFO] - Epoch 24, Batch 3400/3907, Loss: 0.397073\n",
      "[2025-05-13 10:29:44,848][training][INFO] - Epoch 24, Batch 3500/3907, Loss: 0.395939\n",
      "[2025-05-13 10:29:45,235][training][INFO] - Epoch 24, Batch 3600/3907, Loss: 0.391714\n",
      "[2025-05-13 10:29:45,634][training][INFO] - Epoch 24, Batch 3700/3907, Loss: 0.386369\n",
      "[2025-05-13 10:29:46,022][training][INFO] - Epoch 24, Batch 3800/3907, Loss: 0.392755\n",
      "[2025-05-13 10:29:46,431][training][INFO] - Epoch 24, Batch 3900/3907, Loss: 0.390836\n",
      "[2025-05-13 10:29:46,447][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:29:46,447][training][INFO] - Epoch 24 complete. Avg Loss: 0.392356\n",
      "[2025-05-13 10:29:46,485][training][INFO] - Epoch 25, Batch 0/3907, Loss: 0.397985\n",
      "[2025-05-13 10:29:46,885][training][INFO] - Epoch 25, Batch 100/3907, Loss: 0.386162\n",
      "[2025-05-13 10:29:47,276][training][INFO] - Epoch 25, Batch 200/3907, Loss: 0.391837\n",
      "[2025-05-13 10:29:47,664][training][INFO] - Epoch 25, Batch 300/3907, Loss: 0.398839\n",
      "[2025-05-13 10:29:48,048][training][INFO] - Epoch 25, Batch 400/3907, Loss: 0.391896\n",
      "[2025-05-13 10:29:48,435][training][INFO] - Epoch 25, Batch 500/3907, Loss: 0.391697\n",
      "[2025-05-13 10:29:48,828][training][INFO] - Epoch 25, Batch 600/3907, Loss: 0.402231\n",
      "[2025-05-13 10:29:49,219][training][INFO] - Epoch 25, Batch 700/3907, Loss: 0.382244\n",
      "[2025-05-13 10:29:49,602][training][INFO] - Epoch 25, Batch 800/3907, Loss: 0.385395\n",
      "[2025-05-13 10:29:49,990][training][INFO] - Epoch 25, Batch 900/3907, Loss: 0.395274\n",
      "[2025-05-13 10:29:50,379][training][INFO] - Epoch 25, Batch 1000/3907, Loss: 0.387918\n",
      "[2025-05-13 10:29:50,775][training][INFO] - Epoch 25, Batch 1100/3907, Loss: 0.397270\n",
      "[2025-05-13 10:29:51,165][training][INFO] - Epoch 25, Batch 1200/3907, Loss: 0.390535\n",
      "[2025-05-13 10:29:51,555][training][INFO] - Epoch 25, Batch 1300/3907, Loss: 0.382817\n",
      "[2025-05-13 10:29:51,952][training][INFO] - Epoch 25, Batch 1400/3907, Loss: 0.392603\n",
      "[2025-05-13 10:29:52,352][training][INFO] - Epoch 25, Batch 1500/3907, Loss: 0.389898\n",
      "[2025-05-13 10:29:52,743][training][INFO] - Epoch 25, Batch 1600/3907, Loss: 0.390163\n",
      "[2025-05-13 10:29:53,132][training][INFO] - Epoch 25, Batch 1700/3907, Loss: 0.393196\n",
      "[2025-05-13 10:29:53,523][training][INFO] - Epoch 25, Batch 1800/3907, Loss: 0.401410\n",
      "[2025-05-13 10:29:53,920][training][INFO] - Epoch 25, Batch 1900/3907, Loss: 0.386877\n",
      "[2025-05-13 10:29:54,306][training][INFO] - Epoch 25, Batch 2000/3907, Loss: 0.388483\n",
      "[2025-05-13 10:29:54,691][training][INFO] - Epoch 25, Batch 2100/3907, Loss: 0.398245\n",
      "[2025-05-13 10:29:55,080][training][INFO] - Epoch 25, Batch 2200/3907, Loss: 0.386303\n",
      "[2025-05-13 10:29:55,465][training][INFO] - Epoch 25, Batch 2300/3907, Loss: 0.384166\n",
      "[2025-05-13 10:29:55,859][training][INFO] - Epoch 25, Batch 2400/3907, Loss: 0.388611\n",
      "[2025-05-13 10:29:56,255][training][INFO] - Epoch 25, Batch 2500/3907, Loss: 0.394575\n",
      "[2025-05-13 10:29:56,648][training][INFO] - Epoch 25, Batch 2600/3907, Loss: 0.386403\n",
      "[2025-05-13 10:29:57,037][training][INFO] - Epoch 25, Batch 2700/3907, Loss: 0.398044\n",
      "[2025-05-13 10:29:57,432][training][INFO] - Epoch 25, Batch 2800/3907, Loss: 0.401378\n",
      "[2025-05-13 10:29:57,817][training][INFO] - Epoch 25, Batch 2900/3907, Loss: 0.380363\n",
      "[2025-05-13 10:29:58,206][training][INFO] - Epoch 25, Batch 3000/3907, Loss: 0.394010\n",
      "[2025-05-13 10:29:58,592][training][INFO] - Epoch 25, Batch 3100/3907, Loss: 0.393521\n",
      "[2025-05-13 10:29:58,980][training][INFO] - Epoch 25, Batch 3200/3907, Loss: 0.386794\n",
      "[2025-05-13 10:29:59,373][training][INFO] - Epoch 25, Batch 3300/3907, Loss: 0.397789\n",
      "[2025-05-13 10:29:59,763][training][INFO] - Epoch 25, Batch 3400/3907, Loss: 0.385134\n",
      "[2025-05-13 10:30:00,150][training][INFO] - Epoch 25, Batch 3500/3907, Loss: 0.392097\n",
      "[2025-05-13 10:30:00,533][training][INFO] - Epoch 25, Batch 3600/3907, Loss: 0.393493\n",
      "[2025-05-13 10:30:00,931][training][INFO] - Epoch 25, Batch 3700/3907, Loss: 0.396079\n",
      "[2025-05-13 10:30:01,318][training][INFO] - Epoch 25, Batch 3800/3907, Loss: 0.397211\n",
      "[2025-05-13 10:30:01,734][training][INFO] - Epoch 25, Batch 3900/3907, Loss: 0.396954\n",
      "[2025-05-13 10:30:01,751][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:30:01,751][training][INFO] - Epoch 25 complete. Avg Loss: 0.392030\n",
      "[2025-05-13 10:30:01,798][training][INFO] - Epoch 26, Batch 0/3907, Loss: 0.388180\n",
      "[2025-05-13 10:30:02,197][training][INFO] - Epoch 26, Batch 100/3907, Loss: 0.396025\n",
      "[2025-05-13 10:30:02,601][training][INFO] - Epoch 26, Batch 200/3907, Loss: 0.395027\n",
      "[2025-05-13 10:30:03,001][training][INFO] - Epoch 26, Batch 300/3907, Loss: 0.398009\n",
      "[2025-05-13 10:30:03,397][training][INFO] - Epoch 26, Batch 400/3907, Loss: 0.391902\n",
      "[2025-05-13 10:30:03,797][training][INFO] - Epoch 26, Batch 500/3907, Loss: 0.394231\n",
      "[2025-05-13 10:30:04,193][training][INFO] - Epoch 26, Batch 600/3907, Loss: 0.392753\n",
      "[2025-05-13 10:30:04,582][training][INFO] - Epoch 26, Batch 700/3907, Loss: 0.391544\n",
      "[2025-05-13 10:30:04,969][training][INFO] - Epoch 26, Batch 800/3907, Loss: 0.382719\n",
      "[2025-05-13 10:30:05,355][training][INFO] - Epoch 26, Batch 900/3907, Loss: 0.387063\n",
      "[2025-05-13 10:30:05,740][training][INFO] - Epoch 26, Batch 1000/3907, Loss: 0.385858\n",
      "[2025-05-13 10:30:06,135][training][INFO] - Epoch 26, Batch 1100/3907, Loss: 0.384714\n",
      "[2025-05-13 10:30:06,524][training][INFO] - Epoch 26, Batch 1200/3907, Loss: 0.390027\n",
      "[2025-05-13 10:30:06,915][training][INFO] - Epoch 26, Batch 1300/3907, Loss: 0.394042\n",
      "[2025-05-13 10:30:07,301][training][INFO] - Epoch 26, Batch 1400/3907, Loss: 0.396168\n",
      "[2025-05-13 10:30:07,693][training][INFO] - Epoch 26, Batch 1500/3907, Loss: 0.385161\n",
      "[2025-05-13 10:30:08,079][training][INFO] - Epoch 26, Batch 1600/3907, Loss: 0.394519\n",
      "[2025-05-13 10:30:08,467][training][INFO] - Epoch 26, Batch 1700/3907, Loss: 0.387998\n",
      "[2025-05-13 10:30:08,857][training][INFO] - Epoch 26, Batch 1800/3907, Loss: 0.404670\n",
      "[2025-05-13 10:30:09,246][training][INFO] - Epoch 26, Batch 1900/3907, Loss: 0.395945\n",
      "[2025-05-13 10:30:09,637][training][INFO] - Epoch 26, Batch 2000/3907, Loss: 0.394960\n",
      "[2025-05-13 10:30:10,026][training][INFO] - Epoch 26, Batch 2100/3907, Loss: 0.390039\n",
      "[2025-05-13 10:30:10,415][training][INFO] - Epoch 26, Batch 2200/3907, Loss: 0.392032\n",
      "[2025-05-13 10:30:10,802][training][INFO] - Epoch 26, Batch 2300/3907, Loss: 0.392893\n",
      "[2025-05-13 10:30:11,196][training][INFO] - Epoch 26, Batch 2400/3907, Loss: 0.388843\n",
      "[2025-05-13 10:30:11,584][training][INFO] - Epoch 26, Batch 2500/3907, Loss: 0.393006\n",
      "[2025-05-13 10:30:11,979][training][INFO] - Epoch 26, Batch 2600/3907, Loss: 0.395197\n",
      "[2025-05-13 10:30:12,374][training][INFO] - Epoch 26, Batch 2700/3907, Loss: 0.397913\n",
      "[2025-05-13 10:30:12,778][training][INFO] - Epoch 26, Batch 2800/3907, Loss: 0.393958\n",
      "[2025-05-13 10:30:13,177][training][INFO] - Epoch 26, Batch 2900/3907, Loss: 0.394265\n",
      "[2025-05-13 10:30:13,577][training][INFO] - Epoch 26, Batch 3000/3907, Loss: 0.389664\n",
      "[2025-05-13 10:30:13,978][training][INFO] - Epoch 26, Batch 3100/3907, Loss: 0.398028\n",
      "[2025-05-13 10:30:14,382][training][INFO] - Epoch 26, Batch 3200/3907, Loss: 0.385887\n",
      "[2025-05-13 10:30:14,781][training][INFO] - Epoch 26, Batch 3300/3907, Loss: 0.389882\n",
      "[2025-05-13 10:30:15,181][training][INFO] - Epoch 26, Batch 3400/3907, Loss: 0.396998\n",
      "[2025-05-13 10:30:15,581][training][INFO] - Epoch 26, Batch 3500/3907, Loss: 0.389604\n",
      "[2025-05-13 10:30:15,992][training][INFO] - Epoch 26, Batch 3600/3907, Loss: 0.386804\n",
      "[2025-05-13 10:30:16,398][training][INFO] - Epoch 26, Batch 3700/3907, Loss: 0.387570\n",
      "[2025-05-13 10:30:16,803][training][INFO] - Epoch 26, Batch 3800/3907, Loss: 0.395671\n",
      "[2025-05-13 10:30:17,226][training][INFO] - Epoch 26, Batch 3900/3907, Loss: 0.393913\n",
      "[2025-05-13 10:30:17,243][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:30:17,243][training][INFO] - Epoch 26 complete. Avg Loss: 0.391769\n",
      "[2025-05-13 10:30:17,291][training][INFO] - Epoch 27, Batch 0/3907, Loss: 0.391283\n",
      "[2025-05-13 10:30:17,700][training][INFO] - Epoch 27, Batch 100/3907, Loss: 0.397300\n",
      "[2025-05-13 10:30:18,105][training][INFO] - Epoch 27, Batch 200/3907, Loss: 0.397505\n",
      "[2025-05-13 10:30:18,508][training][INFO] - Epoch 27, Batch 300/3907, Loss: 0.388331\n",
      "[2025-05-13 10:30:18,911][training][INFO] - Epoch 27, Batch 400/3907, Loss: 0.388629\n",
      "[2025-05-13 10:30:19,314][training][INFO] - Epoch 27, Batch 500/3907, Loss: 0.386327\n",
      "[2025-05-13 10:30:19,719][training][INFO] - Epoch 27, Batch 600/3907, Loss: 0.394327\n",
      "[2025-05-13 10:30:20,121][training][INFO] - Epoch 27, Batch 700/3907, Loss: 0.389855\n",
      "[2025-05-13 10:30:20,525][training][INFO] - Epoch 27, Batch 800/3907, Loss: 0.392699\n",
      "[2025-05-13 10:30:20,925][training][INFO] - Epoch 27, Batch 900/3907, Loss: 0.394788\n",
      "[2025-05-13 10:30:21,328][training][INFO] - Epoch 27, Batch 1000/3907, Loss: 0.392432\n",
      "[2025-05-13 10:30:21,728][training][INFO] - Epoch 27, Batch 1100/3907, Loss: 0.397348\n",
      "[2025-05-13 10:30:22,131][training][INFO] - Epoch 27, Batch 1200/3907, Loss: 0.407653\n",
      "[2025-05-13 10:30:22,531][training][INFO] - Epoch 27, Batch 1300/3907, Loss: 0.389660\n",
      "[2025-05-13 10:30:22,937][training][INFO] - Epoch 27, Batch 1400/3907, Loss: 0.389277\n",
      "[2025-05-13 10:30:23,338][training][INFO] - Epoch 27, Batch 1500/3907, Loss: 0.394975\n",
      "[2025-05-13 10:30:23,742][training][INFO] - Epoch 27, Batch 1600/3907, Loss: 0.385939\n",
      "[2025-05-13 10:30:24,144][training][INFO] - Epoch 27, Batch 1700/3907, Loss: 0.402506\n",
      "[2025-05-13 10:30:24,546][training][INFO] - Epoch 27, Batch 1800/3907, Loss: 0.397034\n",
      "[2025-05-13 10:30:24,949][training][INFO] - Epoch 27, Batch 1900/3907, Loss: 0.386526\n",
      "[2025-05-13 10:30:25,350][training][INFO] - Epoch 27, Batch 2000/3907, Loss: 0.388725\n",
      "[2025-05-13 10:30:25,751][training][INFO] - Epoch 27, Batch 2100/3907, Loss: 0.383844\n",
      "[2025-05-13 10:30:26,151][training][INFO] - Epoch 27, Batch 2200/3907, Loss: 0.388581\n",
      "[2025-05-13 10:30:26,556][training][INFO] - Epoch 27, Batch 2300/3907, Loss: 0.390949\n",
      "[2025-05-13 10:30:26,957][training][INFO] - Epoch 27, Batch 2400/3907, Loss: 0.386221\n",
      "[2025-05-13 10:30:27,359][training][INFO] - Epoch 27, Batch 2500/3907, Loss: 0.394094\n",
      "[2025-05-13 10:30:27,760][training][INFO] - Epoch 27, Batch 2600/3907, Loss: 0.387547\n",
      "[2025-05-13 10:30:28,167][training][INFO] - Epoch 27, Batch 2700/3907, Loss: 0.392615\n",
      "[2025-05-13 10:30:28,571][training][INFO] - Epoch 27, Batch 2800/3907, Loss: 0.397691\n",
      "[2025-05-13 10:30:28,977][training][INFO] - Epoch 27, Batch 2900/3907, Loss: 0.392605\n",
      "[2025-05-13 10:30:29,382][training][INFO] - Epoch 27, Batch 3000/3907, Loss: 0.383491\n",
      "[2025-05-13 10:30:29,788][training][INFO] - Epoch 27, Batch 3100/3907, Loss: 0.387299\n",
      "[2025-05-13 10:30:30,191][training][INFO] - Epoch 27, Batch 3200/3907, Loss: 0.390986\n",
      "[2025-05-13 10:30:30,596][training][INFO] - Epoch 27, Batch 3300/3907, Loss: 0.396117\n",
      "[2025-05-13 10:30:31,001][training][INFO] - Epoch 27, Batch 3400/3907, Loss: 0.391141\n",
      "[2025-05-13 10:30:31,405][training][INFO] - Epoch 27, Batch 3500/3907, Loss: 0.378981\n",
      "[2025-05-13 10:30:31,808][training][INFO] - Epoch 27, Batch 3600/3907, Loss: 0.389164\n",
      "[2025-05-13 10:30:32,210][training][INFO] - Epoch 27, Batch 3700/3907, Loss: 0.388683\n",
      "[2025-05-13 10:30:32,614][training][INFO] - Epoch 27, Batch 3800/3907, Loss: 0.389925\n",
      "[2025-05-13 10:30:33,042][training][INFO] - Epoch 27, Batch 3900/3907, Loss: 0.386145\n",
      "[2025-05-13 10:30:33,059][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:30:33,060][training][INFO] - Epoch 27 complete. Avg Loss: 0.391384\n",
      "[2025-05-13 10:30:33,108][training][INFO] - Epoch 28, Batch 0/3907, Loss: 0.389557\n",
      "[2025-05-13 10:30:33,518][training][INFO] - Epoch 28, Batch 100/3907, Loss: 0.375403\n",
      "[2025-05-13 10:30:33,919][training][INFO] - Epoch 28, Batch 200/3907, Loss: 0.383884\n",
      "[2025-05-13 10:30:34,320][training][INFO] - Epoch 28, Batch 300/3907, Loss: 0.390771\n",
      "[2025-05-13 10:30:34,721][training][INFO] - Epoch 28, Batch 400/3907, Loss: 0.388383\n",
      "[2025-05-13 10:30:35,124][training][INFO] - Epoch 28, Batch 500/3907, Loss: 0.386219\n",
      "[2025-05-13 10:30:35,526][training][INFO] - Epoch 28, Batch 600/3907, Loss: 0.391070\n",
      "[2025-05-13 10:30:35,927][training][INFO] - Epoch 28, Batch 700/3907, Loss: 0.398327\n",
      "[2025-05-13 10:30:36,331][training][INFO] - Epoch 28, Batch 800/3907, Loss: 0.384088\n",
      "[2025-05-13 10:30:36,735][training][INFO] - Epoch 28, Batch 900/3907, Loss: 0.396477\n",
      "[2025-05-13 10:30:37,136][training][INFO] - Epoch 28, Batch 1000/3907, Loss: 0.392280\n",
      "[2025-05-13 10:30:37,537][training][INFO] - Epoch 28, Batch 1100/3907, Loss: 0.394818\n",
      "[2025-05-13 10:30:37,941][training][INFO] - Epoch 28, Batch 1200/3907, Loss: 0.392606\n",
      "[2025-05-13 10:30:38,346][training][INFO] - Epoch 28, Batch 1300/3907, Loss: 0.398152\n",
      "[2025-05-13 10:30:38,745][training][INFO] - Epoch 28, Batch 1400/3907, Loss: 0.389354\n",
      "[2025-05-13 10:30:39,146][training][INFO] - Epoch 28, Batch 1500/3907, Loss: 0.392770\n",
      "[2025-05-13 10:30:39,548][training][INFO] - Epoch 28, Batch 1600/3907, Loss: 0.397294\n",
      "[2025-05-13 10:30:39,950][training][INFO] - Epoch 28, Batch 1700/3907, Loss: 0.389989\n",
      "[2025-05-13 10:30:40,350][training][INFO] - Epoch 28, Batch 1800/3907, Loss: 0.393512\n",
      "[2025-05-13 10:30:40,751][training][INFO] - Epoch 28, Batch 1900/3907, Loss: 0.392954\n",
      "[2025-05-13 10:30:41,151][training][INFO] - Epoch 28, Batch 2000/3907, Loss: 0.392824\n",
      "[2025-05-13 10:30:41,553][training][INFO] - Epoch 28, Batch 2100/3907, Loss: 0.383499\n",
      "[2025-05-13 10:30:41,957][training][INFO] - Epoch 28, Batch 2200/3907, Loss: 0.384974\n",
      "[2025-05-13 10:30:42,357][training][INFO] - Epoch 28, Batch 2300/3907, Loss: 0.391744\n",
      "[2025-05-13 10:30:42,758][training][INFO] - Epoch 28, Batch 2400/3907, Loss: 0.393478\n",
      "[2025-05-13 10:30:43,161][training][INFO] - Epoch 28, Batch 2500/3907, Loss: 0.394251\n",
      "[2025-05-13 10:30:43,567][training][INFO] - Epoch 28, Batch 2600/3907, Loss: 0.392912\n",
      "[2025-05-13 10:30:43,972][training][INFO] - Epoch 28, Batch 2700/3907, Loss: 0.399539\n",
      "[2025-05-13 10:30:44,376][training][INFO] - Epoch 28, Batch 2800/3907, Loss: 0.392126\n",
      "[2025-05-13 10:30:44,777][training][INFO] - Epoch 28, Batch 2900/3907, Loss: 0.384444\n",
      "[2025-05-13 10:30:45,184][training][INFO] - Epoch 28, Batch 3000/3907, Loss: 0.390542\n",
      "[2025-05-13 10:30:45,589][training][INFO] - Epoch 28, Batch 3100/3907, Loss: 0.392126\n",
      "[2025-05-13 10:30:45,998][training][INFO] - Epoch 28, Batch 3200/3907, Loss: 0.389185\n",
      "[2025-05-13 10:30:46,400][training][INFO] - Epoch 28, Batch 3300/3907, Loss: 0.387441\n",
      "[2025-05-13 10:30:46,802][training][INFO] - Epoch 28, Batch 3400/3907, Loss: 0.388337\n",
      "[2025-05-13 10:30:47,204][training][INFO] - Epoch 28, Batch 3500/3907, Loss: 0.388223\n",
      "[2025-05-13 10:30:47,608][training][INFO] - Epoch 28, Batch 3600/3907, Loss: 0.393362\n",
      "[2025-05-13 10:30:48,009][training][INFO] - Epoch 28, Batch 3700/3907, Loss: 0.394784\n",
      "[2025-05-13 10:30:48,410][training][INFO] - Epoch 28, Batch 3800/3907, Loss: 0.393944\n",
      "[2025-05-13 10:30:48,832][training][INFO] - Epoch 28, Batch 3900/3907, Loss: 0.388440\n",
      "[2025-05-13 10:30:48,849][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:30:48,849][training][INFO] - Epoch 28 complete. Avg Loss: 0.391214\n",
      "[2025-05-13 10:30:48,900][training][INFO] - Epoch 29, Batch 0/3907, Loss: 0.385338\n",
      "[2025-05-13 10:30:49,302][training][INFO] - Epoch 29, Batch 100/3907, Loss: 0.387128\n",
      "[2025-05-13 10:30:49,705][training][INFO] - Epoch 29, Batch 200/3907, Loss: 0.396771\n",
      "[2025-05-13 10:30:50,107][training][INFO] - Epoch 29, Batch 300/3907, Loss: 0.388857\n",
      "[2025-05-13 10:30:50,510][training][INFO] - Epoch 29, Batch 400/3907, Loss: 0.394584\n",
      "[2025-05-13 10:30:50,910][training][INFO] - Epoch 29, Batch 500/3907, Loss: 0.389161\n",
      "[2025-05-13 10:30:51,311][training][INFO] - Epoch 29, Batch 600/3907, Loss: 0.396167\n",
      "[2025-05-13 10:30:51,717][training][INFO] - Epoch 29, Batch 700/3907, Loss: 0.401655\n",
      "[2025-05-13 10:30:52,124][training][INFO] - Epoch 29, Batch 800/3907, Loss: 0.393070\n",
      "[2025-05-13 10:30:52,526][training][INFO] - Epoch 29, Batch 900/3907, Loss: 0.390853\n",
      "[2025-05-13 10:30:52,930][training][INFO] - Epoch 29, Batch 1000/3907, Loss: 0.391977\n",
      "[2025-05-13 10:30:53,335][training][INFO] - Epoch 29, Batch 1100/3907, Loss: 0.389151\n",
      "[2025-05-13 10:30:53,742][training][INFO] - Epoch 29, Batch 1200/3907, Loss: 0.405582\n",
      "[2025-05-13 10:30:54,144][training][INFO] - Epoch 29, Batch 1300/3907, Loss: 0.400257\n",
      "[2025-05-13 10:30:54,547][training][INFO] - Epoch 29, Batch 1400/3907, Loss: 0.391603\n",
      "[2025-05-13 10:30:54,948][training][INFO] - Epoch 29, Batch 1500/3907, Loss: 0.396733\n",
      "[2025-05-13 10:30:55,352][training][INFO] - Epoch 29, Batch 1600/3907, Loss: 0.390529\n",
      "[2025-05-13 10:30:55,752][training][INFO] - Epoch 29, Batch 1700/3907, Loss: 0.387311\n",
      "[2025-05-13 10:30:56,153][training][INFO] - Epoch 29, Batch 1800/3907, Loss: 0.395182\n",
      "[2025-05-13 10:30:56,556][training][INFO] - Epoch 29, Batch 1900/3907, Loss: 0.392649\n",
      "[2025-05-13 10:30:56,958][training][INFO] - Epoch 29, Batch 2000/3907, Loss: 0.393298\n",
      "[2025-05-13 10:30:57,362][training][INFO] - Epoch 29, Batch 2100/3907, Loss: 0.389428\n",
      "[2025-05-13 10:30:57,764][training][INFO] - Epoch 29, Batch 2200/3907, Loss: 0.394312\n",
      "[2025-05-13 10:30:58,165][training][INFO] - Epoch 29, Batch 2300/3907, Loss: 0.389811\n",
      "[2025-05-13 10:30:58,566][training][INFO] - Epoch 29, Batch 2400/3907, Loss: 0.394294\n",
      "[2025-05-13 10:30:58,970][training][INFO] - Epoch 29, Batch 2500/3907, Loss: 0.390432\n",
      "[2025-05-13 10:30:59,371][training][INFO] - Epoch 29, Batch 2600/3907, Loss: 0.391041\n",
      "[2025-05-13 10:30:59,773][training][INFO] - Epoch 29, Batch 2700/3907, Loss: 0.397279\n",
      "[2025-05-13 10:31:00,177][training][INFO] - Epoch 29, Batch 2800/3907, Loss: 0.403248\n",
      "[2025-05-13 10:31:00,585][training][INFO] - Epoch 29, Batch 2900/3907, Loss: 0.385225\n",
      "[2025-05-13 10:31:00,990][training][INFO] - Epoch 29, Batch 3000/3907, Loss: 0.390502\n",
      "[2025-05-13 10:31:01,395][training][INFO] - Epoch 29, Batch 3100/3907, Loss: 0.397520\n",
      "[2025-05-13 10:31:01,799][training][INFO] - Epoch 29, Batch 3200/3907, Loss: 0.385304\n",
      "[2025-05-13 10:31:02,207][training][INFO] - Epoch 29, Batch 3300/3907, Loss: 0.390843\n",
      "[2025-05-13 10:31:02,617][training][INFO] - Epoch 29, Batch 3400/3907, Loss: 0.395518\n",
      "[2025-05-13 10:31:03,021][training][INFO] - Epoch 29, Batch 3500/3907, Loss: 0.392404\n",
      "[2025-05-13 10:31:03,424][training][INFO] - Epoch 29, Batch 3600/3907, Loss: 0.386396\n",
      "[2025-05-13 10:31:03,827][training][INFO] - Epoch 29, Batch 3700/3907, Loss: 0.390520\n",
      "[2025-05-13 10:31:04,231][training][INFO] - Epoch 29, Batch 3800/3907, Loss: 0.384725\n",
      "[2025-05-13 10:31:04,657][training][INFO] - Epoch 29, Batch 3900/3907, Loss: 0.393788\n",
      "[2025-05-13 10:31:04,673][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:31:04,674][training][INFO] - Epoch 29 complete. Avg Loss: 0.390840\n",
      "[2025-05-13 10:31:04,725][training][INFO] - Epoch 30, Batch 0/3907, Loss: 0.395734\n",
      "[2025-05-13 10:31:05,127][training][INFO] - Epoch 30, Batch 100/3907, Loss: 0.392865\n",
      "[2025-05-13 10:31:05,527][training][INFO] - Epoch 30, Batch 200/3907, Loss: 0.394642\n",
      "[2025-05-13 10:31:05,932][training][INFO] - Epoch 30, Batch 300/3907, Loss: 0.392897\n",
      "[2025-05-13 10:31:06,335][training][INFO] - Epoch 30, Batch 400/3907, Loss: 0.384158\n",
      "[2025-05-13 10:31:06,740][training][INFO] - Epoch 30, Batch 500/3907, Loss: 0.388027\n",
      "[2025-05-13 10:31:07,142][training][INFO] - Epoch 30, Batch 600/3907, Loss: 0.391694\n",
      "[2025-05-13 10:31:07,547][training][INFO] - Epoch 30, Batch 700/3907, Loss: 0.392430\n",
      "[2025-05-13 10:31:07,949][training][INFO] - Epoch 30, Batch 800/3907, Loss: 0.396723\n",
      "[2025-05-13 10:31:08,351][training][INFO] - Epoch 30, Batch 900/3907, Loss: 0.396113\n",
      "[2025-05-13 10:31:08,753][training][INFO] - Epoch 30, Batch 1000/3907, Loss: 0.398352\n",
      "[2025-05-13 10:31:09,157][training][INFO] - Epoch 30, Batch 1100/3907, Loss: 0.393933\n",
      "[2025-05-13 10:31:09,559][training][INFO] - Epoch 30, Batch 1200/3907, Loss: 0.392928\n",
      "[2025-05-13 10:31:09,962][training][INFO] - Epoch 30, Batch 1300/3907, Loss: 0.399514\n",
      "[2025-05-13 10:31:10,364][training][INFO] - Epoch 30, Batch 1400/3907, Loss: 0.390088\n",
      "[2025-05-13 10:31:10,770][training][INFO] - Epoch 30, Batch 1500/3907, Loss: 0.388777\n",
      "[2025-05-13 10:31:11,170][training][INFO] - Epoch 30, Batch 1600/3907, Loss: 0.385956\n",
      "[2025-05-13 10:31:11,572][training][INFO] - Epoch 30, Batch 1700/3907, Loss: 0.385664\n",
      "[2025-05-13 10:31:11,972][training][INFO] - Epoch 30, Batch 1800/3907, Loss: 0.379707\n",
      "[2025-05-13 10:31:12,373][training][INFO] - Epoch 30, Batch 1900/3907, Loss: 0.378605\n",
      "[2025-05-13 10:31:12,778][training][INFO] - Epoch 30, Batch 2000/3907, Loss: 0.384560\n",
      "[2025-05-13 10:31:13,182][training][INFO] - Epoch 30, Batch 2100/3907, Loss: 0.390431\n",
      "[2025-05-13 10:31:13,584][training][INFO] - Epoch 30, Batch 2200/3907, Loss: 0.387276\n",
      "[2025-05-13 10:31:13,986][training][INFO] - Epoch 30, Batch 2300/3907, Loss: 0.387252\n",
      "[2025-05-13 10:31:14,390][training][INFO] - Epoch 30, Batch 2400/3907, Loss: 0.389581\n",
      "[2025-05-13 10:31:14,790][training][INFO] - Epoch 30, Batch 2500/3907, Loss: 0.387757\n",
      "[2025-05-13 10:31:15,194][training][INFO] - Epoch 30, Batch 2600/3907, Loss: 0.385923\n",
      "[2025-05-13 10:31:15,603][training][INFO] - Epoch 30, Batch 2700/3907, Loss: 0.397520\n",
      "[2025-05-13 10:31:16,011][training][INFO] - Epoch 30, Batch 2800/3907, Loss: 0.389570\n",
      "[2025-05-13 10:31:16,415][training][INFO] - Epoch 30, Batch 2900/3907, Loss: 0.389236\n",
      "[2025-05-13 10:31:16,819][training][INFO] - Epoch 30, Batch 3000/3907, Loss: 0.389763\n",
      "[2025-05-13 10:31:17,223][training][INFO] - Epoch 30, Batch 3100/3907, Loss: 0.391005\n",
      "[2025-05-13 10:31:17,629][training][INFO] - Epoch 30, Batch 3200/3907, Loss: 0.387392\n",
      "[2025-05-13 10:31:18,033][training][INFO] - Epoch 30, Batch 3300/3907, Loss: 0.403227\n",
      "[2025-05-13 10:31:18,433][training][INFO] - Epoch 30, Batch 3400/3907, Loss: 0.384186\n",
      "[2025-05-13 10:31:18,835][training][INFO] - Epoch 30, Batch 3500/3907, Loss: 0.390594\n",
      "[2025-05-13 10:31:19,235][training][INFO] - Epoch 30, Batch 3600/3907, Loss: 0.394324\n",
      "[2025-05-13 10:31:19,640][training][INFO] - Epoch 30, Batch 3700/3907, Loss: 0.393395\n",
      "[2025-05-13 10:31:20,043][training][INFO] - Epoch 30, Batch 3800/3907, Loss: 0.390712\n",
      "[2025-05-13 10:31:20,464][training][INFO] - Epoch 30, Batch 3900/3907, Loss: 0.391511\n",
      "[2025-05-13 10:31:20,481][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:31:20,481][training][INFO] - Epoch 30 complete. Avg Loss: 0.390506\n",
      "[2025-05-13 10:31:20,528][training][INFO] - Epoch 31, Batch 0/3907, Loss: 0.391703\n",
      "[2025-05-13 10:31:20,932][training][INFO] - Epoch 31, Batch 100/3907, Loss: 0.388894\n",
      "[2025-05-13 10:31:21,335][training][INFO] - Epoch 31, Batch 200/3907, Loss: 0.386268\n",
      "[2025-05-13 10:31:21,735][training][INFO] - Epoch 31, Batch 300/3907, Loss: 0.394073\n",
      "[2025-05-13 10:31:22,137][training][INFO] - Epoch 31, Batch 400/3907, Loss: 0.395167\n",
      "[2025-05-13 10:31:22,538][training][INFO] - Epoch 31, Batch 500/3907, Loss: 0.395981\n",
      "[2025-05-13 10:31:22,943][training][INFO] - Epoch 31, Batch 600/3907, Loss: 0.389842\n",
      "[2025-05-13 10:31:23,344][training][INFO] - Epoch 31, Batch 700/3907, Loss: 0.390190\n",
      "[2025-05-13 10:31:23,744][training][INFO] - Epoch 31, Batch 800/3907, Loss: 0.387257\n",
      "[2025-05-13 10:31:24,144][training][INFO] - Epoch 31, Batch 900/3907, Loss: 0.391510\n",
      "[2025-05-13 10:31:24,548][training][INFO] - Epoch 31, Batch 1000/3907, Loss: 0.396758\n",
      "[2025-05-13 10:31:24,947][training][INFO] - Epoch 31, Batch 1100/3907, Loss: 0.386334\n",
      "[2025-05-13 10:31:25,348][training][INFO] - Epoch 31, Batch 1200/3907, Loss: 0.392906\n",
      "[2025-05-13 10:31:25,748][training][INFO] - Epoch 31, Batch 1300/3907, Loss: 0.387782\n",
      "[2025-05-13 10:31:26,152][training][INFO] - Epoch 31, Batch 1400/3907, Loss: 0.381586\n",
      "[2025-05-13 10:31:26,554][training][INFO] - Epoch 31, Batch 1500/3907, Loss: 0.381330\n",
      "[2025-05-13 10:31:26,953][training][INFO] - Epoch 31, Batch 1600/3907, Loss: 0.389595\n",
      "[2025-05-13 10:31:27,355][training][INFO] - Epoch 31, Batch 1700/3907, Loss: 0.389677\n",
      "[2025-05-13 10:31:27,755][training][INFO] - Epoch 31, Batch 1800/3907, Loss: 0.391666\n",
      "[2025-05-13 10:31:28,162][training][INFO] - Epoch 31, Batch 1900/3907, Loss: 0.391845\n",
      "[2025-05-13 10:31:28,564][training][INFO] - Epoch 31, Batch 2000/3907, Loss: 0.391571\n",
      "[2025-05-13 10:31:28,966][training][INFO] - Epoch 31, Batch 2100/3907, Loss: 0.399722\n",
      "[2025-05-13 10:31:29,368][training][INFO] - Epoch 31, Batch 2200/3907, Loss: 0.388907\n",
      "[2025-05-13 10:31:29,773][training][INFO] - Epoch 31, Batch 2300/3907, Loss: 0.383831\n",
      "[2025-05-13 10:31:30,173][training][INFO] - Epoch 31, Batch 2400/3907, Loss: 0.386924\n",
      "[2025-05-13 10:31:30,577][training][INFO] - Epoch 31, Batch 2500/3907, Loss: 0.393500\n",
      "[2025-05-13 10:31:30,984][training][INFO] - Epoch 31, Batch 2600/3907, Loss: 0.384954\n",
      "[2025-05-13 10:31:31,390][training][INFO] - Epoch 31, Batch 2700/3907, Loss: 0.389634\n",
      "[2025-05-13 10:31:31,792][training][INFO] - Epoch 31, Batch 2800/3907, Loss: 0.379646\n",
      "[2025-05-13 10:31:32,193][training][INFO] - Epoch 31, Batch 2900/3907, Loss: 0.400627\n",
      "[2025-05-13 10:31:32,594][training][INFO] - Epoch 31, Batch 3000/3907, Loss: 0.383915\n",
      "[2025-05-13 10:31:32,998][training][INFO] - Epoch 31, Batch 3100/3907, Loss: 0.378477\n",
      "[2025-05-13 10:31:33,400][training][INFO] - Epoch 31, Batch 3200/3907, Loss: 0.397840\n",
      "[2025-05-13 10:31:33,801][training][INFO] - Epoch 31, Batch 3300/3907, Loss: 0.390511\n",
      "[2025-05-13 10:31:34,203][training][INFO] - Epoch 31, Batch 3400/3907, Loss: 0.396925\n",
      "[2025-05-13 10:31:34,607][training][INFO] - Epoch 31, Batch 3500/3907, Loss: 0.394916\n",
      "[2025-05-13 10:31:35,011][training][INFO] - Epoch 31, Batch 3600/3907, Loss: 0.387934\n",
      "[2025-05-13 10:31:35,413][training][INFO] - Epoch 31, Batch 3700/3907, Loss: 0.390104\n",
      "[2025-05-13 10:31:35,814][training][INFO] - Epoch 31, Batch 3800/3907, Loss: 0.382765\n",
      "[2025-05-13 10:31:36,237][training][INFO] - Epoch 31, Batch 3900/3907, Loss: 0.385463\n",
      "[2025-05-13 10:31:36,253][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:31:36,254][training][INFO] - Epoch 31 complete. Avg Loss: 0.390281\n",
      "[2025-05-13 10:31:36,302][training][INFO] - Epoch 32, Batch 0/3907, Loss: 0.390637\n",
      "[2025-05-13 10:31:36,712][training][INFO] - Epoch 32, Batch 100/3907, Loss: 0.387515\n",
      "[2025-05-13 10:31:37,114][training][INFO] - Epoch 32, Batch 200/3907, Loss: 0.380397\n",
      "[2025-05-13 10:31:37,517][training][INFO] - Epoch 32, Batch 300/3907, Loss: 0.392479\n",
      "[2025-05-13 10:31:37,920][training][INFO] - Epoch 32, Batch 400/3907, Loss: 0.396977\n",
      "[2025-05-13 10:31:38,323][training][INFO] - Epoch 32, Batch 500/3907, Loss: 0.388277\n",
      "[2025-05-13 10:31:38,724][training][INFO] - Epoch 32, Batch 600/3907, Loss: 0.393698\n",
      "[2025-05-13 10:31:39,128][training][INFO] - Epoch 32, Batch 700/3907, Loss: 0.386399\n",
      "[2025-05-13 10:31:39,528][training][INFO] - Epoch 32, Batch 800/3907, Loss: 0.393943\n",
      "[2025-05-13 10:31:39,933][training][INFO] - Epoch 32, Batch 900/3907, Loss: 0.388142\n",
      "[2025-05-13 10:31:40,332][training][INFO] - Epoch 32, Batch 1000/3907, Loss: 0.397003\n",
      "[2025-05-13 10:31:40,732][training][INFO] - Epoch 32, Batch 1100/3907, Loss: 0.386974\n",
      "[2025-05-13 10:31:41,136][training][INFO] - Epoch 32, Batch 1200/3907, Loss: 0.392338\n",
      "[2025-05-13 10:31:41,541][training][INFO] - Epoch 32, Batch 1300/3907, Loss: 0.386455\n",
      "[2025-05-13 10:31:41,941][training][INFO] - Epoch 32, Batch 1400/3907, Loss: 0.385427\n",
      "[2025-05-13 10:31:42,344][training][INFO] - Epoch 32, Batch 1500/3907, Loss: 0.383413\n",
      "[2025-05-13 10:31:42,746][training][INFO] - Epoch 32, Batch 1600/3907, Loss: 0.390255\n",
      "[2025-05-13 10:31:43,150][training][INFO] - Epoch 32, Batch 1700/3907, Loss: 0.392807\n",
      "[2025-05-13 10:31:43,553][training][INFO] - Epoch 32, Batch 1800/3907, Loss: 0.387714\n",
      "[2025-05-13 10:31:43,953][training][INFO] - Epoch 32, Batch 1900/3907, Loss: 0.394184\n",
      "[2025-05-13 10:31:44,352][training][INFO] - Epoch 32, Batch 2000/3907, Loss: 0.392363\n",
      "[2025-05-13 10:31:44,754][training][INFO] - Epoch 32, Batch 2100/3907, Loss: 0.388225\n",
      "[2025-05-13 10:31:45,158][training][INFO] - Epoch 32, Batch 2200/3907, Loss: 0.391416\n",
      "[2025-05-13 10:31:45,557][training][INFO] - Epoch 32, Batch 2300/3907, Loss: 0.391549\n",
      "[2025-05-13 10:31:45,965][training][INFO] - Epoch 32, Batch 2400/3907, Loss: 0.391078\n",
      "[2025-05-13 10:31:46,368][training][INFO] - Epoch 32, Batch 2500/3907, Loss: 0.391909\n",
      "[2025-05-13 10:31:46,772][training][INFO] - Epoch 32, Batch 2600/3907, Loss: 0.389956\n",
      "[2025-05-13 10:31:47,172][training][INFO] - Epoch 32, Batch 2700/3907, Loss: 0.390422\n",
      "[2025-05-13 10:31:47,572][training][INFO] - Epoch 32, Batch 2800/3907, Loss: 0.392535\n",
      "[2025-05-13 10:31:47,973][training][INFO] - Epoch 32, Batch 2900/3907, Loss: 0.395415\n",
      "[2025-05-13 10:31:48,378][training][INFO] - Epoch 32, Batch 3000/3907, Loss: 0.386522\n",
      "[2025-05-13 10:31:48,777][training][INFO] - Epoch 32, Batch 3100/3907, Loss: 0.393003\n",
      "[2025-05-13 10:31:49,178][training][INFO] - Epoch 32, Batch 3200/3907, Loss: 0.394387\n",
      "[2025-05-13 10:31:49,580][training][INFO] - Epoch 32, Batch 3300/3907, Loss: 0.383121\n",
      "[2025-05-13 10:31:49,982][training][INFO] - Epoch 32, Batch 3400/3907, Loss: 0.393150\n",
      "[2025-05-13 10:31:50,386][training][INFO] - Epoch 32, Batch 3500/3907, Loss: 0.391367\n",
      "[2025-05-13 10:31:50,789][training][INFO] - Epoch 32, Batch 3600/3907, Loss: 0.394930\n",
      "[2025-05-13 10:31:51,189][training][INFO] - Epoch 32, Batch 3700/3907, Loss: 0.389286\n",
      "[2025-05-13 10:31:51,591][training][INFO] - Epoch 32, Batch 3800/3907, Loss: 0.394977\n",
      "[2025-05-13 10:31:52,014][training][INFO] - Epoch 32, Batch 3900/3907, Loss: 0.377165\n",
      "[2025-05-13 10:31:52,031][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:31:52,032][training][INFO] - Epoch 32 complete. Avg Loss: 0.390129\n",
      "[2025-05-13 10:31:52,083][training][INFO] - Epoch 33, Batch 0/3907, Loss: 0.393009\n",
      "[2025-05-13 10:31:52,484][training][INFO] - Epoch 33, Batch 100/3907, Loss: 0.396009\n",
      "[2025-05-13 10:31:52,885][training][INFO] - Epoch 33, Batch 200/3907, Loss: 0.394859\n",
      "[2025-05-13 10:31:53,288][training][INFO] - Epoch 33, Batch 300/3907, Loss: 0.393808\n",
      "[2025-05-13 10:31:53,694][training][INFO] - Epoch 33, Batch 400/3907, Loss: 0.388896\n",
      "[2025-05-13 10:31:54,098][training][INFO] - Epoch 33, Batch 500/3907, Loss: 0.384328\n",
      "[2025-05-13 10:31:54,498][training][INFO] - Epoch 33, Batch 600/3907, Loss: 0.388502\n",
      "[2025-05-13 10:31:54,902][training][INFO] - Epoch 33, Batch 700/3907, Loss: 0.396746\n",
      "[2025-05-13 10:31:55,310][training][INFO] - Epoch 33, Batch 800/3907, Loss: 0.386326\n",
      "[2025-05-13 10:31:55,710][training][INFO] - Epoch 33, Batch 900/3907, Loss: 0.388626\n",
      "[2025-05-13 10:31:56,113][training][INFO] - Epoch 33, Batch 1000/3907, Loss: 0.399916\n",
      "[2025-05-13 10:31:56,515][training][INFO] - Epoch 33, Batch 1100/3907, Loss: 0.390348\n",
      "[2025-05-13 10:31:56,918][training][INFO] - Epoch 33, Batch 1200/3907, Loss: 0.390056\n",
      "[2025-05-13 10:31:57,319][training][INFO] - Epoch 33, Batch 1300/3907, Loss: 0.394388\n",
      "[2025-05-13 10:31:57,721][training][INFO] - Epoch 33, Batch 1400/3907, Loss: 0.394053\n",
      "[2025-05-13 10:31:58,124][training][INFO] - Epoch 33, Batch 1500/3907, Loss: 0.387548\n",
      "[2025-05-13 10:31:58,525][training][INFO] - Epoch 33, Batch 1600/3907, Loss: 0.391453\n",
      "[2025-05-13 10:31:58,931][training][INFO] - Epoch 33, Batch 1700/3907, Loss: 0.384962\n",
      "[2025-05-13 10:31:59,332][training][INFO] - Epoch 33, Batch 1800/3907, Loss: 0.384950\n",
      "[2025-05-13 10:31:59,733][training][INFO] - Epoch 33, Batch 1900/3907, Loss: 0.384920\n",
      "[2025-05-13 10:32:00,136][training][INFO] - Epoch 33, Batch 2000/3907, Loss: 0.395663\n",
      "[2025-05-13 10:32:00,540][training][INFO] - Epoch 33, Batch 2100/3907, Loss: 0.395484\n",
      "[2025-05-13 10:32:00,946][training][INFO] - Epoch 33, Batch 2200/3907, Loss: 0.387029\n",
      "[2025-05-13 10:32:01,347][training][INFO] - Epoch 33, Batch 2300/3907, Loss: 0.401555\n",
      "[2025-05-13 10:32:01,747][training][INFO] - Epoch 33, Batch 2400/3907, Loss: 0.405545\n",
      "[2025-05-13 10:32:02,144][training][INFO] - Epoch 33, Batch 2500/3907, Loss: 0.391459\n",
      "[2025-05-13 10:32:02,536][training][INFO] - Epoch 33, Batch 2600/3907, Loss: 0.388531\n",
      "[2025-05-13 10:32:02,926][training][INFO] - Epoch 33, Batch 2700/3907, Loss: 0.390890\n",
      "[2025-05-13 10:32:03,313][training][INFO] - Epoch 33, Batch 2800/3907, Loss: 0.393053\n",
      "[2025-05-13 10:32:03,702][training][INFO] - Epoch 33, Batch 2900/3907, Loss: 0.385069\n",
      "[2025-05-13 10:32:04,097][training][INFO] - Epoch 33, Batch 3000/3907, Loss: 0.389278\n",
      "[2025-05-13 10:32:04,485][training][INFO] - Epoch 33, Batch 3100/3907, Loss: 0.376019\n",
      "[2025-05-13 10:32:04,872][training][INFO] - Epoch 33, Batch 3200/3907, Loss: 0.375082\n",
      "[2025-05-13 10:32:05,255][training][INFO] - Epoch 33, Batch 3300/3907, Loss: 0.391020\n",
      "[2025-05-13 10:32:05,649][training][INFO] - Epoch 33, Batch 3400/3907, Loss: 0.388992\n",
      "[2025-05-13 10:32:06,037][training][INFO] - Epoch 33, Batch 3500/3907, Loss: 0.382916\n",
      "[2025-05-13 10:32:06,424][training][INFO] - Epoch 33, Batch 3600/3907, Loss: 0.387673\n",
      "[2025-05-13 10:32:06,809][training][INFO] - Epoch 33, Batch 3700/3907, Loss: 0.388304\n",
      "[2025-05-13 10:32:07,203][training][INFO] - Epoch 33, Batch 3800/3907, Loss: 0.395177\n",
      "[2025-05-13 10:32:07,612][training][INFO] - Epoch 33, Batch 3900/3907, Loss: 0.400135\n",
      "[2025-05-13 10:32:07,629][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:32:07,629][training][INFO] - Epoch 33 complete. Avg Loss: 0.389824\n",
      "[2025-05-13 10:32:07,674][training][INFO] - Epoch 34, Batch 0/3907, Loss: 0.381195\n",
      "[2025-05-13 10:32:08,064][training][INFO] - Epoch 34, Batch 100/3907, Loss: 0.396611\n",
      "[2025-05-13 10:32:08,449][training][INFO] - Epoch 34, Batch 200/3907, Loss: 0.390640\n",
      "[2025-05-13 10:32:08,838][training][INFO] - Epoch 34, Batch 300/3907, Loss: 0.383577\n",
      "[2025-05-13 10:32:09,225][training][INFO] - Epoch 34, Batch 400/3907, Loss: 0.385178\n",
      "[2025-05-13 10:32:09,615][training][INFO] - Epoch 34, Batch 500/3907, Loss: 0.386167\n",
      "[2025-05-13 10:32:10,003][training][INFO] - Epoch 34, Batch 600/3907, Loss: 0.389176\n",
      "[2025-05-13 10:32:10,389][training][INFO] - Epoch 34, Batch 700/3907, Loss: 0.389422\n",
      "[2025-05-13 10:32:10,780][training][INFO] - Epoch 34, Batch 800/3907, Loss: 0.388900\n",
      "[2025-05-13 10:32:11,169][training][INFO] - Epoch 34, Batch 900/3907, Loss: 0.384394\n",
      "[2025-05-13 10:32:11,553][training][INFO] - Epoch 34, Batch 1000/3907, Loss: 0.390449\n",
      "[2025-05-13 10:32:11,939][training][INFO] - Epoch 34, Batch 1100/3907, Loss: 0.392769\n",
      "[2025-05-13 10:32:12,328][training][INFO] - Epoch 34, Batch 1200/3907, Loss: 0.385840\n",
      "[2025-05-13 10:32:12,716][training][INFO] - Epoch 34, Batch 1300/3907, Loss: 0.392778\n",
      "[2025-05-13 10:32:13,101][training][INFO] - Epoch 34, Batch 1400/3907, Loss: 0.384370\n",
      "[2025-05-13 10:32:13,487][training][INFO] - Epoch 34, Batch 1500/3907, Loss: 0.388834\n",
      "[2025-05-13 10:32:13,876][training][INFO] - Epoch 34, Batch 1600/3907, Loss: 0.395083\n",
      "[2025-05-13 10:32:14,268][training][INFO] - Epoch 34, Batch 1700/3907, Loss: 0.391796\n",
      "[2025-05-13 10:32:14,656][training][INFO] - Epoch 34, Batch 1800/3907, Loss: 0.387160\n",
      "[2025-05-13 10:32:15,042][training][INFO] - Epoch 34, Batch 1900/3907, Loss: 0.399728\n",
      "[2025-05-13 10:32:15,431][training][INFO] - Epoch 34, Batch 2000/3907, Loss: 0.391851\n",
      "[2025-05-13 10:32:15,827][training][INFO] - Epoch 34, Batch 2100/3907, Loss: 0.380781\n",
      "[2025-05-13 10:32:16,219][training][INFO] - Epoch 34, Batch 2200/3907, Loss: 0.393164\n",
      "[2025-05-13 10:32:16,608][training][INFO] - Epoch 34, Batch 2300/3907, Loss: 0.384060\n",
      "[2025-05-13 10:32:16,995][training][INFO] - Epoch 34, Batch 2400/3907, Loss: 0.389359\n",
      "[2025-05-13 10:32:17,386][training][INFO] - Epoch 34, Batch 2500/3907, Loss: 0.386414\n",
      "[2025-05-13 10:32:17,779][training][INFO] - Epoch 34, Batch 2600/3907, Loss: 0.393908\n",
      "[2025-05-13 10:32:18,167][training][INFO] - Epoch 34, Batch 2700/3907, Loss: 0.391889\n",
      "[2025-05-13 10:32:18,552][training][INFO] - Epoch 34, Batch 2800/3907, Loss: 0.384967\n",
      "[2025-05-13 10:32:18,939][training][INFO] - Epoch 34, Batch 2900/3907, Loss: 0.381731\n",
      "[2025-05-13 10:32:19,332][training][INFO] - Epoch 34, Batch 3000/3907, Loss: 0.390912\n",
      "[2025-05-13 10:32:19,719][training][INFO] - Epoch 34, Batch 3100/3907, Loss: 0.385464\n",
      "[2025-05-13 10:32:20,106][training][INFO] - Epoch 34, Batch 3200/3907, Loss: 0.390199\n",
      "[2025-05-13 10:32:20,495][training][INFO] - Epoch 34, Batch 3300/3907, Loss: 0.393517\n",
      "[2025-05-13 10:32:20,882][training][INFO] - Epoch 34, Batch 3400/3907, Loss: 0.392172\n",
      "[2025-05-13 10:32:21,270][training][INFO] - Epoch 34, Batch 3500/3907, Loss: 0.393203\n",
      "[2025-05-13 10:32:21,654][training][INFO] - Epoch 34, Batch 3600/3907, Loss: 0.393754\n",
      "[2025-05-13 10:32:22,041][training][INFO] - Epoch 34, Batch 3700/3907, Loss: 0.385291\n",
      "[2025-05-13 10:32:22,429][training][INFO] - Epoch 34, Batch 3800/3907, Loss: 0.389574\n",
      "[2025-05-13 10:32:22,837][training][INFO] - Epoch 34, Batch 3900/3907, Loss: 0.396609\n",
      "[2025-05-13 10:32:22,854][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:32:22,854][training][INFO] - Epoch 34 complete. Avg Loss: 0.389566\n",
      "[2025-05-13 10:32:22,899][training][INFO] - Epoch 35, Batch 0/3907, Loss: 0.390706\n",
      "[2025-05-13 10:32:23,288][training][INFO] - Epoch 35, Batch 100/3907, Loss: 0.383839\n",
      "[2025-05-13 10:32:23,672][training][INFO] - Epoch 35, Batch 200/3907, Loss: 0.394986\n",
      "[2025-05-13 10:32:24,061][training][INFO] - Epoch 35, Batch 300/3907, Loss: 0.396612\n",
      "[2025-05-13 10:32:24,454][training][INFO] - Epoch 35, Batch 400/3907, Loss: 0.392738\n",
      "[2025-05-13 10:32:24,841][training][INFO] - Epoch 35, Batch 500/3907, Loss: 0.383018\n",
      "[2025-05-13 10:32:25,227][training][INFO] - Epoch 35, Batch 600/3907, Loss: 0.387632\n",
      "[2025-05-13 10:32:25,614][training][INFO] - Epoch 35, Batch 700/3907, Loss: 0.397093\n",
      "[2025-05-13 10:32:26,005][training][INFO] - Epoch 35, Batch 800/3907, Loss: 0.385187\n",
      "[2025-05-13 10:32:26,397][training][INFO] - Epoch 35, Batch 900/3907, Loss: 0.385572\n",
      "[2025-05-13 10:32:26,786][training][INFO] - Epoch 35, Batch 1000/3907, Loss: 0.398217\n",
      "[2025-05-13 10:32:27,174][training][INFO] - Epoch 35, Batch 1100/3907, Loss: 0.394882\n",
      "[2025-05-13 10:32:27,561][training][INFO] - Epoch 35, Batch 1200/3907, Loss: 0.386819\n",
      "[2025-05-13 10:32:27,956][training][INFO] - Epoch 35, Batch 1300/3907, Loss: 0.386248\n",
      "[2025-05-13 10:32:28,345][training][INFO] - Epoch 35, Batch 1400/3907, Loss: 0.387119\n",
      "[2025-05-13 10:32:28,730][training][INFO] - Epoch 35, Batch 1500/3907, Loss: 0.383602\n",
      "[2025-05-13 10:32:29,115][training][INFO] - Epoch 35, Batch 1600/3907, Loss: 0.392986\n",
      "[2025-05-13 10:32:29,506][training][INFO] - Epoch 35, Batch 1700/3907, Loss: 0.386054\n",
      "[2025-05-13 10:32:29,891][training][INFO] - Epoch 35, Batch 1800/3907, Loss: 0.382766\n",
      "[2025-05-13 10:32:30,280][training][INFO] - Epoch 35, Batch 1900/3907, Loss: 0.394614\n",
      "[2025-05-13 10:32:30,671][training][INFO] - Epoch 35, Batch 2000/3907, Loss: 0.401672\n",
      "[2025-05-13 10:32:31,065][training][INFO] - Epoch 35, Batch 2100/3907, Loss: 0.383490\n",
      "[2025-05-13 10:32:31,458][training][INFO] - Epoch 35, Batch 2200/3907, Loss: 0.389512\n",
      "[2025-05-13 10:32:31,843][training][INFO] - Epoch 35, Batch 2300/3907, Loss: 0.381574\n",
      "[2025-05-13 10:32:32,231][training][INFO] - Epoch 35, Batch 2400/3907, Loss: 0.387964\n",
      "[2025-05-13 10:32:32,617][training][INFO] - Epoch 35, Batch 2500/3907, Loss: 0.388495\n",
      "[2025-05-13 10:32:33,012][training][INFO] - Epoch 35, Batch 2600/3907, Loss: 0.392283\n",
      "[2025-05-13 10:32:33,398][training][INFO] - Epoch 35, Batch 2700/3907, Loss: 0.396679\n",
      "[2025-05-13 10:32:33,786][training][INFO] - Epoch 35, Batch 2800/3907, Loss: 0.388549\n",
      "[2025-05-13 10:32:34,176][training][INFO] - Epoch 35, Batch 2900/3907, Loss: 0.391040\n",
      "[2025-05-13 10:32:34,567][training][INFO] - Epoch 35, Batch 3000/3907, Loss: 0.389503\n",
      "[2025-05-13 10:32:34,955][training][INFO] - Epoch 35, Batch 3100/3907, Loss: 0.388723\n",
      "[2025-05-13 10:32:35,345][training][INFO] - Epoch 35, Batch 3200/3907, Loss: 0.392096\n",
      "[2025-05-13 10:32:35,731][training][INFO] - Epoch 35, Batch 3300/3907, Loss: 0.378923\n",
      "[2025-05-13 10:32:36,118][training][INFO] - Epoch 35, Batch 3400/3907, Loss: 0.394872\n",
      "[2025-05-13 10:32:36,512][training][INFO] - Epoch 35, Batch 3500/3907, Loss: 0.383248\n",
      "[2025-05-13 10:32:36,905][training][INFO] - Epoch 35, Batch 3600/3907, Loss: 0.390499\n",
      "[2025-05-13 10:32:37,291][training][INFO] - Epoch 35, Batch 3700/3907, Loss: 0.392398\n",
      "[2025-05-13 10:32:37,678][training][INFO] - Epoch 35, Batch 3800/3907, Loss: 0.393255\n",
      "[2025-05-13 10:32:38,089][training][INFO] - Epoch 35, Batch 3900/3907, Loss: 0.394201\n",
      "[2025-05-13 10:32:38,105][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:32:38,105][training][INFO] - Epoch 35 complete. Avg Loss: 0.389335\n",
      "[2025-05-13 10:32:38,146][training][INFO] - Epoch 36, Batch 0/3907, Loss: 0.395305\n",
      "[2025-05-13 10:32:38,539][training][INFO] - Epoch 36, Batch 100/3907, Loss: 0.395735\n",
      "[2025-05-13 10:32:38,925][training][INFO] - Epoch 36, Batch 200/3907, Loss: 0.393620\n",
      "[2025-05-13 10:32:39,311][training][INFO] - Epoch 36, Batch 300/3907, Loss: 0.389966\n",
      "[2025-05-13 10:32:39,702][training][INFO] - Epoch 36, Batch 400/3907, Loss: 0.405580\n",
      "[2025-05-13 10:32:40,088][training][INFO] - Epoch 36, Batch 500/3907, Loss: 0.388788\n",
      "[2025-05-13 10:32:40,475][training][INFO] - Epoch 36, Batch 600/3907, Loss: 0.388753\n",
      "[2025-05-13 10:32:40,858][training][INFO] - Epoch 36, Batch 700/3907, Loss: 0.394237\n",
      "[2025-05-13 10:32:41,242][training][INFO] - Epoch 36, Batch 800/3907, Loss: 0.392949\n",
      "[2025-05-13 10:32:41,637][training][INFO] - Epoch 36, Batch 900/3907, Loss: 0.397415\n",
      "[2025-05-13 10:32:42,025][training][INFO] - Epoch 36, Batch 1000/3907, Loss: 0.387359\n",
      "[2025-05-13 10:32:42,410][training][INFO] - Epoch 36, Batch 1100/3907, Loss: 0.386004\n",
      "[2025-05-13 10:32:42,795][training][INFO] - Epoch 36, Batch 1200/3907, Loss: 0.388740\n",
      "[2025-05-13 10:32:43,187][training][INFO] - Epoch 36, Batch 1300/3907, Loss: 0.392608\n",
      "[2025-05-13 10:32:43,573][training][INFO] - Epoch 36, Batch 1400/3907, Loss: 0.384101\n",
      "[2025-05-13 10:32:43,959][training][INFO] - Epoch 36, Batch 1500/3907, Loss: 0.388512\n",
      "[2025-05-13 10:32:44,351][training][INFO] - Epoch 36, Batch 1600/3907, Loss: 0.378601\n",
      "[2025-05-13 10:32:44,737][training][INFO] - Epoch 36, Batch 1700/3907, Loss: 0.390164\n",
      "[2025-05-13 10:32:45,126][training][INFO] - Epoch 36, Batch 1800/3907, Loss: 0.392536\n",
      "[2025-05-13 10:32:45,511][training][INFO] - Epoch 36, Batch 1900/3907, Loss: 0.393044\n",
      "[2025-05-13 10:32:45,902][training][INFO] - Epoch 36, Batch 2000/3907, Loss: 0.397329\n",
      "[2025-05-13 10:32:46,293][training][INFO] - Epoch 36, Batch 2100/3907, Loss: 0.372173\n",
      "[2025-05-13 10:32:46,688][training][INFO] - Epoch 36, Batch 2200/3907, Loss: 0.398386\n",
      "[2025-05-13 10:32:47,073][training][INFO] - Epoch 36, Batch 2300/3907, Loss: 0.390764\n",
      "[2025-05-13 10:32:47,474][training][INFO] - Epoch 36, Batch 2400/3907, Loss: 0.393560\n",
      "[2025-05-13 10:32:47,860][training][INFO] - Epoch 36, Batch 2500/3907, Loss: 0.389604\n",
      "[2025-05-13 10:32:48,249][training][INFO] - Epoch 36, Batch 2600/3907, Loss: 0.389085\n",
      "[2025-05-13 10:32:48,638][training][INFO] - Epoch 36, Batch 2700/3907, Loss: 0.392374\n",
      "[2025-05-13 10:32:49,025][training][INFO] - Epoch 36, Batch 2800/3907, Loss: 0.389503\n",
      "[2025-05-13 10:32:49,411][training][INFO] - Epoch 36, Batch 2900/3907, Loss: 0.384409\n",
      "[2025-05-13 10:32:49,797][training][INFO] - Epoch 36, Batch 3000/3907, Loss: 0.390534\n",
      "[2025-05-13 10:32:50,190][training][INFO] - Epoch 36, Batch 3100/3907, Loss: 0.388835\n",
      "[2025-05-13 10:32:50,576][training][INFO] - Epoch 36, Batch 3200/3907, Loss: 0.386995\n",
      "[2025-05-13 10:32:50,963][training][INFO] - Epoch 36, Batch 3300/3907, Loss: 0.381957\n",
      "[2025-05-13 10:32:51,350][training][INFO] - Epoch 36, Batch 3400/3907, Loss: 0.390862\n",
      "[2025-05-13 10:32:51,741][training][INFO] - Epoch 36, Batch 3500/3907, Loss: 0.388738\n",
      "[2025-05-13 10:32:52,130][training][INFO] - Epoch 36, Batch 3600/3907, Loss: 0.393706\n",
      "[2025-05-13 10:32:52,516][training][INFO] - Epoch 36, Batch 3700/3907, Loss: 0.392451\n",
      "[2025-05-13 10:32:52,906][training][INFO] - Epoch 36, Batch 3800/3907, Loss: 0.384421\n",
      "[2025-05-13 10:32:53,313][training][INFO] - Epoch 36, Batch 3900/3907, Loss: 0.388414\n",
      "[2025-05-13 10:32:53,330][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:32:53,330][training][INFO] - Epoch 36 complete. Avg Loss: 0.389084\n",
      "[2025-05-13 10:32:53,374][training][INFO] - Epoch 37, Batch 0/3907, Loss: 0.383356\n",
      "[2025-05-13 10:32:53,768][training][INFO] - Epoch 37, Batch 100/3907, Loss: 0.390618\n",
      "[2025-05-13 10:32:54,154][training][INFO] - Epoch 37, Batch 200/3907, Loss: 0.382973\n",
      "[2025-05-13 10:32:54,541][training][INFO] - Epoch 37, Batch 300/3907, Loss: 0.392824\n",
      "[2025-05-13 10:32:54,928][training][INFO] - Epoch 37, Batch 400/3907, Loss: 0.390699\n",
      "[2025-05-13 10:32:55,321][training][INFO] - Epoch 37, Batch 500/3907, Loss: 0.395378\n",
      "[2025-05-13 10:32:55,709][training][INFO] - Epoch 37, Batch 600/3907, Loss: 0.386971\n",
      "[2025-05-13 10:32:56,093][training][INFO] - Epoch 37, Batch 700/3907, Loss: 0.384884\n",
      "[2025-05-13 10:32:56,486][training][INFO] - Epoch 37, Batch 800/3907, Loss: 0.391479\n",
      "[2025-05-13 10:32:56,876][training][INFO] - Epoch 37, Batch 900/3907, Loss: 0.386006\n",
      "[2025-05-13 10:32:57,263][training][INFO] - Epoch 37, Batch 1000/3907, Loss: 0.399007\n",
      "[2025-05-13 10:32:57,653][training][INFO] - Epoch 37, Batch 1100/3907, Loss: 0.388511\n",
      "[2025-05-13 10:32:58,039][training][INFO] - Epoch 37, Batch 1200/3907, Loss: 0.390294\n",
      "[2025-05-13 10:32:58,430][training][INFO] - Epoch 37, Batch 1300/3907, Loss: 0.393999\n",
      "[2025-05-13 10:32:58,820][training][INFO] - Epoch 37, Batch 1400/3907, Loss: 0.388221\n",
      "[2025-05-13 10:32:59,208][training][INFO] - Epoch 37, Batch 1500/3907, Loss: 0.385808\n",
      "[2025-05-13 10:32:59,595][training][INFO] - Epoch 37, Batch 1600/3907, Loss: 0.391283\n",
      "[2025-05-13 10:32:59,981][training][INFO] - Epoch 37, Batch 1700/3907, Loss: 0.400161\n",
      "[2025-05-13 10:33:00,379][training][INFO] - Epoch 37, Batch 1800/3907, Loss: 0.396895\n",
      "[2025-05-13 10:33:00,771][training][INFO] - Epoch 37, Batch 1900/3907, Loss: 0.398046\n",
      "[2025-05-13 10:33:01,161][training][INFO] - Epoch 37, Batch 2000/3907, Loss: 0.388615\n",
      "[2025-05-13 10:33:01,547][training][INFO] - Epoch 37, Batch 2100/3907, Loss: 0.388509\n",
      "[2025-05-13 10:33:01,940][training][INFO] - Epoch 37, Batch 2200/3907, Loss: 0.388312\n",
      "[2025-05-13 10:33:02,327][training][INFO] - Epoch 37, Batch 2300/3907, Loss: 0.383402\n",
      "[2025-05-13 10:33:02,713][training][INFO] - Epoch 37, Batch 2400/3907, Loss: 0.384478\n",
      "[2025-05-13 10:33:03,097][training][INFO] - Epoch 37, Batch 2500/3907, Loss: 0.390950\n",
      "[2025-05-13 10:33:03,482][training][INFO] - Epoch 37, Batch 2600/3907, Loss: 0.396026\n",
      "[2025-05-13 10:33:03,875][training][INFO] - Epoch 37, Batch 2700/3907, Loss: 0.389483\n",
      "[2025-05-13 10:33:04,262][training][INFO] - Epoch 37, Batch 2800/3907, Loss: 0.387734\n",
      "[2025-05-13 10:33:04,648][training][INFO] - Epoch 37, Batch 2900/3907, Loss: 0.389587\n",
      "[2025-05-13 10:33:05,034][training][INFO] - Epoch 37, Batch 3000/3907, Loss: 0.384733\n",
      "[2025-05-13 10:33:05,427][training][INFO] - Epoch 37, Batch 3100/3907, Loss: 0.387369\n",
      "[2025-05-13 10:33:05,814][training][INFO] - Epoch 37, Batch 3200/3907, Loss: 0.389425\n",
      "[2025-05-13 10:33:06,203][training][INFO] - Epoch 37, Batch 3300/3907, Loss: 0.394613\n",
      "[2025-05-13 10:33:06,589][training][INFO] - Epoch 37, Batch 3400/3907, Loss: 0.380892\n",
      "[2025-05-13 10:33:06,979][training][INFO] - Epoch 37, Batch 3500/3907, Loss: 0.394823\n",
      "[2025-05-13 10:33:07,369][training][INFO] - Epoch 37, Batch 3600/3907, Loss: 0.392796\n",
      "[2025-05-13 10:33:07,755][training][INFO] - Epoch 37, Batch 3700/3907, Loss: 0.388021\n",
      "[2025-05-13 10:33:08,141][training][INFO] - Epoch 37, Batch 3800/3907, Loss: 0.398059\n",
      "[2025-05-13 10:33:08,545][training][INFO] - Epoch 37, Batch 3900/3907, Loss: 0.388808\n",
      "[2025-05-13 10:33:08,561][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:33:08,562][training][INFO] - Epoch 37 complete. Avg Loss: 0.388813\n",
      "[2025-05-13 10:33:08,605][training][INFO] - Epoch 38, Batch 0/3907, Loss: 0.391002\n",
      "[2025-05-13 10:33:08,999][training][INFO] - Epoch 38, Batch 100/3907, Loss: 0.383592\n",
      "[2025-05-13 10:33:09,387][training][INFO] - Epoch 38, Batch 200/3907, Loss: 0.385002\n",
      "[2025-05-13 10:33:09,774][training][INFO] - Epoch 38, Batch 300/3907, Loss: 0.386098\n",
      "[2025-05-13 10:33:10,159][training][INFO] - Epoch 38, Batch 400/3907, Loss: 0.381533\n",
      "[2025-05-13 10:33:10,550][training][INFO] - Epoch 38, Batch 500/3907, Loss: 0.385656\n",
      "[2025-05-13 10:33:10,935][training][INFO] - Epoch 38, Batch 600/3907, Loss: 0.385537\n",
      "[2025-05-13 10:33:11,322][training][INFO] - Epoch 38, Batch 700/3907, Loss: 0.389281\n",
      "[2025-05-13 10:33:11,709][training][INFO] - Epoch 38, Batch 800/3907, Loss: 0.391202\n",
      "[2025-05-13 10:33:12,094][training][INFO] - Epoch 38, Batch 900/3907, Loss: 0.385309\n",
      "[2025-05-13 10:33:12,485][training][INFO] - Epoch 38, Batch 1000/3907, Loss: 0.391868\n",
      "[2025-05-13 10:33:12,869][training][INFO] - Epoch 38, Batch 1100/3907, Loss: 0.391423\n",
      "[2025-05-13 10:33:13,257][training][INFO] - Epoch 38, Batch 1200/3907, Loss: 0.381573\n",
      "[2025-05-13 10:33:13,646][training][INFO] - Epoch 38, Batch 1300/3907, Loss: 0.402239\n",
      "[2025-05-13 10:33:14,038][training][INFO] - Epoch 38, Batch 1400/3907, Loss: 0.390557\n",
      "[2025-05-13 10:33:14,426][training][INFO] - Epoch 38, Batch 1500/3907, Loss: 0.378822\n",
      "[2025-05-13 10:33:14,814][training][INFO] - Epoch 38, Batch 1600/3907, Loss: 0.394205\n",
      "[2025-05-13 10:33:15,208][training][INFO] - Epoch 38, Batch 1700/3907, Loss: 0.394966\n",
      "[2025-05-13 10:33:15,609][training][INFO] - Epoch 38, Batch 1800/3907, Loss: 0.391306\n",
      "[2025-05-13 10:33:16,000][training][INFO] - Epoch 38, Batch 1900/3907, Loss: 0.379332\n",
      "[2025-05-13 10:33:16,389][training][INFO] - Epoch 38, Batch 2000/3907, Loss: 0.390481\n",
      "[2025-05-13 10:33:16,774][training][INFO] - Epoch 38, Batch 2100/3907, Loss: 0.393341\n",
      "[2025-05-13 10:33:17,161][training][INFO] - Epoch 38, Batch 2200/3907, Loss: 0.383688\n",
      "[2025-05-13 10:33:17,556][training][INFO] - Epoch 38, Batch 2300/3907, Loss: 0.392089\n",
      "[2025-05-13 10:33:17,941][training][INFO] - Epoch 38, Batch 2400/3907, Loss: 0.386996\n",
      "[2025-05-13 10:33:18,327][training][INFO] - Epoch 38, Batch 2500/3907, Loss: 0.388170\n",
      "[2025-05-13 10:33:18,719][training][INFO] - Epoch 38, Batch 2600/3907, Loss: 0.392845\n",
      "[2025-05-13 10:33:19,118][training][INFO] - Epoch 38, Batch 2700/3907, Loss: 0.388680\n",
      "[2025-05-13 10:33:19,513][training][INFO] - Epoch 38, Batch 2800/3907, Loss: 0.390352\n",
      "[2025-05-13 10:33:19,905][training][INFO] - Epoch 38, Batch 2900/3907, Loss: 0.393872\n",
      "[2025-05-13 10:33:20,301][training][INFO] - Epoch 38, Batch 3000/3907, Loss: 0.385523\n",
      "[2025-05-13 10:33:20,701][training][INFO] - Epoch 38, Batch 3100/3907, Loss: 0.392348\n",
      "[2025-05-13 10:33:21,096][training][INFO] - Epoch 38, Batch 3200/3907, Loss: 0.392121\n",
      "[2025-05-13 10:33:21,488][training][INFO] - Epoch 38, Batch 3300/3907, Loss: 0.395434\n",
      "[2025-05-13 10:33:21,878][training][INFO] - Epoch 38, Batch 3400/3907, Loss: 0.396150\n",
      "[2025-05-13 10:33:22,269][training][INFO] - Epoch 38, Batch 3500/3907, Loss: 0.383715\n",
      "[2025-05-13 10:33:22,668][training][INFO] - Epoch 38, Batch 3600/3907, Loss: 0.386237\n",
      "[2025-05-13 10:33:23,057][training][INFO] - Epoch 38, Batch 3700/3907, Loss: 0.381672\n",
      "[2025-05-13 10:33:23,442][training][INFO] - Epoch 38, Batch 3800/3907, Loss: 0.384038\n",
      "[2025-05-13 10:33:23,850][training][INFO] - Epoch 38, Batch 3900/3907, Loss: 0.388896\n",
      "[2025-05-13 10:33:23,867][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:33:23,867][training][INFO] - Epoch 38 complete. Avg Loss: 0.388520\n",
      "[2025-05-13 10:33:23,911][training][INFO] - Epoch 39, Batch 0/3907, Loss: 0.393338\n",
      "[2025-05-13 10:33:24,303][training][INFO] - Epoch 39, Batch 100/3907, Loss: 0.390774\n",
      "[2025-05-13 10:33:24,689][training][INFO] - Epoch 39, Batch 200/3907, Loss: 0.378825\n",
      "[2025-05-13 10:33:25,084][training][INFO] - Epoch 39, Batch 300/3907, Loss: 0.391695\n",
      "[2025-05-13 10:33:25,471][training][INFO] - Epoch 39, Batch 400/3907, Loss: 0.391552\n",
      "[2025-05-13 10:33:25,864][training][INFO] - Epoch 39, Batch 500/3907, Loss: 0.387647\n",
      "[2025-05-13 10:33:26,253][training][INFO] - Epoch 39, Batch 600/3907, Loss: 0.395510\n",
      "[2025-05-13 10:33:26,645][training][INFO] - Epoch 39, Batch 700/3907, Loss: 0.398730\n",
      "[2025-05-13 10:33:27,036][training][INFO] - Epoch 39, Batch 800/3907, Loss: 0.389296\n",
      "[2025-05-13 10:33:27,423][training][INFO] - Epoch 39, Batch 900/3907, Loss: 0.382602\n",
      "[2025-05-13 10:33:27,818][training][INFO] - Epoch 39, Batch 1000/3907, Loss: 0.380051\n",
      "[2025-05-13 10:33:28,206][training][INFO] - Epoch 39, Batch 1100/3907, Loss: 0.389279\n",
      "[2025-05-13 10:33:28,595][training][INFO] - Epoch 39, Batch 1200/3907, Loss: 0.382601\n",
      "[2025-05-13 10:33:28,985][training][INFO] - Epoch 39, Batch 1300/3907, Loss: 0.376387\n",
      "[2025-05-13 10:33:29,374][training][INFO] - Epoch 39, Batch 1400/3907, Loss: 0.385203\n",
      "[2025-05-13 10:33:29,761][training][INFO] - Epoch 39, Batch 1500/3907, Loss: 0.384390\n",
      "[2025-05-13 10:33:30,150][training][INFO] - Epoch 39, Batch 1600/3907, Loss: 0.388438\n",
      "[2025-05-13 10:33:30,542][training][INFO] - Epoch 39, Batch 1700/3907, Loss: 0.384298\n",
      "[2025-05-13 10:33:30,943][training][INFO] - Epoch 39, Batch 1800/3907, Loss: 0.388154\n",
      "[2025-05-13 10:33:31,343][training][INFO] - Epoch 39, Batch 1900/3907, Loss: 0.379347\n",
      "[2025-05-13 10:33:31,733][training][INFO] - Epoch 39, Batch 2000/3907, Loss: 0.386053\n",
      "[2025-05-13 10:33:32,123][training][INFO] - Epoch 39, Batch 2100/3907, Loss: 0.389365\n",
      "[2025-05-13 10:33:32,510][training][INFO] - Epoch 39, Batch 2200/3907, Loss: 0.384665\n",
      "[2025-05-13 10:33:32,902][training][INFO] - Epoch 39, Batch 2300/3907, Loss: 0.383935\n",
      "[2025-05-13 10:33:33,289][training][INFO] - Epoch 39, Batch 2400/3907, Loss: 0.383952\n",
      "[2025-05-13 10:33:33,675][training][INFO] - Epoch 39, Batch 2500/3907, Loss: 0.389766\n",
      "[2025-05-13 10:33:34,063][training][INFO] - Epoch 39, Batch 2600/3907, Loss: 0.382221\n",
      "[2025-05-13 10:33:34,458][training][INFO] - Epoch 39, Batch 2700/3907, Loss: 0.383645\n",
      "[2025-05-13 10:33:34,848][training][INFO] - Epoch 39, Batch 2800/3907, Loss: 0.384334\n",
      "[2025-05-13 10:33:35,239][training][INFO] - Epoch 39, Batch 2900/3907, Loss: 0.383402\n",
      "[2025-05-13 10:33:35,630][training][INFO] - Epoch 39, Batch 3000/3907, Loss: 0.381215\n",
      "[2025-05-13 10:33:36,022][training][INFO] - Epoch 39, Batch 3100/3907, Loss: 0.389143\n",
      "[2025-05-13 10:33:36,416][training][INFO] - Epoch 39, Batch 3200/3907, Loss: 0.390142\n",
      "[2025-05-13 10:33:36,804][training][INFO] - Epoch 39, Batch 3300/3907, Loss: 0.382076\n",
      "[2025-05-13 10:33:37,189][training][INFO] - Epoch 39, Batch 3400/3907, Loss: 0.381261\n",
      "[2025-05-13 10:33:37,575][training][INFO] - Epoch 39, Batch 3500/3907, Loss: 0.390453\n",
      "[2025-05-13 10:33:37,966][training][INFO] - Epoch 39, Batch 3600/3907, Loss: 0.388545\n",
      "[2025-05-13 10:33:38,352][training][INFO] - Epoch 39, Batch 3700/3907, Loss: 0.395763\n",
      "[2025-05-13 10:33:38,738][training][INFO] - Epoch 39, Batch 3800/3907, Loss: 0.386021\n",
      "[2025-05-13 10:33:39,138][training][INFO] - Epoch 39, Batch 3900/3907, Loss: 0.387733\n",
      "[2025-05-13 10:33:39,155][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:33:39,155][training][INFO] - Epoch 39 complete. Avg Loss: 0.388424\n",
      "[2025-05-13 10:33:39,191][training][INFO] - Epoch 40, Batch 0/3907, Loss: 0.390668\n",
      "[2025-05-13 10:33:39,591][training][INFO] - Epoch 40, Batch 100/3907, Loss: 0.387041\n",
      "[2025-05-13 10:33:39,981][training][INFO] - Epoch 40, Batch 200/3907, Loss: 0.388707\n",
      "[2025-05-13 10:33:40,368][training][INFO] - Epoch 40, Batch 300/3907, Loss: 0.388874\n",
      "[2025-05-13 10:33:40,752][training][INFO] - Epoch 40, Batch 400/3907, Loss: 0.391438\n",
      "[2025-05-13 10:33:41,145][training][INFO] - Epoch 40, Batch 500/3907, Loss: 0.380324\n",
      "[2025-05-13 10:33:41,536][training][INFO] - Epoch 40, Batch 600/3907, Loss: 0.376678\n",
      "[2025-05-13 10:33:41,923][training][INFO] - Epoch 40, Batch 700/3907, Loss: 0.391609\n",
      "[2025-05-13 10:33:42,311][training][INFO] - Epoch 40, Batch 800/3907, Loss: 0.390889\n",
      "[2025-05-13 10:33:42,701][training][INFO] - Epoch 40, Batch 900/3907, Loss: 0.386594\n",
      "[2025-05-13 10:33:43,095][training][INFO] - Epoch 40, Batch 1000/3907, Loss: 0.391291\n",
      "[2025-05-13 10:33:43,485][training][INFO] - Epoch 40, Batch 1100/3907, Loss: 0.383349\n",
      "[2025-05-13 10:33:43,875][training][INFO] - Epoch 40, Batch 1200/3907, Loss: 0.391302\n",
      "[2025-05-13 10:33:44,268][training][INFO] - Epoch 40, Batch 1300/3907, Loss: 0.389314\n",
      "[2025-05-13 10:33:44,665][training][INFO] - Epoch 40, Batch 1400/3907, Loss: 0.387226\n",
      "[2025-05-13 10:33:45,058][training][INFO] - Epoch 40, Batch 1500/3907, Loss: 0.388802\n",
      "[2025-05-13 10:33:45,449][training][INFO] - Epoch 40, Batch 1600/3907, Loss: 0.398679\n",
      "[2025-05-13 10:33:45,842][training][INFO] - Epoch 40, Batch 1700/3907, Loss: 0.391754\n",
      "[2025-05-13 10:33:46,232][training][INFO] - Epoch 40, Batch 1800/3907, Loss: 0.387256\n",
      "[2025-05-13 10:33:46,627][training][INFO] - Epoch 40, Batch 1900/3907, Loss: 0.382064\n",
      "[2025-05-13 10:33:47,015][training][INFO] - Epoch 40, Batch 2000/3907, Loss: 0.379023\n",
      "[2025-05-13 10:33:47,406][training][INFO] - Epoch 40, Batch 2100/3907, Loss: 0.395984\n",
      "[2025-05-13 10:33:47,793][training][INFO] - Epoch 40, Batch 2200/3907, Loss: 0.382937\n",
      "[2025-05-13 10:33:48,186][training][INFO] - Epoch 40, Batch 2300/3907, Loss: 0.385939\n",
      "[2025-05-13 10:33:48,573][training][INFO] - Epoch 40, Batch 2400/3907, Loss: 0.396620\n",
      "[2025-05-13 10:33:48,958][training][INFO] - Epoch 40, Batch 2500/3907, Loss: 0.385848\n",
      "[2025-05-13 10:33:49,345][training][INFO] - Epoch 40, Batch 2600/3907, Loss: 0.391660\n",
      "[2025-05-13 10:33:49,735][training][INFO] - Epoch 40, Batch 2700/3907, Loss: 0.379357\n",
      "[2025-05-13 10:33:50,128][training][INFO] - Epoch 40, Batch 2800/3907, Loss: 0.382288\n",
      "[2025-05-13 10:33:50,514][training][INFO] - Epoch 40, Batch 2900/3907, Loss: 0.393680\n",
      "[2025-05-13 10:33:50,900][training][INFO] - Epoch 40, Batch 3000/3907, Loss: 0.384625\n",
      "[2025-05-13 10:33:51,288][training][INFO] - Epoch 40, Batch 3100/3907, Loss: 0.394693\n",
      "[2025-05-13 10:33:51,681][training][INFO] - Epoch 40, Batch 3200/3907, Loss: 0.390084\n",
      "[2025-05-13 10:33:52,077][training][INFO] - Epoch 40, Batch 3300/3907, Loss: 0.383436\n",
      "[2025-05-13 10:33:52,470][training][INFO] - Epoch 40, Batch 3400/3907, Loss: 0.382711\n",
      "[2025-05-13 10:33:52,859][training][INFO] - Epoch 40, Batch 3500/3907, Loss: 0.382201\n",
      "[2025-05-13 10:33:53,246][training][INFO] - Epoch 40, Batch 3600/3907, Loss: 0.389569\n",
      "[2025-05-13 10:33:53,631][training][INFO] - Epoch 40, Batch 3700/3907, Loss: 0.393032\n",
      "[2025-05-13 10:33:54,016][training][INFO] - Epoch 40, Batch 3800/3907, Loss: 0.396042\n",
      "[2025-05-13 10:33:54,421][training][INFO] - Epoch 40, Batch 3900/3907, Loss: 0.391927\n",
      "[2025-05-13 10:33:54,438][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:33:54,439][training][INFO] - Epoch 40 complete. Avg Loss: 0.388110\n",
      "[2025-05-13 10:33:54,445][training][INFO] - Saved checkpoint to /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7/checkpoint_epoch_40.pt\n",
      "[2025-05-13 10:33:54,490][training][INFO] - Epoch 41, Batch 0/3907, Loss: 0.383745\n",
      "[2025-05-13 10:33:54,878][training][INFO] - Epoch 41, Batch 100/3907, Loss: 0.387122\n",
      "[2025-05-13 10:33:55,267][training][INFO] - Epoch 41, Batch 200/3907, Loss: 0.390578\n",
      "[2025-05-13 10:33:55,658][training][INFO] - Epoch 41, Batch 300/3907, Loss: 0.389143\n",
      "[2025-05-13 10:33:56,053][training][INFO] - Epoch 41, Batch 400/3907, Loss: 0.384546\n",
      "[2025-05-13 10:33:56,446][training][INFO] - Epoch 41, Batch 500/3907, Loss: 0.384516\n",
      "[2025-05-13 10:33:56,833][training][INFO] - Epoch 41, Batch 600/3907, Loss: 0.387660\n",
      "[2025-05-13 10:33:57,222][training][INFO] - Epoch 41, Batch 700/3907, Loss: 0.378333\n",
      "[2025-05-13 10:33:57,606][training][INFO] - Epoch 41, Batch 800/3907, Loss: 0.389352\n",
      "[2025-05-13 10:33:57,996][training][INFO] - Epoch 41, Batch 900/3907, Loss: 0.393081\n",
      "[2025-05-13 10:33:58,384][training][INFO] - Epoch 41, Batch 1000/3907, Loss: 0.390243\n",
      "[2025-05-13 10:33:58,772][training][INFO] - Epoch 41, Batch 1100/3907, Loss: 0.390295\n",
      "[2025-05-13 10:33:59,156][training][INFO] - Epoch 41, Batch 1200/3907, Loss: 0.396367\n",
      "[2025-05-13 10:33:59,542][training][INFO] - Epoch 41, Batch 1300/3907, Loss: 0.386678\n",
      "[2025-05-13 10:33:59,928][training][INFO] - Epoch 41, Batch 1400/3907, Loss: 0.385912\n",
      "[2025-05-13 10:34:00,314][training][INFO] - Epoch 41, Batch 1500/3907, Loss: 0.387868\n",
      "[2025-05-13 10:34:00,705][training][INFO] - Epoch 41, Batch 1600/3907, Loss: 0.389622\n",
      "[2025-05-13 10:34:01,091][training][INFO] - Epoch 41, Batch 1700/3907, Loss: 0.395462\n",
      "[2025-05-13 10:34:01,481][training][INFO] - Epoch 41, Batch 1800/3907, Loss: 0.389555\n",
      "[2025-05-13 10:34:01,866][training][INFO] - Epoch 41, Batch 1900/3907, Loss: 0.386380\n",
      "[2025-05-13 10:34:02,252][training][INFO] - Epoch 41, Batch 2000/3907, Loss: 0.393582\n",
      "[2025-05-13 10:34:02,636][training][INFO] - Epoch 41, Batch 2100/3907, Loss: 0.390524\n",
      "[2025-05-13 10:34:03,022][training][INFO] - Epoch 41, Batch 2200/3907, Loss: 0.387252\n",
      "[2025-05-13 10:34:03,409][training][INFO] - Epoch 41, Batch 2300/3907, Loss: 0.391916\n",
      "[2025-05-13 10:34:03,793][training][INFO] - Epoch 41, Batch 2400/3907, Loss: 0.388267\n",
      "[2025-05-13 10:34:04,181][training][INFO] - Epoch 41, Batch 2500/3907, Loss: 0.385829\n",
      "[2025-05-13 10:34:04,569][training][INFO] - Epoch 41, Batch 2600/3907, Loss: 0.390037\n",
      "[2025-05-13 10:34:04,954][training][INFO] - Epoch 41, Batch 2700/3907, Loss: 0.394865\n",
      "[2025-05-13 10:34:05,340][training][INFO] - Epoch 41, Batch 2800/3907, Loss: 0.394227\n",
      "[2025-05-13 10:34:05,729][training][INFO] - Epoch 41, Batch 2900/3907, Loss: 0.390914\n",
      "[2025-05-13 10:34:06,114][training][INFO] - Epoch 41, Batch 3000/3907, Loss: 0.383536\n",
      "[2025-05-13 10:34:06,501][training][INFO] - Epoch 41, Batch 3100/3907, Loss: 0.389283\n",
      "[2025-05-13 10:34:06,891][training][INFO] - Epoch 41, Batch 3200/3907, Loss: 0.392457\n",
      "[2025-05-13 10:34:07,277][training][INFO] - Epoch 41, Batch 3300/3907, Loss: 0.387768\n",
      "[2025-05-13 10:34:07,661][training][INFO] - Epoch 41, Batch 3400/3907, Loss: 0.386907\n",
      "[2025-05-13 10:34:08,046][training][INFO] - Epoch 41, Batch 3500/3907, Loss: 0.387191\n",
      "[2025-05-13 10:34:08,430][training][INFO] - Epoch 41, Batch 3600/3907, Loss: 0.395324\n",
      "[2025-05-13 10:34:08,818][training][INFO] - Epoch 41, Batch 3700/3907, Loss: 0.383863\n",
      "[2025-05-13 10:34:09,207][training][INFO] - Epoch 41, Batch 3800/3907, Loss: 0.392544\n",
      "[2025-05-13 10:34:09,608][training][INFO] - Epoch 41, Batch 3900/3907, Loss: 0.384976\n",
      "[2025-05-13 10:34:09,625][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:34:09,625][training][INFO] - Epoch 41 complete. Avg Loss: 0.387912\n",
      "[2025-05-13 10:34:09,665][training][INFO] - Epoch 42, Batch 0/3907, Loss: 0.389037\n",
      "[2025-05-13 10:34:10,055][training][INFO] - Epoch 42, Batch 100/3907, Loss: 0.389989\n",
      "[2025-05-13 10:34:10,443][training][INFO] - Epoch 42, Batch 200/3907, Loss: 0.395244\n",
      "[2025-05-13 10:34:10,833][training][INFO] - Epoch 42, Batch 300/3907, Loss: 0.382723\n",
      "[2025-05-13 10:34:11,219][training][INFO] - Epoch 42, Batch 400/3907, Loss: 0.389016\n",
      "[2025-05-13 10:34:11,607][training][INFO] - Epoch 42, Batch 500/3907, Loss: 0.382335\n",
      "[2025-05-13 10:34:11,991][training][INFO] - Epoch 42, Batch 600/3907, Loss: 0.382534\n",
      "[2025-05-13 10:34:12,378][training][INFO] - Epoch 42, Batch 700/3907, Loss: 0.384987\n",
      "[2025-05-13 10:34:12,764][training][INFO] - Epoch 42, Batch 800/3907, Loss: 0.383692\n",
      "[2025-05-13 10:34:13,153][training][INFO] - Epoch 42, Batch 900/3907, Loss: 0.386915\n",
      "[2025-05-13 10:34:13,543][training][INFO] - Epoch 42, Batch 1000/3907, Loss: 0.387257\n",
      "[2025-05-13 10:34:13,932][training][INFO] - Epoch 42, Batch 1100/3907, Loss: 0.386455\n",
      "[2025-05-13 10:34:14,321][training][INFO] - Epoch 42, Batch 1200/3907, Loss: 0.386503\n",
      "[2025-05-13 10:34:14,709][training][INFO] - Epoch 42, Batch 1300/3907, Loss: 0.390465\n",
      "[2025-05-13 10:34:15,098][training][INFO] - Epoch 42, Batch 1400/3907, Loss: 0.373510\n",
      "[2025-05-13 10:34:15,484][training][INFO] - Epoch 42, Batch 1500/3907, Loss: 0.382427\n",
      "[2025-05-13 10:34:15,878][training][INFO] - Epoch 42, Batch 1600/3907, Loss: 0.379939\n",
      "[2025-05-13 10:34:16,270][training][INFO] - Epoch 42, Batch 1700/3907, Loss: 0.396136\n",
      "[2025-05-13 10:34:16,657][training][INFO] - Epoch 42, Batch 1800/3907, Loss: 0.383499\n",
      "[2025-05-13 10:34:17,042][training][INFO] - Epoch 42, Batch 1900/3907, Loss: 0.378110\n",
      "[2025-05-13 10:34:17,429][training][INFO] - Epoch 42, Batch 2000/3907, Loss: 0.375691\n",
      "[2025-05-13 10:34:17,814][training][INFO] - Epoch 42, Batch 2100/3907, Loss: 0.388417\n",
      "[2025-05-13 10:34:18,201][training][INFO] - Epoch 42, Batch 2200/3907, Loss: 0.378710\n",
      "[2025-05-13 10:34:18,586][training][INFO] - Epoch 42, Batch 2300/3907, Loss: 0.383667\n",
      "[2025-05-13 10:34:18,974][training][INFO] - Epoch 42, Batch 2400/3907, Loss: 0.391084\n",
      "[2025-05-13 10:34:19,364][training][INFO] - Epoch 42, Batch 2500/3907, Loss: 0.383812\n",
      "[2025-05-13 10:34:19,747][training][INFO] - Epoch 42, Batch 2600/3907, Loss: 0.391403\n",
      "[2025-05-13 10:34:20,137][training][INFO] - Epoch 42, Batch 2700/3907, Loss: 0.377266\n",
      "[2025-05-13 10:34:20,527][training][INFO] - Epoch 42, Batch 2800/3907, Loss: 0.392645\n",
      "[2025-05-13 10:34:20,914][training][INFO] - Epoch 42, Batch 2900/3907, Loss: 0.389481\n",
      "[2025-05-13 10:34:21,301][training][INFO] - Epoch 42, Batch 3000/3907, Loss: 0.385659\n",
      "[2025-05-13 10:34:21,692][training][INFO] - Epoch 42, Batch 3100/3907, Loss: 0.388188\n",
      "[2025-05-13 10:34:22,083][training][INFO] - Epoch 42, Batch 3200/3907, Loss: 0.400491\n",
      "[2025-05-13 10:34:22,467][training][INFO] - Epoch 42, Batch 3300/3907, Loss: 0.388447\n",
      "[2025-05-13 10:34:22,860][training][INFO] - Epoch 42, Batch 3400/3907, Loss: 0.391675\n",
      "[2025-05-13 10:34:23,257][training][INFO] - Epoch 42, Batch 3500/3907, Loss: 0.392321\n",
      "[2025-05-13 10:34:23,646][training][INFO] - Epoch 42, Batch 3600/3907, Loss: 0.386539\n",
      "[2025-05-13 10:34:24,031][training][INFO] - Epoch 42, Batch 3700/3907, Loss: 0.390206\n",
      "[2025-05-13 10:34:24,423][training][INFO] - Epoch 42, Batch 3800/3907, Loss: 0.385556\n",
      "[2025-05-13 10:34:24,825][training][INFO] - Epoch 42, Batch 3900/3907, Loss: 0.379407\n",
      "[2025-05-13 10:34:24,842][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:34:24,842][training][INFO] - Epoch 42 complete. Avg Loss: 0.387684\n",
      "[2025-05-13 10:34:24,888][training][INFO] - Epoch 43, Batch 0/3907, Loss: 0.390381\n",
      "[2025-05-13 10:34:25,278][training][INFO] - Epoch 43, Batch 100/3907, Loss: 0.387688\n",
      "[2025-05-13 10:34:25,665][training][INFO] - Epoch 43, Batch 200/3907, Loss: 0.389413\n",
      "[2025-05-13 10:34:26,057][training][INFO] - Epoch 43, Batch 300/3907, Loss: 0.396408\n",
      "[2025-05-13 10:34:26,448][training][INFO] - Epoch 43, Batch 400/3907, Loss: 0.391623\n",
      "[2025-05-13 10:34:26,836][training][INFO] - Epoch 43, Batch 500/3907, Loss: 0.394250\n",
      "[2025-05-13 10:34:27,220][training][INFO] - Epoch 43, Batch 600/3907, Loss: 0.381156\n",
      "[2025-05-13 10:34:27,604][training][INFO] - Epoch 43, Batch 700/3907, Loss: 0.392570\n",
      "[2025-05-13 10:34:27,991][training][INFO] - Epoch 43, Batch 800/3907, Loss: 0.385788\n",
      "[2025-05-13 10:34:28,375][training][INFO] - Epoch 43, Batch 900/3907, Loss: 0.389195\n",
      "[2025-05-13 10:34:28,762][training][INFO] - Epoch 43, Batch 1000/3907, Loss: 0.386023\n",
      "[2025-05-13 10:34:29,147][training][INFO] - Epoch 43, Batch 1100/3907, Loss: 0.389986\n",
      "[2025-05-13 10:34:29,534][training][INFO] - Epoch 43, Batch 1200/3907, Loss: 0.389430\n",
      "[2025-05-13 10:34:29,920][training][INFO] - Epoch 43, Batch 1300/3907, Loss: 0.386890\n",
      "[2025-05-13 10:34:30,307][training][INFO] - Epoch 43, Batch 1400/3907, Loss: 0.386796\n",
      "[2025-05-13 10:34:30,701][training][INFO] - Epoch 43, Batch 1500/3907, Loss: 0.384732\n",
      "[2025-05-13 10:34:31,094][training][INFO] - Epoch 43, Batch 1600/3907, Loss: 0.384456\n",
      "[2025-05-13 10:34:31,481][training][INFO] - Epoch 43, Batch 1700/3907, Loss: 0.387671\n",
      "[2025-05-13 10:34:31,864][training][INFO] - Epoch 43, Batch 1800/3907, Loss: 0.390287\n",
      "[2025-05-13 10:34:32,248][training][INFO] - Epoch 43, Batch 1900/3907, Loss: 0.383081\n",
      "[2025-05-13 10:34:32,635][training][INFO] - Epoch 43, Batch 2000/3907, Loss: 0.389323\n",
      "[2025-05-13 10:34:33,022][training][INFO] - Epoch 43, Batch 2100/3907, Loss: 0.393985\n",
      "[2025-05-13 10:34:33,410][training][INFO] - Epoch 43, Batch 2200/3907, Loss: 0.387964\n",
      "[2025-05-13 10:34:33,800][training][INFO] - Epoch 43, Batch 2300/3907, Loss: 0.384246\n",
      "[2025-05-13 10:34:34,185][training][INFO] - Epoch 43, Batch 2400/3907, Loss: 0.386061\n",
      "[2025-05-13 10:34:34,572][training][INFO] - Epoch 43, Batch 2500/3907, Loss: 0.387501\n",
      "[2025-05-13 10:34:34,959][training][INFO] - Epoch 43, Batch 2600/3907, Loss: 0.392379\n",
      "[2025-05-13 10:34:35,347][training][INFO] - Epoch 43, Batch 2700/3907, Loss: 0.384671\n",
      "[2025-05-13 10:34:35,733][training][INFO] - Epoch 43, Batch 2800/3907, Loss: 0.387773\n",
      "[2025-05-13 10:34:36,122][training][INFO] - Epoch 43, Batch 2900/3907, Loss: 0.386443\n",
      "[2025-05-13 10:34:36,511][training][INFO] - Epoch 43, Batch 3000/3907, Loss: 0.387697\n",
      "[2025-05-13 10:34:36,900][training][INFO] - Epoch 43, Batch 3100/3907, Loss: 0.380081\n",
      "[2025-05-13 10:34:37,285][training][INFO] - Epoch 43, Batch 3200/3907, Loss: 0.387710\n",
      "[2025-05-13 10:34:37,673][training][INFO] - Epoch 43, Batch 3300/3907, Loss: 0.385497\n",
      "[2025-05-13 10:34:38,057][training][INFO] - Epoch 43, Batch 3400/3907, Loss: 0.380484\n",
      "[2025-05-13 10:34:38,445][training][INFO] - Epoch 43, Batch 3500/3907, Loss: 0.395295\n",
      "[2025-05-13 10:34:38,831][training][INFO] - Epoch 43, Batch 3600/3907, Loss: 0.390512\n",
      "[2025-05-13 10:34:39,216][training][INFO] - Epoch 43, Batch 3700/3907, Loss: 0.388791\n",
      "[2025-05-13 10:34:39,603][training][INFO] - Epoch 43, Batch 3800/3907, Loss: 0.393814\n",
      "[2025-05-13 10:34:40,008][training][INFO] - Epoch 43, Batch 3900/3907, Loss: 0.386097\n",
      "[2025-05-13 10:34:40,025][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:34:40,025][training][INFO] - Epoch 43 complete. Avg Loss: 0.387547\n",
      "[2025-05-13 10:34:40,069][training][INFO] - Epoch 44, Batch 0/3907, Loss: 0.381081\n",
      "[2025-05-13 10:34:40,457][training][INFO] - Epoch 44, Batch 100/3907, Loss: 0.396536\n",
      "[2025-05-13 10:34:40,844][training][INFO] - Epoch 44, Batch 200/3907, Loss: 0.389513\n",
      "[2025-05-13 10:34:41,230][training][INFO] - Epoch 44, Batch 300/3907, Loss: 0.386533\n",
      "[2025-05-13 10:34:41,618][training][INFO] - Epoch 44, Batch 400/3907, Loss: 0.379818\n",
      "[2025-05-13 10:34:42,005][training][INFO] - Epoch 44, Batch 500/3907, Loss: 0.387160\n",
      "[2025-05-13 10:34:42,390][training][INFO] - Epoch 44, Batch 600/3907, Loss: 0.383218\n",
      "[2025-05-13 10:34:42,777][training][INFO] - Epoch 44, Batch 700/3907, Loss: 0.388557\n",
      "[2025-05-13 10:34:43,161][training][INFO] - Epoch 44, Batch 800/3907, Loss: 0.388067\n",
      "[2025-05-13 10:34:43,550][training][INFO] - Epoch 44, Batch 900/3907, Loss: 0.394532\n",
      "[2025-05-13 10:34:43,938][training][INFO] - Epoch 44, Batch 1000/3907, Loss: 0.390425\n",
      "[2025-05-13 10:34:44,325][training][INFO] - Epoch 44, Batch 1100/3907, Loss: 0.393577\n",
      "[2025-05-13 10:34:44,713][training][INFO] - Epoch 44, Batch 1200/3907, Loss: 0.375953\n",
      "[2025-05-13 10:34:45,099][training][INFO] - Epoch 44, Batch 1300/3907, Loss: 0.384976\n",
      "[2025-05-13 10:34:45,488][training][INFO] - Epoch 44, Batch 1400/3907, Loss: 0.390581\n",
      "[2025-05-13 10:34:45,881][training][INFO] - Epoch 44, Batch 1500/3907, Loss: 0.393903\n",
      "[2025-05-13 10:34:46,269][training][INFO] - Epoch 44, Batch 1600/3907, Loss: 0.383688\n",
      "[2025-05-13 10:34:46,661][training][INFO] - Epoch 44, Batch 1700/3907, Loss: 0.380923\n",
      "[2025-05-13 10:34:47,057][training][INFO] - Epoch 44, Batch 1800/3907, Loss: 0.385307\n",
      "[2025-05-13 10:34:47,446][training][INFO] - Epoch 44, Batch 1900/3907, Loss: 0.395329\n",
      "[2025-05-13 10:34:47,836][training][INFO] - Epoch 44, Batch 2000/3907, Loss: 0.390100\n",
      "[2025-05-13 10:34:48,226][training][INFO] - Epoch 44, Batch 2100/3907, Loss: 0.379632\n",
      "[2025-05-13 10:34:48,616][training][INFO] - Epoch 44, Batch 2200/3907, Loss: 0.389239\n",
      "[2025-05-13 10:34:49,003][training][INFO] - Epoch 44, Batch 2300/3907, Loss: 0.387755\n",
      "[2025-05-13 10:34:49,390][training][INFO] - Epoch 44, Batch 2400/3907, Loss: 0.390784\n",
      "[2025-05-13 10:34:49,778][training][INFO] - Epoch 44, Batch 2500/3907, Loss: 0.388387\n",
      "[2025-05-13 10:34:50,166][training][INFO] - Epoch 44, Batch 2600/3907, Loss: 0.379120\n",
      "[2025-05-13 10:34:50,553][training][INFO] - Epoch 44, Batch 2700/3907, Loss: 0.380253\n",
      "[2025-05-13 10:34:50,943][training][INFO] - Epoch 44, Batch 2800/3907, Loss: 0.383945\n",
      "[2025-05-13 10:34:51,332][training][INFO] - Epoch 44, Batch 2900/3907, Loss: 0.381179\n",
      "[2025-05-13 10:34:51,723][training][INFO] - Epoch 44, Batch 3000/3907, Loss: 0.395229\n",
      "[2025-05-13 10:34:52,112][training][INFO] - Epoch 44, Batch 3100/3907, Loss: 0.390150\n",
      "[2025-05-13 10:34:52,499][training][INFO] - Epoch 44, Batch 3200/3907, Loss: 0.381758\n",
      "[2025-05-13 10:34:52,888][training][INFO] - Epoch 44, Batch 3300/3907, Loss: 0.382094\n",
      "[2025-05-13 10:34:53,279][training][INFO] - Epoch 44, Batch 3400/3907, Loss: 0.388552\n",
      "[2025-05-13 10:34:53,665][training][INFO] - Epoch 44, Batch 3500/3907, Loss: 0.383959\n",
      "[2025-05-13 10:34:54,050][training][INFO] - Epoch 44, Batch 3600/3907, Loss: 0.389243\n",
      "[2025-05-13 10:34:54,436][training][INFO] - Epoch 44, Batch 3700/3907, Loss: 0.378498\n",
      "[2025-05-13 10:34:54,825][training][INFO] - Epoch 44, Batch 3800/3907, Loss: 0.393291\n",
      "[2025-05-13 10:34:55,231][training][INFO] - Epoch 44, Batch 3900/3907, Loss: 0.388020\n",
      "[2025-05-13 10:34:55,248][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:34:55,248][training][INFO] - Epoch 44 complete. Avg Loss: 0.387276\n",
      "[2025-05-13 10:34:55,294][training][INFO] - Epoch 45, Batch 0/3907, Loss: 0.382305\n",
      "[2025-05-13 10:34:55,688][training][INFO] - Epoch 45, Batch 100/3907, Loss: 0.376786\n",
      "[2025-05-13 10:34:56,081][training][INFO] - Epoch 45, Batch 200/3907, Loss: 0.394212\n",
      "[2025-05-13 10:34:56,471][training][INFO] - Epoch 45, Batch 300/3907, Loss: 0.387256\n",
      "[2025-05-13 10:34:56,860][training][INFO] - Epoch 45, Batch 400/3907, Loss: 0.391300\n",
      "[2025-05-13 10:34:57,249][training][INFO] - Epoch 45, Batch 500/3907, Loss: 0.382307\n",
      "[2025-05-13 10:34:57,638][training][INFO] - Epoch 45, Batch 600/3907, Loss: 0.385175\n",
      "[2025-05-13 10:34:58,029][training][INFO] - Epoch 45, Batch 700/3907, Loss: 0.393244\n",
      "[2025-05-13 10:34:58,417][training][INFO] - Epoch 45, Batch 800/3907, Loss: 0.397806\n",
      "[2025-05-13 10:34:58,806][training][INFO] - Epoch 45, Batch 900/3907, Loss: 0.386664\n",
      "[2025-05-13 10:34:59,192][training][INFO] - Epoch 45, Batch 1000/3907, Loss: 0.393334\n",
      "[2025-05-13 10:34:59,577][training][INFO] - Epoch 45, Batch 1100/3907, Loss: 0.394297\n",
      "[2025-05-13 10:34:59,963][training][INFO] - Epoch 45, Batch 1200/3907, Loss: 0.396655\n",
      "[2025-05-13 10:35:00,351][training][INFO] - Epoch 45, Batch 1300/3907, Loss: 0.383788\n",
      "[2025-05-13 10:35:00,743][training][INFO] - Epoch 45, Batch 1400/3907, Loss: 0.383686\n",
      "[2025-05-13 10:35:01,127][training][INFO] - Epoch 45, Batch 1500/3907, Loss: 0.377645\n",
      "[2025-05-13 10:35:01,515][training][INFO] - Epoch 45, Batch 1600/3907, Loss: 0.382534\n",
      "[2025-05-13 10:35:01,904][training][INFO] - Epoch 45, Batch 1700/3907, Loss: 0.388368\n",
      "[2025-05-13 10:35:02,291][training][INFO] - Epoch 45, Batch 1800/3907, Loss: 0.393549\n",
      "[2025-05-13 10:35:02,682][training][INFO] - Epoch 45, Batch 1900/3907, Loss: 0.380694\n",
      "[2025-05-13 10:35:03,069][training][INFO] - Epoch 45, Batch 2000/3907, Loss: 0.395556\n",
      "[2025-05-13 10:35:03,458][training][INFO] - Epoch 45, Batch 2100/3907, Loss: 0.386837\n",
      "[2025-05-13 10:35:03,844][training][INFO] - Epoch 45, Batch 2200/3907, Loss: 0.379446\n",
      "[2025-05-13 10:35:04,232][training][INFO] - Epoch 45, Batch 2300/3907, Loss: 0.396749\n",
      "[2025-05-13 10:35:04,620][training][INFO] - Epoch 45, Batch 2400/3907, Loss: 0.382353\n",
      "[2025-05-13 10:35:05,007][training][INFO] - Epoch 45, Batch 2500/3907, Loss: 0.394287\n",
      "[2025-05-13 10:35:05,396][training][INFO] - Epoch 45, Batch 2600/3907, Loss: 0.383786\n",
      "[2025-05-13 10:35:05,781][training][INFO] - Epoch 45, Batch 2700/3907, Loss: 0.391599\n",
      "[2025-05-13 10:35:06,166][training][INFO] - Epoch 45, Batch 2800/3907, Loss: 0.390482\n",
      "[2025-05-13 10:35:06,553][training][INFO] - Epoch 45, Batch 2900/3907, Loss: 0.384972\n",
      "[2025-05-13 10:35:06,938][training][INFO] - Epoch 45, Batch 3000/3907, Loss: 0.386197\n",
      "[2025-05-13 10:35:07,325][training][INFO] - Epoch 45, Batch 3100/3907, Loss: 0.389775\n",
      "[2025-05-13 10:35:07,714][training][INFO] - Epoch 45, Batch 3200/3907, Loss: 0.383903\n",
      "[2025-05-13 10:35:08,104][training][INFO] - Epoch 45, Batch 3300/3907, Loss: 0.386896\n",
      "[2025-05-13 10:35:08,494][training][INFO] - Epoch 45, Batch 3400/3907, Loss: 0.390069\n",
      "[2025-05-13 10:35:08,883][training][INFO] - Epoch 45, Batch 3500/3907, Loss: 0.372555\n",
      "[2025-05-13 10:35:09,269][training][INFO] - Epoch 45, Batch 3600/3907, Loss: 0.388849\n",
      "[2025-05-13 10:35:09,657][training][INFO] - Epoch 45, Batch 3700/3907, Loss: 0.385073\n",
      "[2025-05-13 10:35:10,052][training][INFO] - Epoch 45, Batch 3800/3907, Loss: 0.388047\n",
      "[2025-05-13 10:35:10,463][training][INFO] - Epoch 45, Batch 3900/3907, Loss: 0.391921\n",
      "[2025-05-13 10:35:10,481][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:35:10,481][training][INFO] - Epoch 45 complete. Avg Loss: 0.387073\n",
      "[2025-05-13 10:35:10,526][training][INFO] - Epoch 46, Batch 0/3907, Loss: 0.386899\n",
      "[2025-05-13 10:35:10,915][training][INFO] - Epoch 46, Batch 100/3907, Loss: 0.382109\n",
      "[2025-05-13 10:35:11,302][training][INFO] - Epoch 46, Batch 200/3907, Loss: 0.389951\n",
      "[2025-05-13 10:35:11,686][training][INFO] - Epoch 46, Batch 300/3907, Loss: 0.377850\n",
      "[2025-05-13 10:35:12,073][training][INFO] - Epoch 46, Batch 400/3907, Loss: 0.385943\n",
      "[2025-05-13 10:35:12,463][training][INFO] - Epoch 46, Batch 500/3907, Loss: 0.386998\n",
      "[2025-05-13 10:35:12,857][training][INFO] - Epoch 46, Batch 600/3907, Loss: 0.384552\n",
      "[2025-05-13 10:35:13,245][training][INFO] - Epoch 46, Batch 700/3907, Loss: 0.382160\n",
      "[2025-05-13 10:35:13,631][training][INFO] - Epoch 46, Batch 800/3907, Loss: 0.394925\n",
      "[2025-05-13 10:35:14,020][training][INFO] - Epoch 46, Batch 900/3907, Loss: 0.386230\n",
      "[2025-05-13 10:35:14,408][training][INFO] - Epoch 46, Batch 1000/3907, Loss: 0.383156\n",
      "[2025-05-13 10:35:14,795][training][INFO] - Epoch 46, Batch 1100/3907, Loss: 0.387456\n",
      "[2025-05-13 10:35:15,183][training][INFO] - Epoch 46, Batch 1200/3907, Loss: 0.390206\n",
      "[2025-05-13 10:35:15,574][training][INFO] - Epoch 46, Batch 1300/3907, Loss: 0.385584\n",
      "[2025-05-13 10:35:15,968][training][INFO] - Epoch 46, Batch 1400/3907, Loss: 0.389584\n",
      "[2025-05-13 10:35:16,355][training][INFO] - Epoch 46, Batch 1500/3907, Loss: 0.375398\n",
      "[2025-05-13 10:35:16,743][training][INFO] - Epoch 46, Batch 1600/3907, Loss: 0.391648\n",
      "[2025-05-13 10:35:17,129][training][INFO] - Epoch 46, Batch 1700/3907, Loss: 0.383274\n",
      "[2025-05-13 10:35:17,516][training][INFO] - Epoch 46, Batch 1800/3907, Loss: 0.380978\n",
      "[2025-05-13 10:35:17,903][training][INFO] - Epoch 46, Batch 1900/3907, Loss: 0.389058\n",
      "[2025-05-13 10:35:18,293][training][INFO] - Epoch 46, Batch 2000/3907, Loss: 0.378411\n",
      "[2025-05-13 10:35:18,683][training][INFO] - Epoch 46, Batch 2100/3907, Loss: 0.372866\n",
      "[2025-05-13 10:35:19,074][training][INFO] - Epoch 46, Batch 2200/3907, Loss: 0.390882\n",
      "[2025-05-13 10:35:19,463][training][INFO] - Epoch 46, Batch 2300/3907, Loss: 0.395027\n",
      "[2025-05-13 10:35:19,849][training][INFO] - Epoch 46, Batch 2400/3907, Loss: 0.381047\n",
      "[2025-05-13 10:35:20,236][training][INFO] - Epoch 46, Batch 2500/3907, Loss: 0.390302\n",
      "[2025-05-13 10:35:20,624][training][INFO] - Epoch 46, Batch 2600/3907, Loss: 0.376421\n",
      "[2025-05-13 10:35:21,009][training][INFO] - Epoch 46, Batch 2700/3907, Loss: 0.384078\n",
      "[2025-05-13 10:35:21,396][training][INFO] - Epoch 46, Batch 2800/3907, Loss: 0.378039\n",
      "[2025-05-13 10:35:21,786][training][INFO] - Epoch 46, Batch 2900/3907, Loss: 0.387452\n",
      "[2025-05-13 10:35:22,170][training][INFO] - Epoch 46, Batch 3000/3907, Loss: 0.386498\n",
      "[2025-05-13 10:35:22,557][training][INFO] - Epoch 46, Batch 3100/3907, Loss: 0.377507\n",
      "[2025-05-13 10:35:22,943][training][INFO] - Epoch 46, Batch 3200/3907, Loss: 0.386770\n",
      "[2025-05-13 10:35:23,326][training][INFO] - Epoch 46, Batch 3300/3907, Loss: 0.389440\n",
      "[2025-05-13 10:35:23,715][training][INFO] - Epoch 46, Batch 3400/3907, Loss: 0.382599\n",
      "[2025-05-13 10:35:24,110][training][INFO] - Epoch 46, Batch 3500/3907, Loss: 0.392103\n",
      "[2025-05-13 10:35:24,501][training][INFO] - Epoch 46, Batch 3600/3907, Loss: 0.389811\n",
      "[2025-05-13 10:35:24,889][training][INFO] - Epoch 46, Batch 3700/3907, Loss: 0.378725\n",
      "[2025-05-13 10:35:25,277][training][INFO] - Epoch 46, Batch 3800/3907, Loss: 0.383390\n",
      "[2025-05-13 10:35:25,681][training][INFO] - Epoch 46, Batch 3900/3907, Loss: 0.381963\n",
      "[2025-05-13 10:35:25,698][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:35:25,698][training][INFO] - Epoch 46 complete. Avg Loss: 0.386751\n",
      "[2025-05-13 10:35:25,742][training][INFO] - Epoch 47, Batch 0/3907, Loss: 0.388509\n",
      "[2025-05-13 10:35:26,129][training][INFO] - Epoch 47, Batch 100/3907, Loss: 0.393840\n",
      "[2025-05-13 10:35:26,514][training][INFO] - Epoch 47, Batch 200/3907, Loss: 0.383221\n",
      "[2025-05-13 10:35:26,898][training][INFO] - Epoch 47, Batch 300/3907, Loss: 0.388996\n",
      "[2025-05-13 10:35:27,285][training][INFO] - Epoch 47, Batch 400/3907, Loss: 0.382134\n",
      "[2025-05-13 10:35:27,669][training][INFO] - Epoch 47, Batch 500/3907, Loss: 0.385377\n",
      "[2025-05-13 10:35:28,061][training][INFO] - Epoch 47, Batch 600/3907, Loss: 0.393051\n",
      "[2025-05-13 10:35:28,446][training][INFO] - Epoch 47, Batch 700/3907, Loss: 0.381099\n",
      "[2025-05-13 10:35:28,834][training][INFO] - Epoch 47, Batch 800/3907, Loss: 0.386854\n",
      "[2025-05-13 10:35:29,218][training][INFO] - Epoch 47, Batch 900/3907, Loss: 0.374992\n",
      "[2025-05-13 10:35:29,607][training][INFO] - Epoch 47, Batch 1000/3907, Loss: 0.387926\n",
      "[2025-05-13 10:35:29,994][training][INFO] - Epoch 47, Batch 1100/3907, Loss: 0.383046\n",
      "[2025-05-13 10:35:30,381][training][INFO] - Epoch 47, Batch 1200/3907, Loss: 0.388813\n",
      "[2025-05-13 10:35:30,772][training][INFO] - Epoch 47, Batch 1300/3907, Loss: 0.387831\n",
      "[2025-05-13 10:35:31,159][training][INFO] - Epoch 47, Batch 1400/3907, Loss: 0.384789\n",
      "[2025-05-13 10:35:31,545][training][INFO] - Epoch 47, Batch 1500/3907, Loss: 0.387197\n",
      "[2025-05-13 10:35:31,929][training][INFO] - Epoch 47, Batch 1600/3907, Loss: 0.386930\n",
      "[2025-05-13 10:35:32,312][training][INFO] - Epoch 47, Batch 1700/3907, Loss: 0.381325\n",
      "[2025-05-13 10:35:32,698][training][INFO] - Epoch 47, Batch 1800/3907, Loss: 0.386830\n",
      "[2025-05-13 10:35:33,085][training][INFO] - Epoch 47, Batch 1900/3907, Loss: 0.382226\n",
      "[2025-05-13 10:35:33,468][training][INFO] - Epoch 47, Batch 2000/3907, Loss: 0.390393\n",
      "[2025-05-13 10:35:33,854][training][INFO] - Epoch 47, Batch 2100/3907, Loss: 0.382951\n",
      "[2025-05-13 10:35:34,240][training][INFO] - Epoch 47, Batch 2200/3907, Loss: 0.377189\n",
      "[2025-05-13 10:35:34,624][training][INFO] - Epoch 47, Batch 2300/3907, Loss: 0.392756\n",
      "[2025-05-13 10:35:35,009][training][INFO] - Epoch 47, Batch 2400/3907, Loss: 0.390725\n",
      "[2025-05-13 10:35:35,395][training][INFO] - Epoch 47, Batch 2500/3907, Loss: 0.384590\n",
      "[2025-05-13 10:35:35,782][training][INFO] - Epoch 47, Batch 2600/3907, Loss: 0.389915\n",
      "[2025-05-13 10:35:36,169][training][INFO] - Epoch 47, Batch 2700/3907, Loss: 0.385478\n",
      "[2025-05-13 10:35:36,556][training][INFO] - Epoch 47, Batch 2800/3907, Loss: 0.394713\n",
      "[2025-05-13 10:35:36,939][training][INFO] - Epoch 47, Batch 2900/3907, Loss: 0.388349\n",
      "[2025-05-13 10:35:37,326][training][INFO] - Epoch 47, Batch 3000/3907, Loss: 0.386281\n",
      "[2025-05-13 10:35:37,715][training][INFO] - Epoch 47, Batch 3100/3907, Loss: 0.389432\n",
      "[2025-05-13 10:35:38,100][training][INFO] - Epoch 47, Batch 3200/3907, Loss: 0.388124\n",
      "[2025-05-13 10:35:38,489][training][INFO] - Epoch 47, Batch 3300/3907, Loss: 0.383316\n",
      "[2025-05-13 10:35:38,880][training][INFO] - Epoch 47, Batch 3400/3907, Loss: 0.382712\n",
      "[2025-05-13 10:35:39,265][training][INFO] - Epoch 47, Batch 3500/3907, Loss: 0.380997\n",
      "[2025-05-13 10:35:39,650][training][INFO] - Epoch 47, Batch 3600/3907, Loss: 0.394198\n",
      "[2025-05-13 10:35:40,039][training][INFO] - Epoch 47, Batch 3700/3907, Loss: 0.390191\n",
      "[2025-05-13 10:35:40,430][training][INFO] - Epoch 47, Batch 3800/3907, Loss: 0.394067\n",
      "[2025-05-13 10:35:40,837][training][INFO] - Epoch 47, Batch 3900/3907, Loss: 0.383677\n",
      "[2025-05-13 10:35:40,854][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:35:40,854][training][INFO] - Epoch 47 complete. Avg Loss: 0.386403\n",
      "[2025-05-13 10:35:40,896][training][INFO] - Epoch 48, Batch 0/3907, Loss: 0.381566\n",
      "[2025-05-13 10:35:41,285][training][INFO] - Epoch 48, Batch 100/3907, Loss: 0.391511\n",
      "[2025-05-13 10:35:41,674][training][INFO] - Epoch 48, Batch 200/3907, Loss: 0.386684\n",
      "[2025-05-13 10:35:42,063][training][INFO] - Epoch 48, Batch 300/3907, Loss: 0.388042\n",
      "[2025-05-13 10:35:42,450][training][INFO] - Epoch 48, Batch 400/3907, Loss: 0.382792\n",
      "[2025-05-13 10:35:42,837][training][INFO] - Epoch 48, Batch 500/3907, Loss: 0.384633\n",
      "[2025-05-13 10:35:43,223][training][INFO] - Epoch 48, Batch 600/3907, Loss: 0.385577\n",
      "[2025-05-13 10:35:43,609][training][INFO] - Epoch 48, Batch 700/3907, Loss: 0.378036\n",
      "[2025-05-13 10:35:43,992][training][INFO] - Epoch 48, Batch 800/3907, Loss: 0.377500\n",
      "[2025-05-13 10:35:44,380][training][INFO] - Epoch 48, Batch 900/3907, Loss: 0.386334\n",
      "[2025-05-13 10:35:44,767][training][INFO] - Epoch 48, Batch 1000/3907, Loss: 0.378095\n",
      "[2025-05-13 10:35:45,153][training][INFO] - Epoch 48, Batch 1100/3907, Loss: 0.387691\n",
      "[2025-05-13 10:35:45,539][training][INFO] - Epoch 48, Batch 1200/3907, Loss: 0.383241\n",
      "[2025-05-13 10:35:45,930][training][INFO] - Epoch 48, Batch 1300/3907, Loss: 0.377363\n",
      "[2025-05-13 10:35:46,327][training][INFO] - Epoch 48, Batch 1400/3907, Loss: 0.381169\n",
      "[2025-05-13 10:35:46,725][training][INFO] - Epoch 48, Batch 1500/3907, Loss: 0.388561\n",
      "[2025-05-13 10:35:47,112][training][INFO] - Epoch 48, Batch 1600/3907, Loss: 0.392918\n",
      "[2025-05-13 10:35:47,503][training][INFO] - Epoch 48, Batch 1700/3907, Loss: 0.385847\n",
      "[2025-05-13 10:35:47,903][training][INFO] - Epoch 48, Batch 1800/3907, Loss: 0.395772\n",
      "[2025-05-13 10:35:48,290][training][INFO] - Epoch 48, Batch 1900/3907, Loss: 0.379864\n",
      "[2025-05-13 10:35:48,676][training][INFO] - Epoch 48, Batch 2000/3907, Loss: 0.390661\n",
      "[2025-05-13 10:35:49,065][training][INFO] - Epoch 48, Batch 2100/3907, Loss: 0.382873\n",
      "[2025-05-13 10:35:49,455][training][INFO] - Epoch 48, Batch 2200/3907, Loss: 0.395107\n",
      "[2025-05-13 10:35:49,843][training][INFO] - Epoch 48, Batch 2300/3907, Loss: 0.381236\n",
      "[2025-05-13 10:35:50,230][training][INFO] - Epoch 48, Batch 2400/3907, Loss: 0.396788\n",
      "[2025-05-13 10:35:50,619][training][INFO] - Epoch 48, Batch 2500/3907, Loss: 0.395482\n",
      "[2025-05-13 10:35:51,009][training][INFO] - Epoch 48, Batch 2600/3907, Loss: 0.386625\n",
      "[2025-05-13 10:35:51,394][training][INFO] - Epoch 48, Batch 2700/3907, Loss: 0.392542\n",
      "[2025-05-13 10:35:51,781][training][INFO] - Epoch 48, Batch 2800/3907, Loss: 0.382304\n",
      "[2025-05-13 10:35:52,171][training][INFO] - Epoch 48, Batch 2900/3907, Loss: 0.394455\n",
      "[2025-05-13 10:35:52,556][training][INFO] - Epoch 48, Batch 3000/3907, Loss: 0.392630\n",
      "[2025-05-13 10:35:52,940][training][INFO] - Epoch 48, Batch 3100/3907, Loss: 0.383513\n",
      "[2025-05-13 10:35:53,330][training][INFO] - Epoch 48, Batch 3200/3907, Loss: 0.368253\n",
      "[2025-05-13 10:35:53,716][training][INFO] - Epoch 48, Batch 3300/3907, Loss: 0.394860\n",
      "[2025-05-13 10:35:54,103][training][INFO] - Epoch 48, Batch 3400/3907, Loss: 0.388175\n",
      "[2025-05-13 10:35:54,490][training][INFO] - Epoch 48, Batch 3500/3907, Loss: 0.392853\n",
      "[2025-05-13 10:35:54,875][training][INFO] - Epoch 48, Batch 3600/3907, Loss: 0.382032\n",
      "[2025-05-13 10:35:55,260][training][INFO] - Epoch 48, Batch 3700/3907, Loss: 0.388542\n",
      "[2025-05-13 10:35:55,647][training][INFO] - Epoch 48, Batch 3800/3907, Loss: 0.389048\n",
      "[2025-05-13 10:35:56,051][training][INFO] - Epoch 48, Batch 3900/3907, Loss: 0.386780\n",
      "[2025-05-13 10:35:56,068][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:35:56,068][training][INFO] - Epoch 48 complete. Avg Loss: 0.386392\n",
      "[2025-05-13 10:35:56,107][training][INFO] - Epoch 49, Batch 0/3907, Loss: 0.384844\n",
      "[2025-05-13 10:35:56,497][training][INFO] - Epoch 49, Batch 100/3907, Loss: 0.382921\n",
      "[2025-05-13 10:35:56,882][training][INFO] - Epoch 49, Batch 200/3907, Loss: 0.387790\n",
      "[2025-05-13 10:35:57,268][training][INFO] - Epoch 49, Batch 300/3907, Loss: 0.386632\n",
      "[2025-05-13 10:35:57,655][training][INFO] - Epoch 49, Batch 400/3907, Loss: 0.372705\n",
      "[2025-05-13 10:35:58,044][training][INFO] - Epoch 49, Batch 500/3907, Loss: 0.376770\n",
      "[2025-05-13 10:35:58,431][training][INFO] - Epoch 49, Batch 600/3907, Loss: 0.389484\n",
      "[2025-05-13 10:35:58,816][training][INFO] - Epoch 49, Batch 700/3907, Loss: 0.385207\n",
      "[2025-05-13 10:35:59,201][training][INFO] - Epoch 49, Batch 800/3907, Loss: 0.390403\n",
      "[2025-05-13 10:35:59,586][training][INFO] - Epoch 49, Batch 900/3907, Loss: 0.389653\n",
      "[2025-05-13 10:35:59,970][training][INFO] - Epoch 49, Batch 1000/3907, Loss: 0.391077\n",
      "[2025-05-13 10:36:00,353][training][INFO] - Epoch 49, Batch 1100/3907, Loss: 0.392356\n",
      "[2025-05-13 10:36:00,742][training][INFO] - Epoch 49, Batch 1200/3907, Loss: 0.380374\n",
      "[2025-05-13 10:36:01,130][training][INFO] - Epoch 49, Batch 1300/3907, Loss: 0.382926\n",
      "[2025-05-13 10:36:01,516][training][INFO] - Epoch 49, Batch 1400/3907, Loss: 0.388718\n",
      "[2025-05-13 10:36:01,905][training][INFO] - Epoch 49, Batch 1500/3907, Loss: 0.386521\n",
      "[2025-05-13 10:36:02,288][training][INFO] - Epoch 49, Batch 1600/3907, Loss: 0.384557\n",
      "[2025-05-13 10:36:02,673][training][INFO] - Epoch 49, Batch 1700/3907, Loss: 0.383576\n",
      "[2025-05-13 10:36:03,062][training][INFO] - Epoch 49, Batch 1800/3907, Loss: 0.392737\n",
      "[2025-05-13 10:36:03,450][training][INFO] - Epoch 49, Batch 1900/3907, Loss: 0.378029\n",
      "[2025-05-13 10:36:03,838][training][INFO] - Epoch 49, Batch 2000/3907, Loss: 0.382049\n",
      "[2025-05-13 10:36:04,226][training][INFO] - Epoch 49, Batch 2100/3907, Loss: 0.386330\n",
      "[2025-05-13 10:36:04,611][training][INFO] - Epoch 49, Batch 2200/3907, Loss: 0.374127\n",
      "[2025-05-13 10:36:04,997][training][INFO] - Epoch 49, Batch 2300/3907, Loss: 0.379021\n",
      "[2025-05-13 10:36:05,386][training][INFO] - Epoch 49, Batch 2400/3907, Loss: 0.381373\n",
      "[2025-05-13 10:36:05,773][training][INFO] - Epoch 49, Batch 2500/3907, Loss: 0.385081\n",
      "[2025-05-13 10:36:06,160][training][INFO] - Epoch 49, Batch 2600/3907, Loss: 0.384375\n",
      "[2025-05-13 10:36:06,547][training][INFO] - Epoch 49, Batch 2700/3907, Loss: 0.386295\n",
      "[2025-05-13 10:36:06,933][training][INFO] - Epoch 49, Batch 2800/3907, Loss: 0.386335\n",
      "[2025-05-13 10:36:07,322][training][INFO] - Epoch 49, Batch 2900/3907, Loss: 0.386465\n",
      "[2025-05-13 10:36:07,707][training][INFO] - Epoch 49, Batch 3000/3907, Loss: 0.391099\n",
      "[2025-05-13 10:36:08,093][training][INFO] - Epoch 49, Batch 3100/3907, Loss: 0.394323\n",
      "[2025-05-13 10:36:08,480][training][INFO] - Epoch 49, Batch 3200/3907, Loss: 0.381238\n",
      "[2025-05-13 10:36:08,868][training][INFO] - Epoch 49, Batch 3300/3907, Loss: 0.387196\n",
      "[2025-05-13 10:36:09,256][training][INFO] - Epoch 49, Batch 3400/3907, Loss: 0.387555\n",
      "[2025-05-13 10:36:09,642][training][INFO] - Epoch 49, Batch 3500/3907, Loss: 0.385265\n",
      "[2025-05-13 10:36:10,030][training][INFO] - Epoch 49, Batch 3600/3907, Loss: 0.390634\n",
      "[2025-05-13 10:36:10,417][training][INFO] - Epoch 49, Batch 3700/3907, Loss: 0.385181\n",
      "[2025-05-13 10:36:10,805][training][INFO] - Epoch 49, Batch 3800/3907, Loss: 0.379898\n",
      "[2025-05-13 10:36:11,208][training][INFO] - Epoch 49, Batch 3900/3907, Loss: 0.386662\n",
      "[2025-05-13 10:36:11,225][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:36:11,225][training][INFO] - Epoch 49 complete. Avg Loss: 0.385951\n",
      "[2025-05-13 10:36:11,268][training][INFO] - Epoch 50, Batch 0/3907, Loss: 0.389361\n",
      "[2025-05-13 10:36:11,656][training][INFO] - Epoch 50, Batch 100/3907, Loss: 0.390661\n",
      "[2025-05-13 10:36:12,041][training][INFO] - Epoch 50, Batch 200/3907, Loss: 0.390850\n",
      "[2025-05-13 10:36:12,426][training][INFO] - Epoch 50, Batch 300/3907, Loss: 0.389410\n",
      "[2025-05-13 10:36:12,810][training][INFO] - Epoch 50, Batch 400/3907, Loss: 0.386946\n",
      "[2025-05-13 10:36:13,198][training][INFO] - Epoch 50, Batch 500/3907, Loss: 0.385025\n",
      "[2025-05-13 10:36:13,581][training][INFO] - Epoch 50, Batch 600/3907, Loss: 0.381469\n",
      "[2025-05-13 10:36:13,969][training][INFO] - Epoch 50, Batch 700/3907, Loss: 0.385123\n",
      "[2025-05-13 10:36:14,357][training][INFO] - Epoch 50, Batch 800/3907, Loss: 0.390538\n",
      "[2025-05-13 10:36:14,748][training][INFO] - Epoch 50, Batch 900/3907, Loss: 0.383200\n",
      "[2025-05-13 10:36:15,137][training][INFO] - Epoch 50, Batch 1000/3907, Loss: 0.382597\n",
      "[2025-05-13 10:36:15,521][training][INFO] - Epoch 50, Batch 1100/3907, Loss: 0.389847\n",
      "[2025-05-13 10:36:15,911][training][INFO] - Epoch 50, Batch 1200/3907, Loss: 0.388915\n",
      "[2025-05-13 10:36:16,299][training][INFO] - Epoch 50, Batch 1300/3907, Loss: 0.390976\n",
      "[2025-05-13 10:36:16,687][training][INFO] - Epoch 50, Batch 1400/3907, Loss: 0.374425\n",
      "[2025-05-13 10:36:17,073][training][INFO] - Epoch 50, Batch 1500/3907, Loss: 0.388205\n",
      "[2025-05-13 10:36:17,461][training][INFO] - Epoch 50, Batch 1600/3907, Loss: 0.390433\n",
      "[2025-05-13 10:36:17,847][training][INFO] - Epoch 50, Batch 1700/3907, Loss: 0.385554\n",
      "[2025-05-13 10:36:18,233][training][INFO] - Epoch 50, Batch 1800/3907, Loss: 0.392196\n",
      "[2025-05-13 10:36:18,620][training][INFO] - Epoch 50, Batch 1900/3907, Loss: 0.386924\n",
      "[2025-05-13 10:36:19,004][training][INFO] - Epoch 50, Batch 2000/3907, Loss: 0.382841\n",
      "[2025-05-13 10:36:19,392][training][INFO] - Epoch 50, Batch 2100/3907, Loss: 0.384120\n",
      "[2025-05-13 10:36:19,781][training][INFO] - Epoch 50, Batch 2200/3907, Loss: 0.385069\n",
      "[2025-05-13 10:36:20,169][training][INFO] - Epoch 50, Batch 2300/3907, Loss: 0.383261\n",
      "[2025-05-13 10:36:20,553][training][INFO] - Epoch 50, Batch 2400/3907, Loss: 0.381164\n",
      "[2025-05-13 10:36:20,938][training][INFO] - Epoch 50, Batch 2500/3907, Loss: 0.382213\n",
      "[2025-05-13 10:36:21,322][training][INFO] - Epoch 50, Batch 2600/3907, Loss: 0.390449\n",
      "[2025-05-13 10:36:21,710][training][INFO] - Epoch 50, Batch 2700/3907, Loss: 0.394663\n",
      "[2025-05-13 10:36:22,102][training][INFO] - Epoch 50, Batch 2800/3907, Loss: 0.380646\n",
      "[2025-05-13 10:36:22,490][training][INFO] - Epoch 50, Batch 2900/3907, Loss: 0.393883\n",
      "[2025-05-13 10:36:22,878][training][INFO] - Epoch 50, Batch 3000/3907, Loss: 0.375515\n",
      "[2025-05-13 10:36:23,266][training][INFO] - Epoch 50, Batch 3100/3907, Loss: 0.382655\n",
      "[2025-05-13 10:36:23,657][training][INFO] - Epoch 50, Batch 3200/3907, Loss: 0.391781\n",
      "[2025-05-13 10:36:24,046][training][INFO] - Epoch 50, Batch 3300/3907, Loss: 0.378459\n",
      "[2025-05-13 10:36:24,433][training][INFO] - Epoch 50, Batch 3400/3907, Loss: 0.386036\n",
      "[2025-05-13 10:36:24,819][training][INFO] - Epoch 50, Batch 3500/3907, Loss: 0.381162\n",
      "[2025-05-13 10:36:25,207][training][INFO] - Epoch 50, Batch 3600/3907, Loss: 0.381249\n",
      "[2025-05-13 10:36:25,595][training][INFO] - Epoch 50, Batch 3700/3907, Loss: 0.389523\n",
      "[2025-05-13 10:36:25,983][training][INFO] - Epoch 50, Batch 3800/3907, Loss: 0.395993\n",
      "[2025-05-13 10:36:26,384][training][INFO] - Epoch 50, Batch 3900/3907, Loss: 0.384389\n",
      "[2025-05-13 10:36:26,400][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:36:26,400][training][INFO] - Epoch 50 complete. Avg Loss: 0.385774\n",
      "[2025-05-13 10:36:26,439][training][INFO] - Epoch 51, Batch 0/3907, Loss: 0.380583\n",
      "[2025-05-13 10:36:26,832][training][INFO] - Epoch 51, Batch 100/3907, Loss: 0.389736\n",
      "[2025-05-13 10:36:27,219][training][INFO] - Epoch 51, Batch 200/3907, Loss: 0.384925\n",
      "[2025-05-13 10:36:27,603][training][INFO] - Epoch 51, Batch 300/3907, Loss: 0.385602\n",
      "[2025-05-13 10:36:27,993][training][INFO] - Epoch 51, Batch 400/3907, Loss: 0.381794\n",
      "[2025-05-13 10:36:28,378][training][INFO] - Epoch 51, Batch 500/3907, Loss: 0.379665\n",
      "[2025-05-13 10:36:28,765][training][INFO] - Epoch 51, Batch 600/3907, Loss: 0.375939\n",
      "[2025-05-13 10:36:29,151][training][INFO] - Epoch 51, Batch 700/3907, Loss: 0.383915\n",
      "[2025-05-13 10:36:29,540][training][INFO] - Epoch 51, Batch 800/3907, Loss: 0.393537\n",
      "[2025-05-13 10:36:29,930][training][INFO] - Epoch 51, Batch 900/3907, Loss: 0.381436\n",
      "[2025-05-13 10:36:30,319][training][INFO] - Epoch 51, Batch 1000/3907, Loss: 0.388039\n",
      "[2025-05-13 10:36:30,712][training][INFO] - Epoch 51, Batch 1100/3907, Loss: 0.382912\n",
      "[2025-05-13 10:36:31,102][training][INFO] - Epoch 51, Batch 1200/3907, Loss: 0.394099\n",
      "[2025-05-13 10:36:31,488][training][INFO] - Epoch 51, Batch 1300/3907, Loss: 0.392118\n",
      "[2025-05-13 10:36:31,877][training][INFO] - Epoch 51, Batch 1400/3907, Loss: 0.378087\n",
      "[2025-05-13 10:36:32,265][training][INFO] - Epoch 51, Batch 1500/3907, Loss: 0.387894\n",
      "[2025-05-13 10:36:32,651][training][INFO] - Epoch 51, Batch 1600/3907, Loss: 0.384570\n",
      "[2025-05-13 10:36:33,039][training][INFO] - Epoch 51, Batch 1700/3907, Loss: 0.384688\n",
      "[2025-05-13 10:36:33,425][training][INFO] - Epoch 51, Batch 1800/3907, Loss: 0.387195\n",
      "[2025-05-13 10:36:33,811][training][INFO] - Epoch 51, Batch 1900/3907, Loss: 0.389812\n",
      "[2025-05-13 10:36:34,203][training][INFO] - Epoch 51, Batch 2000/3907, Loss: 0.380642\n",
      "[2025-05-13 10:36:34,589][training][INFO] - Epoch 51, Batch 2100/3907, Loss: 0.382171\n",
      "[2025-05-13 10:36:34,976][training][INFO] - Epoch 51, Batch 2200/3907, Loss: 0.387518\n",
      "[2025-05-13 10:36:35,365][training][INFO] - Epoch 51, Batch 2300/3907, Loss: 0.391291\n",
      "[2025-05-13 10:36:35,753][training][INFO] - Epoch 51, Batch 2400/3907, Loss: 0.387338\n",
      "[2025-05-13 10:36:36,141][training][INFO] - Epoch 51, Batch 2500/3907, Loss: 0.387391\n",
      "[2025-05-13 10:36:36,531][training][INFO] - Epoch 51, Batch 2600/3907, Loss: 0.387474\n",
      "[2025-05-13 10:36:36,919][training][INFO] - Epoch 51, Batch 2700/3907, Loss: 0.383994\n",
      "[2025-05-13 10:36:37,305][training][INFO] - Epoch 51, Batch 2800/3907, Loss: 0.377145\n",
      "[2025-05-13 10:36:37,697][training][INFO] - Epoch 51, Batch 2900/3907, Loss: 0.383817\n",
      "[2025-05-13 10:36:38,084][training][INFO] - Epoch 51, Batch 3000/3907, Loss: 0.386575\n",
      "[2025-05-13 10:36:38,481][training][INFO] - Epoch 51, Batch 3100/3907, Loss: 0.378262\n",
      "[2025-05-13 10:36:38,871][training][INFO] - Epoch 51, Batch 3200/3907, Loss: 0.383103\n",
      "[2025-05-13 10:36:39,263][training][INFO] - Epoch 51, Batch 3300/3907, Loss: 0.385774\n",
      "[2025-05-13 10:36:39,651][training][INFO] - Epoch 51, Batch 3400/3907, Loss: 0.379162\n",
      "[2025-05-13 10:36:40,040][training][INFO] - Epoch 51, Batch 3500/3907, Loss: 0.386693\n",
      "[2025-05-13 10:36:40,431][training][INFO] - Epoch 51, Batch 3600/3907, Loss: 0.372456\n",
      "[2025-05-13 10:36:40,817][training][INFO] - Epoch 51, Batch 3700/3907, Loss: 0.388922\n",
      "[2025-05-13 10:36:41,207][training][INFO] - Epoch 51, Batch 3800/3907, Loss: 0.394122\n",
      "[2025-05-13 10:36:41,612][training][INFO] - Epoch 51, Batch 3900/3907, Loss: 0.389766\n",
      "[2025-05-13 10:36:41,629][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:36:41,629][training][INFO] - Epoch 51 complete. Avg Loss: 0.385505\n",
      "[2025-05-13 10:36:41,673][training][INFO] - Epoch 52, Batch 0/3907, Loss: 0.387549\n",
      "[2025-05-13 10:36:42,061][training][INFO] - Epoch 52, Batch 100/3907, Loss: 0.384057\n",
      "[2025-05-13 10:36:42,450][training][INFO] - Epoch 52, Batch 200/3907, Loss: 0.396981\n",
      "[2025-05-13 10:36:42,835][training][INFO] - Epoch 52, Batch 300/3907, Loss: 0.390456\n",
      "[2025-05-13 10:36:43,224][training][INFO] - Epoch 52, Batch 400/3907, Loss: 0.380789\n",
      "[2025-05-13 10:36:43,610][training][INFO] - Epoch 52, Batch 500/3907, Loss: 0.383572\n",
      "[2025-05-13 10:36:43,999][training][INFO] - Epoch 52, Batch 600/3907, Loss: 0.388822\n",
      "[2025-05-13 10:36:44,384][training][INFO] - Epoch 52, Batch 700/3907, Loss: 0.384356\n",
      "[2025-05-13 10:36:44,770][training][INFO] - Epoch 52, Batch 800/3907, Loss: 0.379534\n",
      "[2025-05-13 10:36:45,154][training][INFO] - Epoch 52, Batch 900/3907, Loss: 0.389107\n",
      "[2025-05-13 10:36:45,539][training][INFO] - Epoch 52, Batch 1000/3907, Loss: 0.387147\n",
      "[2025-05-13 10:36:45,929][training][INFO] - Epoch 52, Batch 1100/3907, Loss: 0.382182\n",
      "[2025-05-13 10:36:46,319][training][INFO] - Epoch 52, Batch 1200/3907, Loss: 0.381590\n",
      "[2025-05-13 10:36:46,710][training][INFO] - Epoch 52, Batch 1300/3907, Loss: 0.391075\n",
      "[2025-05-13 10:36:47,099][training][INFO] - Epoch 52, Batch 1400/3907, Loss: 0.397301\n",
      "[2025-05-13 10:36:47,484][training][INFO] - Epoch 52, Batch 1500/3907, Loss: 0.392397\n",
      "[2025-05-13 10:36:47,873][training][INFO] - Epoch 52, Batch 1600/3907, Loss: 0.387514\n",
      "[2025-05-13 10:36:48,258][training][INFO] - Epoch 52, Batch 1700/3907, Loss: 0.386097\n",
      "[2025-05-13 10:36:48,643][training][INFO] - Epoch 52, Batch 1800/3907, Loss: 0.389640\n",
      "[2025-05-13 10:36:49,032][training][INFO] - Epoch 52, Batch 1900/3907, Loss: 0.376291\n",
      "[2025-05-13 10:36:49,421][training][INFO] - Epoch 52, Batch 2000/3907, Loss: 0.382786\n",
      "[2025-05-13 10:36:49,809][training][INFO] - Epoch 52, Batch 2100/3907, Loss: 0.385918\n",
      "[2025-05-13 10:36:50,200][training][INFO] - Epoch 52, Batch 2200/3907, Loss: 0.389121\n",
      "[2025-05-13 10:36:50,588][training][INFO] - Epoch 52, Batch 2300/3907, Loss: 0.369181\n",
      "[2025-05-13 10:36:50,979][training][INFO] - Epoch 52, Batch 2400/3907, Loss: 0.385295\n",
      "[2025-05-13 10:36:51,365][training][INFO] - Epoch 52, Batch 2500/3907, Loss: 0.384715\n",
      "[2025-05-13 10:36:51,755][training][INFO] - Epoch 52, Batch 2600/3907, Loss: 0.377052\n",
      "[2025-05-13 10:36:52,146][training][INFO] - Epoch 52, Batch 2700/3907, Loss: 0.386336\n",
      "[2025-05-13 10:36:52,535][training][INFO] - Epoch 52, Batch 2800/3907, Loss: 0.391069\n",
      "[2025-05-13 10:36:52,923][training][INFO] - Epoch 52, Batch 2900/3907, Loss: 0.384263\n",
      "[2025-05-13 10:36:53,312][training][INFO] - Epoch 52, Batch 3000/3907, Loss: 0.378725\n",
      "[2025-05-13 10:36:53,699][training][INFO] - Epoch 52, Batch 3100/3907, Loss: 0.395765\n",
      "[2025-05-13 10:36:54,085][training][INFO] - Epoch 52, Batch 3200/3907, Loss: 0.380428\n",
      "[2025-05-13 10:36:54,470][training][INFO] - Epoch 52, Batch 3300/3907, Loss: 0.380372\n",
      "[2025-05-13 10:36:54,858][training][INFO] - Epoch 52, Batch 3400/3907, Loss: 0.382291\n",
      "[2025-05-13 10:36:55,246][training][INFO] - Epoch 52, Batch 3500/3907, Loss: 0.376373\n",
      "[2025-05-13 10:36:55,641][training][INFO] - Epoch 52, Batch 3600/3907, Loss: 0.377256\n",
      "[2025-05-13 10:36:56,030][training][INFO] - Epoch 52, Batch 3700/3907, Loss: 0.382659\n",
      "[2025-05-13 10:36:56,420][training][INFO] - Epoch 52, Batch 3800/3907, Loss: 0.384504\n",
      "[2025-05-13 10:36:56,826][training][INFO] - Epoch 52, Batch 3900/3907, Loss: 0.381188\n",
      "[2025-05-13 10:36:56,843][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:36:56,843][training][INFO] - Epoch 52 complete. Avg Loss: 0.385095\n",
      "[2025-05-13 10:36:56,885][training][INFO] - Epoch 53, Batch 0/3907, Loss: 0.385417\n",
      "[2025-05-13 10:36:57,275][training][INFO] - Epoch 53, Batch 100/3907, Loss: 0.385106\n",
      "[2025-05-13 10:36:57,666][training][INFO] - Epoch 53, Batch 200/3907, Loss: 0.380601\n",
      "[2025-05-13 10:36:58,055][training][INFO] - Epoch 53, Batch 300/3907, Loss: 0.384215\n",
      "[2025-05-13 10:36:58,442][training][INFO] - Epoch 53, Batch 400/3907, Loss: 0.387498\n",
      "[2025-05-13 10:36:58,830][training][INFO] - Epoch 53, Batch 500/3907, Loss: 0.383828\n",
      "[2025-05-13 10:36:59,218][training][INFO] - Epoch 53, Batch 600/3907, Loss: 0.379852\n",
      "[2025-05-13 10:36:59,610][training][INFO] - Epoch 53, Batch 700/3907, Loss: 0.390840\n",
      "[2025-05-13 10:36:59,996][training][INFO] - Epoch 53, Batch 800/3907, Loss: 0.380300\n",
      "[2025-05-13 10:37:00,386][training][INFO] - Epoch 53, Batch 900/3907, Loss: 0.388062\n",
      "[2025-05-13 10:37:00,779][training][INFO] - Epoch 53, Batch 1000/3907, Loss: 0.389664\n",
      "[2025-05-13 10:37:01,167][training][INFO] - Epoch 53, Batch 1100/3907, Loss: 0.390572\n",
      "[2025-05-13 10:37:01,559][training][INFO] - Epoch 53, Batch 1200/3907, Loss: 0.385374\n",
      "[2025-05-13 10:37:01,950][training][INFO] - Epoch 53, Batch 1300/3907, Loss: 0.377118\n",
      "[2025-05-13 10:37:02,340][training][INFO] - Epoch 53, Batch 1400/3907, Loss: 0.385753\n",
      "[2025-05-13 10:37:02,731][training][INFO] - Epoch 53, Batch 1500/3907, Loss: 0.394449\n",
      "[2025-05-13 10:37:03,117][training][INFO] - Epoch 53, Batch 1600/3907, Loss: 0.380859\n",
      "[2025-05-13 10:37:03,502][training][INFO] - Epoch 53, Batch 1700/3907, Loss: 0.393852\n",
      "[2025-05-13 10:37:03,890][training][INFO] - Epoch 53, Batch 1800/3907, Loss: 0.387013\n",
      "[2025-05-13 10:37:04,281][training][INFO] - Epoch 53, Batch 1900/3907, Loss: 0.381002\n",
      "[2025-05-13 10:37:04,669][training][INFO] - Epoch 53, Batch 2000/3907, Loss: 0.383875\n",
      "[2025-05-13 10:37:05,056][training][INFO] - Epoch 53, Batch 2100/3907, Loss: 0.381807\n",
      "[2025-05-13 10:37:05,446][training][INFO] - Epoch 53, Batch 2200/3907, Loss: 0.388403\n",
      "[2025-05-13 10:37:05,836][training][INFO] - Epoch 53, Batch 2300/3907, Loss: 0.390917\n",
      "[2025-05-13 10:37:06,226][training][INFO] - Epoch 53, Batch 2400/3907, Loss: 0.383081\n",
      "[2025-05-13 10:37:06,613][training][INFO] - Epoch 53, Batch 2500/3907, Loss: 0.385794\n",
      "[2025-05-13 10:37:07,003][training][INFO] - Epoch 53, Batch 2600/3907, Loss: 0.390679\n",
      "[2025-05-13 10:37:07,388][training][INFO] - Epoch 53, Batch 2700/3907, Loss: 0.376870\n",
      "[2025-05-13 10:37:07,774][training][INFO] - Epoch 53, Batch 2800/3907, Loss: 0.394651\n",
      "[2025-05-13 10:37:08,162][training][INFO] - Epoch 53, Batch 2900/3907, Loss: 0.371442\n",
      "[2025-05-13 10:37:08,553][training][INFO] - Epoch 53, Batch 3000/3907, Loss: 0.379674\n",
      "[2025-05-13 10:37:08,938][training][INFO] - Epoch 53, Batch 3100/3907, Loss: 0.375583\n",
      "[2025-05-13 10:37:09,327][training][INFO] - Epoch 53, Batch 3200/3907, Loss: 0.382187\n",
      "[2025-05-13 10:37:09,716][training][INFO] - Epoch 53, Batch 3300/3907, Loss: 0.380808\n",
      "[2025-05-13 10:37:10,107][training][INFO] - Epoch 53, Batch 3400/3907, Loss: 0.382472\n",
      "[2025-05-13 10:37:10,499][training][INFO] - Epoch 53, Batch 3500/3907, Loss: 0.384506\n",
      "[2025-05-13 10:37:10,887][training][INFO] - Epoch 53, Batch 3600/3907, Loss: 0.387209\n",
      "[2025-05-13 10:37:11,278][training][INFO] - Epoch 53, Batch 3700/3907, Loss: 0.390355\n",
      "[2025-05-13 10:37:11,666][training][INFO] - Epoch 53, Batch 3800/3907, Loss: 0.390841\n",
      "[2025-05-13 10:37:12,069][training][INFO] - Epoch 53, Batch 3900/3907, Loss: 0.383186\n",
      "[2025-05-13 10:37:12,086][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:37:12,086][training][INFO] - Epoch 53 complete. Avg Loss: 0.384858\n",
      "[2025-05-13 10:37:12,129][training][INFO] - Epoch 54, Batch 0/3907, Loss: 0.388346\n",
      "[2025-05-13 10:37:12,519][training][INFO] - Epoch 54, Batch 100/3907, Loss: 0.384014\n",
      "[2025-05-13 10:37:12,906][training][INFO] - Epoch 54, Batch 200/3907, Loss: 0.380272\n",
      "[2025-05-13 10:37:13,292][training][INFO] - Epoch 54, Batch 300/3907, Loss: 0.392496\n",
      "[2025-05-13 10:37:13,679][training][INFO] - Epoch 54, Batch 400/3907, Loss: 0.381356\n",
      "[2025-05-13 10:37:14,064][training][INFO] - Epoch 54, Batch 500/3907, Loss: 0.389350\n",
      "[2025-05-13 10:37:14,452][training][INFO] - Epoch 54, Batch 600/3907, Loss: 0.387100\n",
      "[2025-05-13 10:37:14,839][training][INFO] - Epoch 54, Batch 700/3907, Loss: 0.392033\n",
      "[2025-05-13 10:37:15,229][training][INFO] - Epoch 54, Batch 800/3907, Loss: 0.385487\n",
      "[2025-05-13 10:37:15,623][training][INFO] - Epoch 54, Batch 900/3907, Loss: 0.387663\n",
      "[2025-05-13 10:37:16,026][training][INFO] - Epoch 54, Batch 1000/3907, Loss: 0.386960\n",
      "[2025-05-13 10:37:16,415][training][INFO] - Epoch 54, Batch 1100/3907, Loss: 0.386664\n",
      "[2025-05-13 10:37:16,803][training][INFO] - Epoch 54, Batch 1200/3907, Loss: 0.384807\n",
      "[2025-05-13 10:37:17,196][training][INFO] - Epoch 54, Batch 1300/3907, Loss: 0.389331\n",
      "[2025-05-13 10:37:17,584][training][INFO] - Epoch 54, Batch 1400/3907, Loss: 0.381557\n",
      "[2025-05-13 10:37:17,975][training][INFO] - Epoch 54, Batch 1500/3907, Loss: 0.383947\n",
      "[2025-05-13 10:37:18,367][training][INFO] - Epoch 54, Batch 1600/3907, Loss: 0.376059\n",
      "[2025-05-13 10:37:18,753][training][INFO] - Epoch 54, Batch 1700/3907, Loss: 0.389576\n",
      "[2025-05-13 10:37:19,141][training][INFO] - Epoch 54, Batch 1800/3907, Loss: 0.384581\n",
      "[2025-05-13 10:37:19,529][training][INFO] - Epoch 54, Batch 1900/3907, Loss: 0.386931\n",
      "[2025-05-13 10:37:19,916][training][INFO] - Epoch 54, Batch 2000/3907, Loss: 0.384894\n",
      "[2025-05-13 10:37:20,311][training][INFO] - Epoch 54, Batch 2100/3907, Loss: 0.387033\n",
      "[2025-05-13 10:37:20,697][training][INFO] - Epoch 54, Batch 2200/3907, Loss: 0.381789\n",
      "[2025-05-13 10:37:21,086][training][INFO] - Epoch 54, Batch 2300/3907, Loss: 0.377067\n",
      "[2025-05-13 10:37:21,473][training][INFO] - Epoch 54, Batch 2400/3907, Loss: 0.392957\n",
      "[2025-05-13 10:37:21,862][training][INFO] - Epoch 54, Batch 2500/3907, Loss: 0.385853\n",
      "[2025-05-13 10:37:22,251][training][INFO] - Epoch 54, Batch 2600/3907, Loss: 0.376490\n",
      "[2025-05-13 10:37:22,640][training][INFO] - Epoch 54, Batch 2700/3907, Loss: 0.388647\n",
      "[2025-05-13 10:37:23,028][training][INFO] - Epoch 54, Batch 2800/3907, Loss: 0.387628\n",
      "[2025-05-13 10:37:23,412][training][INFO] - Epoch 54, Batch 2900/3907, Loss: 0.390840\n",
      "[2025-05-13 10:37:23,800][training][INFO] - Epoch 54, Batch 3000/3907, Loss: 0.385982\n",
      "[2025-05-13 10:37:24,188][training][INFO] - Epoch 54, Batch 3100/3907, Loss: 0.385476\n",
      "[2025-05-13 10:37:24,576][training][INFO] - Epoch 54, Batch 3200/3907, Loss: 0.376342\n",
      "[2025-05-13 10:37:24,967][training][INFO] - Epoch 54, Batch 3300/3907, Loss: 0.389288\n",
      "[2025-05-13 10:37:25,360][training][INFO] - Epoch 54, Batch 3400/3907, Loss: 0.376009\n",
      "[2025-05-13 10:37:25,749][training][INFO] - Epoch 54, Batch 3500/3907, Loss: 0.383919\n",
      "[2025-05-13 10:37:26,137][training][INFO] - Epoch 54, Batch 3600/3907, Loss: 0.381285\n",
      "[2025-05-13 10:37:26,525][training][INFO] - Epoch 54, Batch 3700/3907, Loss: 0.383331\n",
      "[2025-05-13 10:37:26,912][training][INFO] - Epoch 54, Batch 3800/3907, Loss: 0.384248\n",
      "[2025-05-13 10:37:27,323][training][INFO] - Epoch 54, Batch 3900/3907, Loss: 0.380596\n",
      "[2025-05-13 10:37:27,340][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:37:27,340][training][INFO] - Epoch 54 complete. Avg Loss: 0.384450\n",
      "[2025-05-13 10:37:27,380][training][INFO] - Epoch 55, Batch 0/3907, Loss: 0.389509\n",
      "[2025-05-13 10:37:27,773][training][INFO] - Epoch 55, Batch 100/3907, Loss: 0.383183\n",
      "[2025-05-13 10:37:28,164][training][INFO] - Epoch 55, Batch 200/3907, Loss: 0.390292\n",
      "[2025-05-13 10:37:28,559][training][INFO] - Epoch 55, Batch 300/3907, Loss: 0.382637\n",
      "[2025-05-13 10:37:28,947][training][INFO] - Epoch 55, Batch 400/3907, Loss: 0.381492\n",
      "[2025-05-13 10:37:29,342][training][INFO] - Epoch 55, Batch 500/3907, Loss: 0.385386\n",
      "[2025-05-13 10:37:29,732][training][INFO] - Epoch 55, Batch 600/3907, Loss: 0.387555\n",
      "[2025-05-13 10:37:30,119][training][INFO] - Epoch 55, Batch 700/3907, Loss: 0.379625\n",
      "[2025-05-13 10:37:30,505][training][INFO] - Epoch 55, Batch 800/3907, Loss: 0.389574\n",
      "[2025-05-13 10:37:30,896][training][INFO] - Epoch 55, Batch 900/3907, Loss: 0.386888\n",
      "[2025-05-13 10:37:31,291][training][INFO] - Epoch 55, Batch 1000/3907, Loss: 0.383076\n",
      "[2025-05-13 10:37:31,686][training][INFO] - Epoch 55, Batch 1100/3907, Loss: 0.383756\n",
      "[2025-05-13 10:37:32,077][training][INFO] - Epoch 55, Batch 1200/3907, Loss: 0.388346\n",
      "[2025-05-13 10:37:32,464][training][INFO] - Epoch 55, Batch 1300/3907, Loss: 0.379788\n",
      "[2025-05-13 10:37:32,855][training][INFO] - Epoch 55, Batch 1400/3907, Loss: 0.390521\n",
      "[2025-05-13 10:37:33,247][training][INFO] - Epoch 55, Batch 1500/3907, Loss: 0.380182\n",
      "[2025-05-13 10:37:33,639][training][INFO] - Epoch 55, Batch 1600/3907, Loss: 0.371374\n",
      "[2025-05-13 10:37:34,038][training][INFO] - Epoch 55, Batch 1700/3907, Loss: 0.383299\n",
      "[2025-05-13 10:37:34,439][training][INFO] - Epoch 55, Batch 1800/3907, Loss: 0.380145\n",
      "[2025-05-13 10:37:34,840][training][INFO] - Epoch 55, Batch 1900/3907, Loss: 0.382856\n",
      "[2025-05-13 10:37:35,242][training][INFO] - Epoch 55, Batch 2000/3907, Loss: 0.383382\n",
      "[2025-05-13 10:37:35,642][training][INFO] - Epoch 55, Batch 2100/3907, Loss: 0.380702\n",
      "[2025-05-13 10:37:36,046][training][INFO] - Epoch 55, Batch 2200/3907, Loss: 0.389165\n",
      "[2025-05-13 10:37:36,446][training][INFO] - Epoch 55, Batch 2300/3907, Loss: 0.377632\n",
      "[2025-05-13 10:37:36,850][training][INFO] - Epoch 55, Batch 2400/3907, Loss: 0.390349\n",
      "[2025-05-13 10:37:37,252][training][INFO] - Epoch 55, Batch 2500/3907, Loss: 0.386842\n",
      "[2025-05-13 10:37:37,653][training][INFO] - Epoch 55, Batch 2600/3907, Loss: 0.386052\n",
      "[2025-05-13 10:37:38,054][training][INFO] - Epoch 55, Batch 2700/3907, Loss: 0.382037\n",
      "[2025-05-13 10:37:38,459][training][INFO] - Epoch 55, Batch 2800/3907, Loss: 0.382797\n",
      "[2025-05-13 10:37:38,864][training][INFO] - Epoch 55, Batch 2900/3907, Loss: 0.388497\n",
      "[2025-05-13 10:37:39,267][training][INFO] - Epoch 55, Batch 3000/3907, Loss: 0.377854\n",
      "[2025-05-13 10:37:39,672][training][INFO] - Epoch 55, Batch 3100/3907, Loss: 0.386975\n",
      "[2025-05-13 10:37:40,076][training][INFO] - Epoch 55, Batch 3200/3907, Loss: 0.378255\n",
      "[2025-05-13 10:37:40,480][training][INFO] - Epoch 55, Batch 3300/3907, Loss: 0.379213\n",
      "[2025-05-13 10:37:40,882][training][INFO] - Epoch 55, Batch 3400/3907, Loss: 0.384334\n",
      "[2025-05-13 10:37:41,285][training][INFO] - Epoch 55, Batch 3500/3907, Loss: 0.376586\n",
      "[2025-05-13 10:37:41,686][training][INFO] - Epoch 55, Batch 3600/3907, Loss: 0.388727\n",
      "[2025-05-13 10:37:42,088][training][INFO] - Epoch 55, Batch 3700/3907, Loss: 0.384894\n",
      "[2025-05-13 10:37:42,490][training][INFO] - Epoch 55, Batch 3800/3907, Loss: 0.375583\n",
      "[2025-05-13 10:37:42,909][training][INFO] - Epoch 55, Batch 3900/3907, Loss: 0.377495\n",
      "[2025-05-13 10:37:42,926][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:37:42,926][training][INFO] - Epoch 55 complete. Avg Loss: 0.384173\n",
      "[2025-05-13 10:37:42,975][training][INFO] - Epoch 56, Batch 0/3907, Loss: 0.387191\n",
      "[2025-05-13 10:37:43,376][training][INFO] - Epoch 56, Batch 100/3907, Loss: 0.385936\n",
      "[2025-05-13 10:37:43,777][training][INFO] - Epoch 56, Batch 200/3907, Loss: 0.395096\n",
      "[2025-05-13 10:37:44,179][training][INFO] - Epoch 56, Batch 300/3907, Loss: 0.382522\n",
      "[2025-05-13 10:37:44,581][training][INFO] - Epoch 56, Batch 400/3907, Loss: 0.376342\n",
      "[2025-05-13 10:37:44,983][training][INFO] - Epoch 56, Batch 500/3907, Loss: 0.385893\n",
      "[2025-05-13 10:37:45,385][training][INFO] - Epoch 56, Batch 600/3907, Loss: 0.389024\n",
      "[2025-05-13 10:37:45,791][training][INFO] - Epoch 56, Batch 700/3907, Loss: 0.388877\n",
      "[2025-05-13 10:37:46,195][training][INFO] - Epoch 56, Batch 800/3907, Loss: 0.379611\n",
      "[2025-05-13 10:37:46,597][training][INFO] - Epoch 56, Batch 900/3907, Loss: 0.387196\n",
      "[2025-05-13 10:37:46,998][training][INFO] - Epoch 56, Batch 1000/3907, Loss: 0.391197\n",
      "[2025-05-13 10:37:47,399][training][INFO] - Epoch 56, Batch 1100/3907, Loss: 0.385316\n",
      "[2025-05-13 10:37:47,801][training][INFO] - Epoch 56, Batch 1200/3907, Loss: 0.382822\n",
      "[2025-05-13 10:37:48,201][training][INFO] - Epoch 56, Batch 1300/3907, Loss: 0.383936\n",
      "[2025-05-13 10:37:48,603][training][INFO] - Epoch 56, Batch 1400/3907, Loss: 0.384310\n",
      "[2025-05-13 10:37:49,005][training][INFO] - Epoch 56, Batch 1500/3907, Loss: 0.380272\n",
      "[2025-05-13 10:37:49,405][training][INFO] - Epoch 56, Batch 1600/3907, Loss: 0.379841\n",
      "[2025-05-13 10:37:49,805][training][INFO] - Epoch 56, Batch 1700/3907, Loss: 0.378391\n",
      "[2025-05-13 10:37:50,207][training][INFO] - Epoch 56, Batch 1800/3907, Loss: 0.383885\n",
      "[2025-05-13 10:37:50,607][training][INFO] - Epoch 56, Batch 1900/3907, Loss: 0.388092\n",
      "[2025-05-13 10:37:51,012][training][INFO] - Epoch 56, Batch 2000/3907, Loss: 0.394106\n",
      "[2025-05-13 10:37:51,417][training][INFO] - Epoch 56, Batch 2100/3907, Loss: 0.381225\n",
      "[2025-05-13 10:37:51,822][training][INFO] - Epoch 56, Batch 2200/3907, Loss: 0.382594\n",
      "[2025-05-13 10:37:52,225][training][INFO] - Epoch 56, Batch 2300/3907, Loss: 0.382780\n",
      "[2025-05-13 10:37:52,631][training][INFO] - Epoch 56, Batch 2400/3907, Loss: 0.381796\n",
      "[2025-05-13 10:37:53,033][training][INFO] - Epoch 56, Batch 2500/3907, Loss: 0.388681\n",
      "[2025-05-13 10:37:53,437][training][INFO] - Epoch 56, Batch 2600/3907, Loss: 0.393660\n",
      "[2025-05-13 10:37:53,839][training][INFO] - Epoch 56, Batch 2700/3907, Loss: 0.378500\n",
      "[2025-05-13 10:37:54,239][training][INFO] - Epoch 56, Batch 2800/3907, Loss: 0.382273\n",
      "[2025-05-13 10:37:54,643][training][INFO] - Epoch 56, Batch 2900/3907, Loss: 0.381880\n",
      "[2025-05-13 10:37:55,047][training][INFO] - Epoch 56, Batch 3000/3907, Loss: 0.384046\n",
      "[2025-05-13 10:37:55,452][training][INFO] - Epoch 56, Batch 3100/3907, Loss: 0.380650\n",
      "[2025-05-13 10:37:55,855][training][INFO] - Epoch 56, Batch 3200/3907, Loss: 0.388372\n",
      "[2025-05-13 10:37:56,260][training][INFO] - Epoch 56, Batch 3300/3907, Loss: 0.381201\n",
      "[2025-05-13 10:37:56,664][training][INFO] - Epoch 56, Batch 3400/3907, Loss: 0.384356\n",
      "[2025-05-13 10:37:57,065][training][INFO] - Epoch 56, Batch 3500/3907, Loss: 0.380579\n",
      "[2025-05-13 10:37:57,467][training][INFO] - Epoch 56, Batch 3600/3907, Loss: 0.383330\n",
      "[2025-05-13 10:37:57,869][training][INFO] - Epoch 56, Batch 3700/3907, Loss: 0.380939\n",
      "[2025-05-13 10:37:58,271][training][INFO] - Epoch 56, Batch 3800/3907, Loss: 0.381571\n",
      "[2025-05-13 10:37:58,693][training][INFO] - Epoch 56, Batch 3900/3907, Loss: 0.387951\n",
      "[2025-05-13 10:37:58,710][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:37:58,710][training][INFO] - Epoch 56 complete. Avg Loss: 0.383843\n",
      "[2025-05-13 10:37:58,753][training][INFO] - Epoch 57, Batch 0/3907, Loss: 0.386264\n",
      "[2025-05-13 10:37:59,161][training][INFO] - Epoch 57, Batch 100/3907, Loss: 0.388712\n",
      "[2025-05-13 10:37:59,562][training][INFO] - Epoch 57, Batch 200/3907, Loss: 0.382363\n",
      "[2025-05-13 10:37:59,966][training][INFO] - Epoch 57, Batch 300/3907, Loss: 0.382123\n",
      "[2025-05-13 10:38:00,370][training][INFO] - Epoch 57, Batch 400/3907, Loss: 0.380937\n",
      "[2025-05-13 10:38:00,778][training][INFO] - Epoch 57, Batch 500/3907, Loss: 0.381451\n",
      "[2025-05-13 10:38:01,181][training][INFO] - Epoch 57, Batch 600/3907, Loss: 0.389114\n",
      "[2025-05-13 10:38:01,581][training][INFO] - Epoch 57, Batch 700/3907, Loss: 0.390881\n",
      "[2025-05-13 10:38:01,982][training][INFO] - Epoch 57, Batch 800/3907, Loss: 0.388629\n",
      "[2025-05-13 10:38:02,383][training][INFO] - Epoch 57, Batch 900/3907, Loss: 0.377596\n",
      "[2025-05-13 10:38:02,785][training][INFO] - Epoch 57, Batch 1000/3907, Loss: 0.392006\n",
      "[2025-05-13 10:38:03,187][training][INFO] - Epoch 57, Batch 1100/3907, Loss: 0.384941\n",
      "[2025-05-13 10:38:03,588][training][INFO] - Epoch 57, Batch 1200/3907, Loss: 0.380702\n",
      "[2025-05-13 10:38:03,990][training][INFO] - Epoch 57, Batch 1300/3907, Loss: 0.383433\n",
      "[2025-05-13 10:38:04,393][training][INFO] - Epoch 57, Batch 1400/3907, Loss: 0.387942\n",
      "[2025-05-13 10:38:04,794][training][INFO] - Epoch 57, Batch 1500/3907, Loss: 0.375276\n",
      "[2025-05-13 10:38:05,199][training][INFO] - Epoch 57, Batch 1600/3907, Loss: 0.386508\n",
      "[2025-05-13 10:38:05,603][training][INFO] - Epoch 57, Batch 1700/3907, Loss: 0.390200\n",
      "[2025-05-13 10:38:06,010][training][INFO] - Epoch 57, Batch 1800/3907, Loss: 0.381322\n",
      "[2025-05-13 10:38:06,414][training][INFO] - Epoch 57, Batch 1900/3907, Loss: 0.383595\n",
      "[2025-05-13 10:38:06,816][training][INFO] - Epoch 57, Batch 2000/3907, Loss: 0.387764\n",
      "[2025-05-13 10:38:07,219][training][INFO] - Epoch 57, Batch 2100/3907, Loss: 0.382383\n",
      "[2025-05-13 10:38:07,622][training][INFO] - Epoch 57, Batch 2200/3907, Loss: 0.389419\n",
      "[2025-05-13 10:38:08,023][training][INFO] - Epoch 57, Batch 2300/3907, Loss: 0.373121\n",
      "[2025-05-13 10:38:08,424][training][INFO] - Epoch 57, Batch 2400/3907, Loss: 0.384265\n",
      "[2025-05-13 10:38:08,824][training][INFO] - Epoch 57, Batch 2500/3907, Loss: 0.391865\n",
      "[2025-05-13 10:38:09,224][training][INFO] - Epoch 57, Batch 2600/3907, Loss: 0.387511\n",
      "[2025-05-13 10:38:09,625][training][INFO] - Epoch 57, Batch 2700/3907, Loss: 0.382023\n",
      "[2025-05-13 10:38:10,025][training][INFO] - Epoch 57, Batch 2800/3907, Loss: 0.387851\n",
      "[2025-05-13 10:38:10,429][training][INFO] - Epoch 57, Batch 2900/3907, Loss: 0.383711\n",
      "[2025-05-13 10:38:10,829][training][INFO] - Epoch 57, Batch 3000/3907, Loss: 0.376027\n",
      "[2025-05-13 10:38:11,230][training][INFO] - Epoch 57, Batch 3100/3907, Loss: 0.382778\n",
      "[2025-05-13 10:38:11,634][training][INFO] - Epoch 57, Batch 3200/3907, Loss: 0.383657\n",
      "[2025-05-13 10:38:12,037][training][INFO] - Epoch 57, Batch 3300/3907, Loss: 0.388079\n",
      "[2025-05-13 10:38:12,437][training][INFO] - Epoch 57, Batch 3400/3907, Loss: 0.390391\n",
      "[2025-05-13 10:38:12,841][training][INFO] - Epoch 57, Batch 3500/3907, Loss: 0.379290\n",
      "[2025-05-13 10:38:13,244][training][INFO] - Epoch 57, Batch 3600/3907, Loss: 0.384267\n",
      "[2025-05-13 10:38:13,645][training][INFO] - Epoch 57, Batch 3700/3907, Loss: 0.387849\n",
      "[2025-05-13 10:38:14,047][training][INFO] - Epoch 57, Batch 3800/3907, Loss: 0.390588\n",
      "[2025-05-13 10:38:14,471][training][INFO] - Epoch 57, Batch 3900/3907, Loss: 0.379850\n",
      "[2025-05-13 10:38:14,488][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:38:14,488][training][INFO] - Epoch 57 complete. Avg Loss: 0.383317\n",
      "[2025-05-13 10:38:14,533][training][INFO] - Epoch 58, Batch 0/3907, Loss: 0.395054\n",
      "[2025-05-13 10:38:14,939][training][INFO] - Epoch 58, Batch 100/3907, Loss: 0.386311\n",
      "[2025-05-13 10:38:15,342][training][INFO] - Epoch 58, Batch 200/3907, Loss: 0.390931\n",
      "[2025-05-13 10:38:15,748][training][INFO] - Epoch 58, Batch 300/3907, Loss: 0.382520\n",
      "[2025-05-13 10:38:16,154][training][INFO] - Epoch 58, Batch 400/3907, Loss: 0.381177\n",
      "[2025-05-13 10:38:16,560][training][INFO] - Epoch 58, Batch 500/3907, Loss: 0.383991\n",
      "[2025-05-13 10:38:16,961][training][INFO] - Epoch 58, Batch 600/3907, Loss: 0.369950\n",
      "[2025-05-13 10:38:17,363][training][INFO] - Epoch 58, Batch 700/3907, Loss: 0.386496\n",
      "[2025-05-13 10:38:17,763][training][INFO] - Epoch 58, Batch 800/3907, Loss: 0.376744\n",
      "[2025-05-13 10:38:18,163][training][INFO] - Epoch 58, Batch 900/3907, Loss: 0.381977\n",
      "[2025-05-13 10:38:18,566][training][INFO] - Epoch 58, Batch 1000/3907, Loss: 0.388313\n",
      "[2025-05-13 10:38:18,968][training][INFO] - Epoch 58, Batch 1100/3907, Loss: 0.382176\n",
      "[2025-05-13 10:38:19,370][training][INFO] - Epoch 58, Batch 1200/3907, Loss: 0.382630\n",
      "[2025-05-13 10:38:19,772][training][INFO] - Epoch 58, Batch 1300/3907, Loss: 0.387252\n",
      "[2025-05-13 10:38:20,174][training][INFO] - Epoch 58, Batch 1400/3907, Loss: 0.381526\n",
      "[2025-05-13 10:38:20,576][training][INFO] - Epoch 58, Batch 1500/3907, Loss: 0.374500\n",
      "[2025-05-13 10:38:20,980][training][INFO] - Epoch 58, Batch 1600/3907, Loss: 0.380728\n",
      "[2025-05-13 10:38:21,381][training][INFO] - Epoch 58, Batch 1700/3907, Loss: 0.384521\n",
      "[2025-05-13 10:38:21,785][training][INFO] - Epoch 58, Batch 1800/3907, Loss: 0.383388\n",
      "[2025-05-13 10:38:22,186][training][INFO] - Epoch 58, Batch 1900/3907, Loss: 0.384152\n",
      "[2025-05-13 10:38:22,592][training][INFO] - Epoch 58, Batch 2000/3907, Loss: 0.379887\n",
      "[2025-05-13 10:38:23,002][training][INFO] - Epoch 58, Batch 2100/3907, Loss: 0.381646\n",
      "[2025-05-13 10:38:23,409][training][INFO] - Epoch 58, Batch 2200/3907, Loss: 0.379565\n",
      "[2025-05-13 10:38:23,812][training][INFO] - Epoch 58, Batch 2300/3907, Loss: 0.379248\n",
      "[2025-05-13 10:38:24,214][training][INFO] - Epoch 58, Batch 2400/3907, Loss: 0.380999\n",
      "[2025-05-13 10:38:24,616][training][INFO] - Epoch 58, Batch 2500/3907, Loss: 0.374827\n",
      "[2025-05-13 10:38:25,021][training][INFO] - Epoch 58, Batch 2600/3907, Loss: 0.387828\n",
      "[2025-05-13 10:38:25,426][training][INFO] - Epoch 58, Batch 2700/3907, Loss: 0.382554\n",
      "[2025-05-13 10:38:25,828][training][INFO] - Epoch 58, Batch 2800/3907, Loss: 0.386580\n",
      "[2025-05-13 10:38:26,231][training][INFO] - Epoch 58, Batch 2900/3907, Loss: 0.384849\n",
      "[2025-05-13 10:38:26,636][training][INFO] - Epoch 58, Batch 3000/3907, Loss: 0.392841\n",
      "[2025-05-13 10:38:27,038][training][INFO] - Epoch 58, Batch 3100/3907, Loss: 0.386106\n",
      "[2025-05-13 10:38:27,440][training][INFO] - Epoch 58, Batch 3200/3907, Loss: 0.384234\n",
      "[2025-05-13 10:38:27,840][training][INFO] - Epoch 58, Batch 3300/3907, Loss: 0.379394\n",
      "[2025-05-13 10:38:28,242][training][INFO] - Epoch 58, Batch 3400/3907, Loss: 0.372946\n",
      "[2025-05-13 10:38:28,643][training][INFO] - Epoch 58, Batch 3500/3907, Loss: 0.389846\n",
      "[2025-05-13 10:38:29,045][training][INFO] - Epoch 58, Batch 3600/3907, Loss: 0.389511\n",
      "[2025-05-13 10:38:29,448][training][INFO] - Epoch 58, Batch 3700/3907, Loss: 0.375070\n",
      "[2025-05-13 10:38:29,851][training][INFO] - Epoch 58, Batch 3800/3907, Loss: 0.385864\n",
      "[2025-05-13 10:38:30,270][training][INFO] - Epoch 58, Batch 3900/3907, Loss: 0.382196\n",
      "[2025-05-13 10:38:30,287][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:38:30,287][training][INFO] - Epoch 58 complete. Avg Loss: 0.383008\n",
      "[2025-05-13 10:38:30,328][training][INFO] - Epoch 59, Batch 0/3907, Loss: 0.378925\n",
      "[2025-05-13 10:38:30,741][training][INFO] - Epoch 59, Batch 100/3907, Loss: 0.383906\n",
      "[2025-05-13 10:38:31,145][training][INFO] - Epoch 59, Batch 200/3907, Loss: 0.393306\n",
      "[2025-05-13 10:38:31,546][training][INFO] - Epoch 59, Batch 300/3907, Loss: 0.388631\n",
      "[2025-05-13 10:38:31,947][training][INFO] - Epoch 59, Batch 400/3907, Loss: 0.378729\n",
      "[2025-05-13 10:38:32,349][training][INFO] - Epoch 59, Batch 500/3907, Loss: 0.376768\n",
      "[2025-05-13 10:38:32,751][training][INFO] - Epoch 59, Batch 600/3907, Loss: 0.387375\n",
      "[2025-05-13 10:38:33,152][training][INFO] - Epoch 59, Batch 700/3907, Loss: 0.380419\n",
      "[2025-05-13 10:38:33,554][training][INFO] - Epoch 59, Batch 800/3907, Loss: 0.382443\n",
      "[2025-05-13 10:38:33,957][training][INFO] - Epoch 59, Batch 900/3907, Loss: 0.390780\n",
      "[2025-05-13 10:38:34,358][training][INFO] - Epoch 59, Batch 1000/3907, Loss: 0.374683\n",
      "[2025-05-13 10:38:34,757][training][INFO] - Epoch 59, Batch 1100/3907, Loss: 0.383223\n",
      "[2025-05-13 10:38:35,158][training][INFO] - Epoch 59, Batch 1200/3907, Loss: 0.385664\n",
      "[2025-05-13 10:38:35,562][training][INFO] - Epoch 59, Batch 1300/3907, Loss: 0.382524\n",
      "[2025-05-13 10:38:35,963][training][INFO] - Epoch 59, Batch 1400/3907, Loss: 0.384056\n",
      "[2025-05-13 10:38:36,365][training][INFO] - Epoch 59, Batch 1500/3907, Loss: 0.377501\n",
      "[2025-05-13 10:38:36,767][training][INFO] - Epoch 59, Batch 1600/3907, Loss: 0.379479\n",
      "[2025-05-13 10:38:37,169][training][INFO] - Epoch 59, Batch 1700/3907, Loss: 0.382495\n",
      "[2025-05-13 10:38:37,571][training][INFO] - Epoch 59, Batch 1800/3907, Loss: 0.383390\n",
      "[2025-05-13 10:38:37,972][training][INFO] - Epoch 59, Batch 1900/3907, Loss: 0.384006\n",
      "[2025-05-13 10:38:38,376][training][INFO] - Epoch 59, Batch 2000/3907, Loss: 0.387179\n",
      "[2025-05-13 10:38:38,778][training][INFO] - Epoch 59, Batch 2100/3907, Loss: 0.370472\n",
      "[2025-05-13 10:38:39,181][training][INFO] - Epoch 59, Batch 2200/3907, Loss: 0.371768\n",
      "[2025-05-13 10:38:39,582][training][INFO] - Epoch 59, Batch 2300/3907, Loss: 0.383657\n",
      "[2025-05-13 10:38:39,984][training][INFO] - Epoch 59, Batch 2400/3907, Loss: 0.384882\n",
      "[2025-05-13 10:38:40,386][training][INFO] - Epoch 59, Batch 2500/3907, Loss: 0.388136\n",
      "[2025-05-13 10:38:40,788][training][INFO] - Epoch 59, Batch 2600/3907, Loss: 0.378717\n",
      "[2025-05-13 10:38:41,189][training][INFO] - Epoch 59, Batch 2700/3907, Loss: 0.380632\n",
      "[2025-05-13 10:38:41,591][training][INFO] - Epoch 59, Batch 2800/3907, Loss: 0.386621\n",
      "[2025-05-13 10:38:41,992][training][INFO] - Epoch 59, Batch 2900/3907, Loss: 0.386878\n",
      "[2025-05-13 10:38:42,396][training][INFO] - Epoch 59, Batch 3000/3907, Loss: 0.382520\n",
      "[2025-05-13 10:38:42,799][training][INFO] - Epoch 59, Batch 3100/3907, Loss: 0.385670\n",
      "[2025-05-13 10:38:43,202][training][INFO] - Epoch 59, Batch 3200/3907, Loss: 0.378558\n",
      "[2025-05-13 10:38:43,604][training][INFO] - Epoch 59, Batch 3300/3907, Loss: 0.389957\n",
      "[2025-05-13 10:38:44,006][training][INFO] - Epoch 59, Batch 3400/3907, Loss: 0.387696\n",
      "[2025-05-13 10:38:44,408][training][INFO] - Epoch 59, Batch 3500/3907, Loss: 0.375017\n",
      "[2025-05-13 10:38:44,809][training][INFO] - Epoch 59, Batch 3600/3907, Loss: 0.388310\n",
      "[2025-05-13 10:38:45,212][training][INFO] - Epoch 59, Batch 3700/3907, Loss: 0.384403\n",
      "[2025-05-13 10:38:45,618][training][INFO] - Epoch 59, Batch 3800/3907, Loss: 0.377333\n",
      "[2025-05-13 10:38:46,038][training][INFO] - Epoch 59, Batch 3900/3907, Loss: 0.385024\n",
      "[2025-05-13 10:38:46,055][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:38:46,055][training][INFO] - Epoch 59 complete. Avg Loss: 0.382677\n",
      "[2025-05-13 10:38:46,102][training][INFO] - Epoch 60, Batch 0/3907, Loss: 0.385522\n",
      "[2025-05-13 10:38:46,506][training][INFO] - Epoch 60, Batch 100/3907, Loss: 0.379375\n",
      "[2025-05-13 10:38:46,910][training][INFO] - Epoch 60, Batch 200/3907, Loss: 0.380286\n",
      "[2025-05-13 10:38:47,312][training][INFO] - Epoch 60, Batch 300/3907, Loss: 0.393550\n",
      "[2025-05-13 10:38:47,714][training][INFO] - Epoch 60, Batch 400/3907, Loss: 0.377996\n",
      "[2025-05-13 10:38:48,125][training][INFO] - Epoch 60, Batch 500/3907, Loss: 0.384560\n",
      "[2025-05-13 10:38:48,533][training][INFO] - Epoch 60, Batch 600/3907, Loss: 0.396810\n",
      "[2025-05-13 10:38:48,934][training][INFO] - Epoch 60, Batch 700/3907, Loss: 0.382715\n",
      "[2025-05-13 10:38:49,336][training][INFO] - Epoch 60, Batch 800/3907, Loss: 0.378701\n",
      "[2025-05-13 10:38:49,738][training][INFO] - Epoch 60, Batch 900/3907, Loss: 0.389136\n",
      "[2025-05-13 10:38:50,140][training][INFO] - Epoch 60, Batch 1000/3907, Loss: 0.380266\n",
      "[2025-05-13 10:38:50,543][training][INFO] - Epoch 60, Batch 1100/3907, Loss: 0.380534\n",
      "[2025-05-13 10:38:50,943][training][INFO] - Epoch 60, Batch 1200/3907, Loss: 0.386218\n",
      "[2025-05-13 10:38:51,346][training][INFO] - Epoch 60, Batch 1300/3907, Loss: 0.375986\n",
      "[2025-05-13 10:38:51,748][training][INFO] - Epoch 60, Batch 1400/3907, Loss: 0.374784\n",
      "[2025-05-13 10:38:52,156][training][INFO] - Epoch 60, Batch 1500/3907, Loss: 0.383998\n",
      "[2025-05-13 10:38:52,562][training][INFO] - Epoch 60, Batch 1600/3907, Loss: 0.381305\n",
      "[2025-05-13 10:38:52,964][training][INFO] - Epoch 60, Batch 1700/3907, Loss: 0.387021\n",
      "[2025-05-13 10:38:53,365][training][INFO] - Epoch 60, Batch 1800/3907, Loss: 0.383659\n",
      "[2025-05-13 10:38:53,766][training][INFO] - Epoch 60, Batch 1900/3907, Loss: 0.382133\n",
      "[2025-05-13 10:38:54,167][training][INFO] - Epoch 60, Batch 2000/3907, Loss: 0.379247\n",
      "[2025-05-13 10:38:54,570][training][INFO] - Epoch 60, Batch 2100/3907, Loss: 0.384892\n",
      "[2025-05-13 10:38:54,977][training][INFO] - Epoch 60, Batch 2200/3907, Loss: 0.384682\n",
      "[2025-05-13 10:38:55,377][training][INFO] - Epoch 60, Batch 2300/3907, Loss: 0.385166\n",
      "[2025-05-13 10:38:55,779][training][INFO] - Epoch 60, Batch 2400/3907, Loss: 0.380129\n",
      "[2025-05-13 10:38:56,182][training][INFO] - Epoch 60, Batch 2500/3907, Loss: 0.383692\n",
      "[2025-05-13 10:38:56,586][training][INFO] - Epoch 60, Batch 2600/3907, Loss: 0.385026\n",
      "[2025-05-13 10:38:56,987][training][INFO] - Epoch 60, Batch 2700/3907, Loss: 0.372992\n",
      "[2025-05-13 10:38:57,390][training][INFO] - Epoch 60, Batch 2800/3907, Loss: 0.391902\n",
      "[2025-05-13 10:38:57,791][training][INFO] - Epoch 60, Batch 2900/3907, Loss: 0.382589\n",
      "[2025-05-13 10:38:58,192][training][INFO] - Epoch 60, Batch 3000/3907, Loss: 0.377388\n",
      "[2025-05-13 10:38:58,597][training][INFO] - Epoch 60, Batch 3100/3907, Loss: 0.387359\n",
      "[2025-05-13 10:38:58,998][training][INFO] - Epoch 60, Batch 3200/3907, Loss: 0.383266\n",
      "[2025-05-13 10:38:59,400][training][INFO] - Epoch 60, Batch 3300/3907, Loss: 0.378032\n",
      "[2025-05-13 10:38:59,806][training][INFO] - Epoch 60, Batch 3400/3907, Loss: 0.393560\n",
      "[2025-05-13 10:39:00,210][training][INFO] - Epoch 60, Batch 3500/3907, Loss: 0.384158\n",
      "[2025-05-13 10:39:00,618][training][INFO] - Epoch 60, Batch 3600/3907, Loss: 0.378030\n",
      "[2025-05-13 10:39:01,023][training][INFO] - Epoch 60, Batch 3700/3907, Loss: 0.376470\n",
      "[2025-05-13 10:39:01,428][training][INFO] - Epoch 60, Batch 3800/3907, Loss: 0.383556\n",
      "[2025-05-13 10:39:01,849][training][INFO] - Epoch 60, Batch 3900/3907, Loss: 0.384874\n",
      "[2025-05-13 10:39:01,866][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:39:01,866][training][INFO] - Epoch 60 complete. Avg Loss: 0.382423\n",
      "[2025-05-13 10:39:01,873][training][INFO] - Saved checkpoint to /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7/checkpoint_epoch_60.pt\n",
      "[2025-05-13 10:39:01,912][training][INFO] - Epoch 61, Batch 0/3907, Loss: 0.385792\n",
      "[2025-05-13 10:39:02,327][training][INFO] - Epoch 61, Batch 100/3907, Loss: 0.376602\n",
      "[2025-05-13 10:39:02,730][training][INFO] - Epoch 61, Batch 200/3907, Loss: 0.372964\n",
      "[2025-05-13 10:39:03,130][training][INFO] - Epoch 61, Batch 300/3907, Loss: 0.389535\n",
      "[2025-05-13 10:39:03,534][training][INFO] - Epoch 61, Batch 400/3907, Loss: 0.384101\n",
      "[2025-05-13 10:39:03,936][training][INFO] - Epoch 61, Batch 500/3907, Loss: 0.385336\n",
      "[2025-05-13 10:39:04,338][training][INFO] - Epoch 61, Batch 600/3907, Loss: 0.384303\n",
      "[2025-05-13 10:39:04,738][training][INFO] - Epoch 61, Batch 700/3907, Loss: 0.386095\n",
      "[2025-05-13 10:39:05,140][training][INFO] - Epoch 61, Batch 800/3907, Loss: 0.375479\n",
      "[2025-05-13 10:39:05,542][training][INFO] - Epoch 61, Batch 900/3907, Loss: 0.381408\n",
      "[2025-05-13 10:39:05,944][training][INFO] - Epoch 61, Batch 1000/3907, Loss: 0.381007\n",
      "[2025-05-13 10:39:06,346][training][INFO] - Epoch 61, Batch 1100/3907, Loss: 0.375190\n",
      "[2025-05-13 10:39:06,750][training][INFO] - Epoch 61, Batch 1200/3907, Loss: 0.377665\n",
      "[2025-05-13 10:39:07,152][training][INFO] - Epoch 61, Batch 1300/3907, Loss: 0.380732\n",
      "[2025-05-13 10:39:07,551][training][INFO] - Epoch 61, Batch 1400/3907, Loss: 0.380478\n",
      "[2025-05-13 10:39:07,952][training][INFO] - Epoch 61, Batch 1500/3907, Loss: 0.377472\n",
      "[2025-05-13 10:39:08,354][training][INFO] - Epoch 61, Batch 1600/3907, Loss: 0.377748\n",
      "[2025-05-13 10:39:08,754][training][INFO] - Epoch 61, Batch 1700/3907, Loss: 0.385133\n",
      "[2025-05-13 10:39:09,155][training][INFO] - Epoch 61, Batch 1800/3907, Loss: 0.382300\n",
      "[2025-05-13 10:39:09,557][training][INFO] - Epoch 61, Batch 1900/3907, Loss: 0.373024\n",
      "[2025-05-13 10:39:09,959][training][INFO] - Epoch 61, Batch 2000/3907, Loss: 0.381010\n",
      "[2025-05-13 10:39:10,361][training][INFO] - Epoch 61, Batch 2100/3907, Loss: 0.382892\n",
      "[2025-05-13 10:39:10,762][training][INFO] - Epoch 61, Batch 2200/3907, Loss: 0.384431\n",
      "[2025-05-13 10:39:11,164][training][INFO] - Epoch 61, Batch 2300/3907, Loss: 0.371804\n",
      "[2025-05-13 10:39:11,565][training][INFO] - Epoch 61, Batch 2400/3907, Loss: 0.385812\n",
      "[2025-05-13 10:39:11,968][training][INFO] - Epoch 61, Batch 2500/3907, Loss: 0.370567\n",
      "[2025-05-13 10:39:12,369][training][INFO] - Epoch 61, Batch 2600/3907, Loss: 0.381348\n",
      "[2025-05-13 10:39:12,771][training][INFO] - Epoch 61, Batch 2700/3907, Loss: 0.393960\n",
      "[2025-05-13 10:39:13,174][training][INFO] - Epoch 61, Batch 2800/3907, Loss: 0.374502\n",
      "[2025-05-13 10:39:13,577][training][INFO] - Epoch 61, Batch 2900/3907, Loss: 0.379003\n",
      "[2025-05-13 10:39:13,978][training][INFO] - Epoch 61, Batch 3000/3907, Loss: 0.374618\n",
      "[2025-05-13 10:39:14,381][training][INFO] - Epoch 61, Batch 3100/3907, Loss: 0.394297\n",
      "[2025-05-13 10:39:14,783][training][INFO] - Epoch 61, Batch 3200/3907, Loss: 0.393071\n",
      "[2025-05-13 10:39:15,185][training][INFO] - Epoch 61, Batch 3300/3907, Loss: 0.383934\n",
      "[2025-05-13 10:39:15,589][training][INFO] - Epoch 61, Batch 3400/3907, Loss: 0.389234\n",
      "[2025-05-13 10:39:15,998][training][INFO] - Epoch 61, Batch 3500/3907, Loss: 0.390125\n",
      "[2025-05-13 10:39:16,399][training][INFO] - Epoch 61, Batch 3600/3907, Loss: 0.383983\n",
      "[2025-05-13 10:39:16,801][training][INFO] - Epoch 61, Batch 3700/3907, Loss: 0.389809\n",
      "[2025-05-13 10:39:17,203][training][INFO] - Epoch 61, Batch 3800/3907, Loss: 0.377335\n",
      "[2025-05-13 10:39:17,629][training][INFO] - Epoch 61, Batch 3900/3907, Loss: 0.380121\n",
      "[2025-05-13 10:39:17,646][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:39:17,646][training][INFO] - Epoch 61 complete. Avg Loss: 0.382138\n",
      "[2025-05-13 10:39:17,696][training][INFO] - Epoch 62, Batch 0/3907, Loss: 0.371776\n",
      "[2025-05-13 10:39:18,098][training][INFO] - Epoch 62, Batch 100/3907, Loss: 0.384498\n",
      "[2025-05-13 10:39:18,501][training][INFO] - Epoch 62, Batch 200/3907, Loss: 0.380995\n",
      "[2025-05-13 10:39:18,903][training][INFO] - Epoch 62, Batch 300/3907, Loss: 0.382903\n",
      "[2025-05-13 10:39:19,304][training][INFO] - Epoch 62, Batch 400/3907, Loss: 0.399705\n",
      "[2025-05-13 10:39:19,705][training][INFO] - Epoch 62, Batch 500/3907, Loss: 0.371609\n",
      "[2025-05-13 10:39:20,106][training][INFO] - Epoch 62, Batch 600/3907, Loss: 0.381804\n",
      "[2025-05-13 10:39:20,508][training][INFO] - Epoch 62, Batch 700/3907, Loss: 0.379544\n",
      "[2025-05-13 10:39:20,909][training][INFO] - Epoch 62, Batch 800/3907, Loss: 0.381869\n",
      "[2025-05-13 10:39:21,310][training][INFO] - Epoch 62, Batch 900/3907, Loss: 0.384397\n",
      "[2025-05-13 10:39:21,713][training][INFO] - Epoch 62, Batch 1000/3907, Loss: 0.382671\n",
      "[2025-05-13 10:39:22,116][training][INFO] - Epoch 62, Batch 1100/3907, Loss: 0.387231\n",
      "[2025-05-13 10:39:22,518][training][INFO] - Epoch 62, Batch 1200/3907, Loss: 0.376478\n",
      "[2025-05-13 10:39:22,918][training][INFO] - Epoch 62, Batch 1300/3907, Loss: 0.374955\n",
      "[2025-05-13 10:39:23,319][training][INFO] - Epoch 62, Batch 1400/3907, Loss: 0.378533\n",
      "[2025-05-13 10:39:23,721][training][INFO] - Epoch 62, Batch 1500/3907, Loss: 0.376430\n",
      "[2025-05-13 10:39:24,120][training][INFO] - Epoch 62, Batch 1600/3907, Loss: 0.390687\n",
      "[2025-05-13 10:39:24,519][training][INFO] - Epoch 62, Batch 1700/3907, Loss: 0.385130\n",
      "[2025-05-13 10:39:24,920][training][INFO] - Epoch 62, Batch 1800/3907, Loss: 0.382145\n",
      "[2025-05-13 10:39:25,320][training][INFO] - Epoch 62, Batch 1900/3907, Loss: 0.376231\n",
      "[2025-05-13 10:39:25,721][training][INFO] - Epoch 62, Batch 2000/3907, Loss: 0.390157\n",
      "[2025-05-13 10:39:26,117][training][INFO] - Epoch 62, Batch 2100/3907, Loss: 0.389098\n",
      "[2025-05-13 10:39:26,504][training][INFO] - Epoch 62, Batch 2200/3907, Loss: 0.386154\n",
      "[2025-05-13 10:39:26,894][training][INFO] - Epoch 62, Batch 2300/3907, Loss: 0.372218\n",
      "[2025-05-13 10:39:27,279][training][INFO] - Epoch 62, Batch 2400/3907, Loss: 0.388261\n",
      "[2025-05-13 10:39:27,666][training][INFO] - Epoch 62, Batch 2500/3907, Loss: 0.373561\n",
      "[2025-05-13 10:39:28,050][training][INFO] - Epoch 62, Batch 2600/3907, Loss: 0.380121\n",
      "[2025-05-13 10:39:28,435][training][INFO] - Epoch 62, Batch 2700/3907, Loss: 0.381213\n",
      "[2025-05-13 10:39:28,823][training][INFO] - Epoch 62, Batch 2800/3907, Loss: 0.375269\n",
      "[2025-05-13 10:39:29,211][training][INFO] - Epoch 62, Batch 2900/3907, Loss: 0.382236\n",
      "[2025-05-13 10:39:29,598][training][INFO] - Epoch 62, Batch 3000/3907, Loss: 0.384587\n",
      "[2025-05-13 10:39:29,984][training][INFO] - Epoch 62, Batch 3100/3907, Loss: 0.377741\n",
      "[2025-05-13 10:39:30,370][training][INFO] - Epoch 62, Batch 3200/3907, Loss: 0.378430\n",
      "[2025-05-13 10:39:30,758][training][INFO] - Epoch 62, Batch 3300/3907, Loss: 0.392029\n",
      "[2025-05-13 10:39:31,147][training][INFO] - Epoch 62, Batch 3400/3907, Loss: 0.377375\n",
      "[2025-05-13 10:39:31,540][training][INFO] - Epoch 62, Batch 3500/3907, Loss: 0.386218\n",
      "[2025-05-13 10:39:31,926][training][INFO] - Epoch 62, Batch 3600/3907, Loss: 0.387852\n",
      "[2025-05-13 10:39:32,312][training][INFO] - Epoch 62, Batch 3700/3907, Loss: 0.383468\n",
      "[2025-05-13 10:39:32,701][training][INFO] - Epoch 62, Batch 3800/3907, Loss: 0.376688\n",
      "[2025-05-13 10:39:33,105][training][INFO] - Epoch 62, Batch 3900/3907, Loss: 0.381890\n",
      "[2025-05-13 10:39:33,121][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:39:33,122][training][INFO] - Epoch 62 complete. Avg Loss: 0.381767\n",
      "[2025-05-13 10:39:33,165][training][INFO] - Epoch 63, Batch 0/3907, Loss: 0.389765\n",
      "[2025-05-13 10:39:33,554][training][INFO] - Epoch 63, Batch 100/3907, Loss: 0.380347\n",
      "[2025-05-13 10:39:33,940][training][INFO] - Epoch 63, Batch 200/3907, Loss: 0.385723\n",
      "[2025-05-13 10:39:34,329][training][INFO] - Epoch 63, Batch 300/3907, Loss: 0.368175\n",
      "[2025-05-13 10:39:34,717][training][INFO] - Epoch 63, Batch 400/3907, Loss: 0.390156\n",
      "[2025-05-13 10:39:35,101][training][INFO] - Epoch 63, Batch 500/3907, Loss: 0.388664\n",
      "[2025-05-13 10:39:35,488][training][INFO] - Epoch 63, Batch 600/3907, Loss: 0.369661\n",
      "[2025-05-13 10:39:35,875][training][INFO] - Epoch 63, Batch 700/3907, Loss: 0.381987\n",
      "[2025-05-13 10:39:36,260][training][INFO] - Epoch 63, Batch 800/3907, Loss: 0.380000\n",
      "[2025-05-13 10:39:36,645][training][INFO] - Epoch 63, Batch 900/3907, Loss: 0.375901\n",
      "[2025-05-13 10:39:37,029][training][INFO] - Epoch 63, Batch 1000/3907, Loss: 0.368935\n",
      "[2025-05-13 10:39:37,415][training][INFO] - Epoch 63, Batch 1100/3907, Loss: 0.377405\n",
      "[2025-05-13 10:39:37,804][training][INFO] - Epoch 63, Batch 1200/3907, Loss: 0.380444\n",
      "[2025-05-13 10:39:38,189][training][INFO] - Epoch 63, Batch 1300/3907, Loss: 0.382003\n",
      "[2025-05-13 10:39:38,575][training][INFO] - Epoch 63, Batch 1400/3907, Loss: 0.376201\n",
      "[2025-05-13 10:39:38,959][training][INFO] - Epoch 63, Batch 1500/3907, Loss: 0.382886\n",
      "[2025-05-13 10:39:39,345][training][INFO] - Epoch 63, Batch 1600/3907, Loss: 0.381529\n",
      "[2025-05-13 10:39:39,732][training][INFO] - Epoch 63, Batch 1700/3907, Loss: 0.383789\n",
      "[2025-05-13 10:39:40,119][training][INFO] - Epoch 63, Batch 1800/3907, Loss: 0.381380\n",
      "[2025-05-13 10:39:40,506][training][INFO] - Epoch 63, Batch 1900/3907, Loss: 0.384668\n",
      "[2025-05-13 10:39:40,892][training][INFO] - Epoch 63, Batch 2000/3907, Loss: 0.388567\n",
      "[2025-05-13 10:39:41,276][training][INFO] - Epoch 63, Batch 2100/3907, Loss: 0.375772\n",
      "[2025-05-13 10:39:41,663][training][INFO] - Epoch 63, Batch 2200/3907, Loss: 0.382507\n",
      "[2025-05-13 10:39:42,048][training][INFO] - Epoch 63, Batch 2300/3907, Loss: 0.383742\n",
      "[2025-05-13 10:39:42,433][training][INFO] - Epoch 63, Batch 2400/3907, Loss: 0.391181\n",
      "[2025-05-13 10:39:42,820][training][INFO] - Epoch 63, Batch 2500/3907, Loss: 0.381793\n",
      "[2025-05-13 10:39:43,210][training][INFO] - Epoch 63, Batch 2600/3907, Loss: 0.372625\n",
      "[2025-05-13 10:39:43,596][training][INFO] - Epoch 63, Batch 2700/3907, Loss: 0.387291\n",
      "[2025-05-13 10:39:43,986][training][INFO] - Epoch 63, Batch 2800/3907, Loss: 0.390070\n",
      "[2025-05-13 10:39:44,373][training][INFO] - Epoch 63, Batch 2900/3907, Loss: 0.380592\n",
      "[2025-05-13 10:39:44,761][training][INFO] - Epoch 63, Batch 3000/3907, Loss: 0.384051\n",
      "[2025-05-13 10:39:45,146][training][INFO] - Epoch 63, Batch 3100/3907, Loss: 0.385121\n",
      "[2025-05-13 10:39:45,537][training][INFO] - Epoch 63, Batch 3200/3907, Loss: 0.382394\n",
      "[2025-05-13 10:39:45,928][training][INFO] - Epoch 63, Batch 3300/3907, Loss: 0.372258\n",
      "[2025-05-13 10:39:46,315][training][INFO] - Epoch 63, Batch 3400/3907, Loss: 0.383597\n",
      "[2025-05-13 10:39:46,706][training][INFO] - Epoch 63, Batch 3500/3907, Loss: 0.385546\n",
      "[2025-05-13 10:39:47,094][training][INFO] - Epoch 63, Batch 3600/3907, Loss: 0.394597\n",
      "[2025-05-13 10:39:47,484][training][INFO] - Epoch 63, Batch 3700/3907, Loss: 0.380594\n",
      "[2025-05-13 10:39:47,874][training][INFO] - Epoch 63, Batch 3800/3907, Loss: 0.381605\n",
      "[2025-05-13 10:39:48,281][training][INFO] - Epoch 63, Batch 3900/3907, Loss: 0.378694\n",
      "[2025-05-13 10:39:48,298][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:39:48,298][training][INFO] - Epoch 63 complete. Avg Loss: 0.381505\n",
      "[2025-05-13 10:39:48,340][training][INFO] - Epoch 64, Batch 0/3907, Loss: 0.385961\n",
      "[2025-05-13 10:39:48,727][training][INFO] - Epoch 64, Batch 100/3907, Loss: 0.385216\n",
      "[2025-05-13 10:39:49,115][training][INFO] - Epoch 64, Batch 200/3907, Loss: 0.373901\n",
      "[2025-05-13 10:39:49,502][training][INFO] - Epoch 64, Batch 300/3907, Loss: 0.377620\n",
      "[2025-05-13 10:39:49,890][training][INFO] - Epoch 64, Batch 400/3907, Loss: 0.377760\n",
      "[2025-05-13 10:39:50,276][training][INFO] - Epoch 64, Batch 500/3907, Loss: 0.377958\n",
      "[2025-05-13 10:39:50,660][training][INFO] - Epoch 64, Batch 600/3907, Loss: 0.391036\n",
      "[2025-05-13 10:39:51,044][training][INFO] - Epoch 64, Batch 700/3907, Loss: 0.385141\n",
      "[2025-05-13 10:39:51,435][training][INFO] - Epoch 64, Batch 800/3907, Loss: 0.371357\n",
      "[2025-05-13 10:39:51,823][training][INFO] - Epoch 64, Batch 900/3907, Loss: 0.381461\n",
      "[2025-05-13 10:39:52,210][training][INFO] - Epoch 64, Batch 1000/3907, Loss: 0.379240\n",
      "[2025-05-13 10:39:52,595][training][INFO] - Epoch 64, Batch 1100/3907, Loss: 0.378919\n",
      "[2025-05-13 10:39:52,983][training][INFO] - Epoch 64, Batch 1200/3907, Loss: 0.379928\n",
      "[2025-05-13 10:39:53,370][training][INFO] - Epoch 64, Batch 1300/3907, Loss: 0.384841\n",
      "[2025-05-13 10:39:53,757][training][INFO] - Epoch 64, Batch 1400/3907, Loss: 0.379512\n",
      "[2025-05-13 10:39:54,144][training][INFO] - Epoch 64, Batch 1500/3907, Loss: 0.380633\n",
      "[2025-05-13 10:39:54,528][training][INFO] - Epoch 64, Batch 1600/3907, Loss: 0.371726\n",
      "[2025-05-13 10:39:54,915][training][INFO] - Epoch 64, Batch 1700/3907, Loss: 0.382296\n",
      "[2025-05-13 10:39:55,302][training][INFO] - Epoch 64, Batch 1800/3907, Loss: 0.381236\n",
      "[2025-05-13 10:39:55,687][training][INFO] - Epoch 64, Batch 1900/3907, Loss: 0.393025\n",
      "[2025-05-13 10:39:56,073][training][INFO] - Epoch 64, Batch 2000/3907, Loss: 0.376996\n",
      "[2025-05-13 10:39:56,463][training][INFO] - Epoch 64, Batch 2100/3907, Loss: 0.382921\n",
      "[2025-05-13 10:39:56,849][training][INFO] - Epoch 64, Batch 2200/3907, Loss: 0.387911\n",
      "[2025-05-13 10:39:57,234][training][INFO] - Epoch 64, Batch 2300/3907, Loss: 0.390016\n",
      "[2025-05-13 10:39:57,619][training][INFO] - Epoch 64, Batch 2400/3907, Loss: 0.373758\n",
      "[2025-05-13 10:39:58,007][training][INFO] - Epoch 64, Batch 2500/3907, Loss: 0.388100\n",
      "[2025-05-13 10:39:58,394][training][INFO] - Epoch 64, Batch 2600/3907, Loss: 0.385609\n",
      "[2025-05-13 10:39:58,781][training][INFO] - Epoch 64, Batch 2700/3907, Loss: 0.383718\n",
      "[2025-05-13 10:39:59,168][training][INFO] - Epoch 64, Batch 2800/3907, Loss: 0.382693\n",
      "[2025-05-13 10:39:59,556][training][INFO] - Epoch 64, Batch 2900/3907, Loss: 0.375232\n",
      "[2025-05-13 10:39:59,940][training][INFO] - Epoch 64, Batch 3000/3907, Loss: 0.388937\n",
      "[2025-05-13 10:40:00,325][training][INFO] - Epoch 64, Batch 3100/3907, Loss: 0.382911\n",
      "[2025-05-13 10:40:00,713][training][INFO] - Epoch 64, Batch 3200/3907, Loss: 0.382207\n",
      "[2025-05-13 10:40:01,100][training][INFO] - Epoch 64, Batch 3300/3907, Loss: 0.383203\n",
      "[2025-05-13 10:40:01,488][training][INFO] - Epoch 64, Batch 3400/3907, Loss: 0.379536\n",
      "[2025-05-13 10:40:01,872][training][INFO] - Epoch 64, Batch 3500/3907, Loss: 0.371951\n",
      "[2025-05-13 10:40:02,262][training][INFO] - Epoch 64, Batch 3600/3907, Loss: 0.373212\n",
      "[2025-05-13 10:40:02,646][training][INFO] - Epoch 64, Batch 3700/3907, Loss: 0.378320\n",
      "[2025-05-13 10:40:03,033][training][INFO] - Epoch 64, Batch 3800/3907, Loss: 0.374723\n",
      "[2025-05-13 10:40:03,436][training][INFO] - Epoch 64, Batch 3900/3907, Loss: 0.378153\n",
      "[2025-05-13 10:40:03,453][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:40:03,453][training][INFO] - Epoch 64 complete. Avg Loss: 0.381086\n",
      "[2025-05-13 10:40:03,498][training][INFO] - Epoch 65, Batch 0/3907, Loss: 0.377735\n",
      "[2025-05-13 10:40:03,884][training][INFO] - Epoch 65, Batch 100/3907, Loss: 0.382530\n",
      "[2025-05-13 10:40:04,271][training][INFO] - Epoch 65, Batch 200/3907, Loss: 0.377409\n",
      "[2025-05-13 10:40:04,658][training][INFO] - Epoch 65, Batch 300/3907, Loss: 0.388239\n",
      "[2025-05-13 10:40:05,047][training][INFO] - Epoch 65, Batch 400/3907, Loss: 0.372796\n",
      "[2025-05-13 10:40:05,434][training][INFO] - Epoch 65, Batch 500/3907, Loss: 0.382547\n",
      "[2025-05-13 10:40:05,825][training][INFO] - Epoch 65, Batch 600/3907, Loss: 0.380611\n",
      "[2025-05-13 10:40:06,214][training][INFO] - Epoch 65, Batch 700/3907, Loss: 0.387253\n",
      "[2025-05-13 10:40:06,598][training][INFO] - Epoch 65, Batch 800/3907, Loss: 0.378133\n",
      "[2025-05-13 10:40:06,986][training][INFO] - Epoch 65, Batch 900/3907, Loss: 0.379510\n",
      "[2025-05-13 10:40:07,369][training][INFO] - Epoch 65, Batch 1000/3907, Loss: 0.382166\n",
      "[2025-05-13 10:40:07,753][training][INFO] - Epoch 65, Batch 1100/3907, Loss: 0.377921\n",
      "[2025-05-13 10:40:08,139][training][INFO] - Epoch 65, Batch 1200/3907, Loss: 0.381104\n",
      "[2025-05-13 10:40:08,523][training][INFO] - Epoch 65, Batch 1300/3907, Loss: 0.375210\n",
      "[2025-05-13 10:40:08,913][training][INFO] - Epoch 65, Batch 1400/3907, Loss: 0.375529\n",
      "[2025-05-13 10:40:09,302][training][INFO] - Epoch 65, Batch 1500/3907, Loss: 0.381502\n",
      "[2025-05-13 10:40:09,690][training][INFO] - Epoch 65, Batch 1600/3907, Loss: 0.376968\n",
      "[2025-05-13 10:40:10,074][training][INFO] - Epoch 65, Batch 1700/3907, Loss: 0.384966\n",
      "[2025-05-13 10:40:10,460][training][INFO] - Epoch 65, Batch 1800/3907, Loss: 0.381000\n",
      "[2025-05-13 10:40:10,846][training][INFO] - Epoch 65, Batch 1900/3907, Loss: 0.383752\n",
      "[2025-05-13 10:40:11,235][training][INFO] - Epoch 65, Batch 2000/3907, Loss: 0.371239\n",
      "[2025-05-13 10:40:11,621][training][INFO] - Epoch 65, Batch 2100/3907, Loss: 0.378920\n",
      "[2025-05-13 10:40:12,006][training][INFO] - Epoch 65, Batch 2200/3907, Loss: 0.385363\n",
      "[2025-05-13 10:40:12,393][training][INFO] - Epoch 65, Batch 2300/3907, Loss: 0.379276\n",
      "[2025-05-13 10:40:12,781][training][INFO] - Epoch 65, Batch 2400/3907, Loss: 0.377432\n",
      "[2025-05-13 10:40:13,173][training][INFO] - Epoch 65, Batch 2500/3907, Loss: 0.383632\n",
      "[2025-05-13 10:40:13,561][training][INFO] - Epoch 65, Batch 2600/3907, Loss: 0.374439\n",
      "[2025-05-13 10:40:13,946][training][INFO] - Epoch 65, Batch 2700/3907, Loss: 0.377120\n",
      "[2025-05-13 10:40:14,334][training][INFO] - Epoch 65, Batch 2800/3907, Loss: 0.388315\n",
      "[2025-05-13 10:40:14,720][training][INFO] - Epoch 65, Batch 2900/3907, Loss: 0.380073\n",
      "[2025-05-13 10:40:15,106][training][INFO] - Epoch 65, Batch 3000/3907, Loss: 0.379109\n",
      "[2025-05-13 10:40:15,492][training][INFO] - Epoch 65, Batch 3100/3907, Loss: 0.375038\n",
      "[2025-05-13 10:40:15,886][training][INFO] - Epoch 65, Batch 3200/3907, Loss: 0.393300\n",
      "[2025-05-13 10:40:16,279][training][INFO] - Epoch 65, Batch 3300/3907, Loss: 0.388561\n",
      "[2025-05-13 10:40:16,668][training][INFO] - Epoch 65, Batch 3400/3907, Loss: 0.381828\n",
      "[2025-05-13 10:40:17,057][training][INFO] - Epoch 65, Batch 3500/3907, Loss: 0.375419\n",
      "[2025-05-13 10:40:17,443][training][INFO] - Epoch 65, Batch 3600/3907, Loss: 0.380735\n",
      "[2025-05-13 10:40:17,830][training][INFO] - Epoch 65, Batch 3700/3907, Loss: 0.372358\n",
      "[2025-05-13 10:40:18,219][training][INFO] - Epoch 65, Batch 3800/3907, Loss: 0.377223\n",
      "[2025-05-13 10:40:18,626][training][INFO] - Epoch 65, Batch 3900/3907, Loss: 0.387956\n",
      "[2025-05-13 10:40:18,642][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:40:18,643][training][INFO] - Epoch 65 complete. Avg Loss: 0.380918\n",
      "[2025-05-13 10:40:18,687][training][INFO] - Epoch 66, Batch 0/3907, Loss: 0.379224\n",
      "[2025-05-13 10:40:19,077][training][INFO] - Epoch 66, Batch 100/3907, Loss: 0.376191\n",
      "[2025-05-13 10:40:19,463][training][INFO] - Epoch 66, Batch 200/3907, Loss: 0.379041\n",
      "[2025-05-13 10:40:19,852][training][INFO] - Epoch 66, Batch 300/3907, Loss: 0.383588\n",
      "[2025-05-13 10:40:20,238][training][INFO] - Epoch 66, Batch 400/3907, Loss: 0.371085\n",
      "[2025-05-13 10:40:20,623][training][INFO] - Epoch 66, Batch 500/3907, Loss: 0.385430\n",
      "[2025-05-13 10:40:21,009][training][INFO] - Epoch 66, Batch 600/3907, Loss: 0.386202\n",
      "[2025-05-13 10:40:21,398][training][INFO] - Epoch 66, Batch 700/3907, Loss: 0.386182\n",
      "[2025-05-13 10:40:21,787][training][INFO] - Epoch 66, Batch 800/3907, Loss: 0.377300\n",
      "[2025-05-13 10:40:22,172][training][INFO] - Epoch 66, Batch 900/3907, Loss: 0.377614\n",
      "[2025-05-13 10:40:22,558][training][INFO] - Epoch 66, Batch 1000/3907, Loss: 0.382121\n",
      "[2025-05-13 10:40:22,947][training][INFO] - Epoch 66, Batch 1100/3907, Loss: 0.377233\n",
      "[2025-05-13 10:40:23,335][training][INFO] - Epoch 66, Batch 1200/3907, Loss: 0.376523\n",
      "[2025-05-13 10:40:23,719][training][INFO] - Epoch 66, Batch 1300/3907, Loss: 0.386085\n",
      "[2025-05-13 10:40:24,103][training][INFO] - Epoch 66, Batch 1400/3907, Loss: 0.375709\n",
      "[2025-05-13 10:40:24,488][training][INFO] - Epoch 66, Batch 1500/3907, Loss: 0.385046\n",
      "[2025-05-13 10:40:24,878][training][INFO] - Epoch 66, Batch 1600/3907, Loss: 0.379643\n",
      "[2025-05-13 10:40:25,265][training][INFO] - Epoch 66, Batch 1700/3907, Loss: 0.383956\n",
      "[2025-05-13 10:40:25,649][training][INFO] - Epoch 66, Batch 1800/3907, Loss: 0.381681\n",
      "[2025-05-13 10:40:26,038][training][INFO] - Epoch 66, Batch 1900/3907, Loss: 0.385010\n",
      "[2025-05-13 10:40:26,424][training][INFO] - Epoch 66, Batch 2000/3907, Loss: 0.378409\n",
      "[2025-05-13 10:40:26,813][training][INFO] - Epoch 66, Batch 2100/3907, Loss: 0.389653\n",
      "[2025-05-13 10:40:27,201][training][INFO] - Epoch 66, Batch 2200/3907, Loss: 0.391436\n",
      "[2025-05-13 10:40:27,587][training][INFO] - Epoch 66, Batch 2300/3907, Loss: 0.377074\n",
      "[2025-05-13 10:40:27,976][training][INFO] - Epoch 66, Batch 2400/3907, Loss: 0.375535\n",
      "[2025-05-13 10:40:28,365][training][INFO] - Epoch 66, Batch 2500/3907, Loss: 0.371451\n",
      "[2025-05-13 10:40:28,752][training][INFO] - Epoch 66, Batch 2600/3907, Loss: 0.389566\n",
      "[2025-05-13 10:40:29,143][training][INFO] - Epoch 66, Batch 2700/3907, Loss: 0.388748\n",
      "[2025-05-13 10:40:29,529][training][INFO] - Epoch 66, Batch 2800/3907, Loss: 0.384415\n",
      "[2025-05-13 10:40:29,915][training][INFO] - Epoch 66, Batch 2900/3907, Loss: 0.381203\n",
      "[2025-05-13 10:40:30,300][training][INFO] - Epoch 66, Batch 3000/3907, Loss: 0.378928\n",
      "[2025-05-13 10:40:30,689][training][INFO] - Epoch 66, Batch 3100/3907, Loss: 0.385325\n",
      "[2025-05-13 10:40:31,075][training][INFO] - Epoch 66, Batch 3200/3907, Loss: 0.376687\n",
      "[2025-05-13 10:40:31,463][training][INFO] - Epoch 66, Batch 3300/3907, Loss: 0.384726\n",
      "[2025-05-13 10:40:31,850][training][INFO] - Epoch 66, Batch 3400/3907, Loss: 0.380424\n",
      "[2025-05-13 10:40:32,241][training][INFO] - Epoch 66, Batch 3500/3907, Loss: 0.387141\n",
      "[2025-05-13 10:40:32,626][training][INFO] - Epoch 66, Batch 3600/3907, Loss: 0.386261\n",
      "[2025-05-13 10:40:33,017][training][INFO] - Epoch 66, Batch 3700/3907, Loss: 0.393807\n",
      "[2025-05-13 10:40:33,405][training][INFO] - Epoch 66, Batch 3800/3907, Loss: 0.389912\n",
      "[2025-05-13 10:40:33,810][training][INFO] - Epoch 66, Batch 3900/3907, Loss: 0.376370\n",
      "[2025-05-13 10:40:33,827][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:40:33,828][training][INFO] - Epoch 66 complete. Avg Loss: 0.380705\n",
      "[2025-05-13 10:40:33,870][training][INFO] - Epoch 67, Batch 0/3907, Loss: 0.391658\n",
      "[2025-05-13 10:40:34,256][training][INFO] - Epoch 67, Batch 100/3907, Loss: 0.377458\n",
      "[2025-05-13 10:40:34,642][training][INFO] - Epoch 67, Batch 200/3907, Loss: 0.379590\n",
      "[2025-05-13 10:40:35,027][training][INFO] - Epoch 67, Batch 300/3907, Loss: 0.378786\n",
      "[2025-05-13 10:40:35,412][training][INFO] - Epoch 67, Batch 400/3907, Loss: 0.369433\n",
      "[2025-05-13 10:40:35,798][training][INFO] - Epoch 67, Batch 500/3907, Loss: 0.377448\n",
      "[2025-05-13 10:40:36,182][training][INFO] - Epoch 67, Batch 600/3907, Loss: 0.371331\n",
      "[2025-05-13 10:40:36,565][training][INFO] - Epoch 67, Batch 700/3907, Loss: 0.386990\n",
      "[2025-05-13 10:40:36,954][training][INFO] - Epoch 67, Batch 800/3907, Loss: 0.377443\n",
      "[2025-05-13 10:40:37,342][training][INFO] - Epoch 67, Batch 900/3907, Loss: 0.378593\n",
      "[2025-05-13 10:40:37,726][training][INFO] - Epoch 67, Batch 1000/3907, Loss: 0.383253\n",
      "[2025-05-13 10:40:38,113][training][INFO] - Epoch 67, Batch 1100/3907, Loss: 0.379147\n",
      "[2025-05-13 10:40:38,499][training][INFO] - Epoch 67, Batch 1200/3907, Loss: 0.377969\n",
      "[2025-05-13 10:40:38,887][training][INFO] - Epoch 67, Batch 1300/3907, Loss: 0.380879\n",
      "[2025-05-13 10:40:39,273][training][INFO] - Epoch 67, Batch 1400/3907, Loss: 0.384195\n",
      "[2025-05-13 10:40:39,657][training][INFO] - Epoch 67, Batch 1500/3907, Loss: 0.381646\n",
      "[2025-05-13 10:40:40,046][training][INFO] - Epoch 67, Batch 1600/3907, Loss: 0.379142\n",
      "[2025-05-13 10:40:40,433][training][INFO] - Epoch 67, Batch 1700/3907, Loss: 0.383655\n",
      "[2025-05-13 10:40:40,821][training][INFO] - Epoch 67, Batch 1800/3907, Loss: 0.380003\n",
      "[2025-05-13 10:40:41,207][training][INFO] - Epoch 67, Batch 1900/3907, Loss: 0.381174\n",
      "[2025-05-13 10:40:41,592][training][INFO] - Epoch 67, Batch 2000/3907, Loss: 0.385138\n",
      "[2025-05-13 10:40:41,982][training][INFO] - Epoch 67, Batch 2100/3907, Loss: 0.383100\n",
      "[2025-05-13 10:40:42,370][training][INFO] - Epoch 67, Batch 2200/3907, Loss: 0.379907\n",
      "[2025-05-13 10:40:42,754][training][INFO] - Epoch 67, Batch 2300/3907, Loss: 0.386446\n",
      "[2025-05-13 10:40:43,145][training][INFO] - Epoch 67, Batch 2400/3907, Loss: 0.382281\n",
      "[2025-05-13 10:40:43,534][training][INFO] - Epoch 67, Batch 2500/3907, Loss: 0.376317\n",
      "[2025-05-13 10:40:43,925][training][INFO] - Epoch 67, Batch 2600/3907, Loss: 0.385005\n",
      "[2025-05-13 10:40:44,314][training][INFO] - Epoch 67, Batch 2700/3907, Loss: 0.382454\n",
      "[2025-05-13 10:40:44,702][training][INFO] - Epoch 67, Batch 2800/3907, Loss: 0.386349\n",
      "[2025-05-13 10:40:45,088][training][INFO] - Epoch 67, Batch 2900/3907, Loss: 0.379840\n",
      "[2025-05-13 10:40:45,477][training][INFO] - Epoch 67, Batch 3000/3907, Loss: 0.381050\n",
      "[2025-05-13 10:40:45,870][training][INFO] - Epoch 67, Batch 3100/3907, Loss: 0.387171\n",
      "[2025-05-13 10:40:46,260][training][INFO] - Epoch 67, Batch 3200/3907, Loss: 0.382925\n",
      "[2025-05-13 10:40:46,650][training][INFO] - Epoch 67, Batch 3300/3907, Loss: 0.384781\n",
      "[2025-05-13 10:40:47,038][training][INFO] - Epoch 67, Batch 3400/3907, Loss: 0.377649\n",
      "[2025-05-13 10:40:47,426][training][INFO] - Epoch 67, Batch 3500/3907, Loss: 0.384333\n",
      "[2025-05-13 10:40:47,811][training][INFO] - Epoch 67, Batch 3600/3907, Loss: 0.371275\n",
      "[2025-05-13 10:40:48,196][training][INFO] - Epoch 67, Batch 3700/3907, Loss: 0.379598\n",
      "[2025-05-13 10:40:48,580][training][INFO] - Epoch 67, Batch 3800/3907, Loss: 0.377777\n",
      "[2025-05-13 10:40:48,985][training][INFO] - Epoch 67, Batch 3900/3907, Loss: 0.392976\n",
      "[2025-05-13 10:40:49,002][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:40:49,002][training][INFO] - Epoch 67 complete. Avg Loss: 0.380400\n",
      "[2025-05-13 10:40:49,042][training][INFO] - Epoch 68, Batch 0/3907, Loss: 0.388056\n",
      "[2025-05-13 10:40:49,434][training][INFO] - Epoch 68, Batch 100/3907, Loss: 0.386401\n",
      "[2025-05-13 10:40:49,818][training][INFO] - Epoch 68, Batch 200/3907, Loss: 0.372120\n",
      "[2025-05-13 10:40:50,205][training][INFO] - Epoch 68, Batch 300/3907, Loss: 0.376904\n",
      "[2025-05-13 10:40:50,594][training][INFO] - Epoch 68, Batch 400/3907, Loss: 0.379274\n",
      "[2025-05-13 10:40:50,978][training][INFO] - Epoch 68, Batch 500/3907, Loss: 0.379272\n",
      "[2025-05-13 10:40:51,364][training][INFO] - Epoch 68, Batch 600/3907, Loss: 0.374719\n",
      "[2025-05-13 10:40:51,752][training][INFO] - Epoch 68, Batch 700/3907, Loss: 0.388445\n",
      "[2025-05-13 10:40:52,137][training][INFO] - Epoch 68, Batch 800/3907, Loss: 0.381333\n",
      "[2025-05-13 10:40:52,521][training][INFO] - Epoch 68, Batch 900/3907, Loss: 0.382761\n",
      "[2025-05-13 10:40:52,907][training][INFO] - Epoch 68, Batch 1000/3907, Loss: 0.382794\n",
      "[2025-05-13 10:40:53,295][training][INFO] - Epoch 68, Batch 1100/3907, Loss: 0.383405\n",
      "[2025-05-13 10:40:53,684][training][INFO] - Epoch 68, Batch 1200/3907, Loss: 0.382005\n",
      "[2025-05-13 10:40:54,072][training][INFO] - Epoch 68, Batch 1300/3907, Loss: 0.387634\n",
      "[2025-05-13 10:40:54,456][training][INFO] - Epoch 68, Batch 1400/3907, Loss: 0.380014\n",
      "[2025-05-13 10:40:54,841][training][INFO] - Epoch 68, Batch 1500/3907, Loss: 0.374761\n",
      "[2025-05-13 10:40:55,230][training][INFO] - Epoch 68, Batch 1600/3907, Loss: 0.378523\n",
      "[2025-05-13 10:40:55,615][training][INFO] - Epoch 68, Batch 1700/3907, Loss: 0.382946\n",
      "[2025-05-13 10:40:55,999][training][INFO] - Epoch 68, Batch 1800/3907, Loss: 0.374383\n",
      "[2025-05-13 10:40:56,384][training][INFO] - Epoch 68, Batch 1900/3907, Loss: 0.373782\n",
      "[2025-05-13 10:40:56,769][training][INFO] - Epoch 68, Batch 2000/3907, Loss: 0.376069\n",
      "[2025-05-13 10:40:57,155][training][INFO] - Epoch 68, Batch 2100/3907, Loss: 0.380025\n",
      "[2025-05-13 10:40:57,540][training][INFO] - Epoch 68, Batch 2200/3907, Loss: 0.390935\n",
      "[2025-05-13 10:40:57,928][training][INFO] - Epoch 68, Batch 2300/3907, Loss: 0.385280\n",
      "[2025-05-13 10:40:58,312][training][INFO] - Epoch 68, Batch 2400/3907, Loss: 0.384362\n",
      "[2025-05-13 10:40:58,698][training][INFO] - Epoch 68, Batch 2500/3907, Loss: 0.376504\n",
      "[2025-05-13 10:40:59,082][training][INFO] - Epoch 68, Batch 2600/3907, Loss: 0.382131\n",
      "[2025-05-13 10:40:59,465][training][INFO] - Epoch 68, Batch 2700/3907, Loss: 0.378259\n",
      "[2025-05-13 10:40:59,854][training][INFO] - Epoch 68, Batch 2800/3907, Loss: 0.385829\n",
      "[2025-05-13 10:41:00,242][training][INFO] - Epoch 68, Batch 2900/3907, Loss: 0.378538\n",
      "[2025-05-13 10:41:00,631][training][INFO] - Epoch 68, Batch 3000/3907, Loss: 0.374219\n",
      "[2025-05-13 10:41:01,020][training][INFO] - Epoch 68, Batch 3100/3907, Loss: 0.375678\n",
      "[2025-05-13 10:41:01,405][training][INFO] - Epoch 68, Batch 3200/3907, Loss: 0.380119\n",
      "[2025-05-13 10:41:01,794][training][INFO] - Epoch 68, Batch 3300/3907, Loss: 0.378450\n",
      "[2025-05-13 10:41:02,181][training][INFO] - Epoch 68, Batch 3400/3907, Loss: 0.388477\n",
      "[2025-05-13 10:41:02,571][training][INFO] - Epoch 68, Batch 3500/3907, Loss: 0.383279\n",
      "[2025-05-13 10:41:02,961][training][INFO] - Epoch 68, Batch 3600/3907, Loss: 0.380619\n",
      "[2025-05-13 10:41:03,350][training][INFO] - Epoch 68, Batch 3700/3907, Loss: 0.388384\n",
      "[2025-05-13 10:41:03,737][training][INFO] - Epoch 68, Batch 3800/3907, Loss: 0.381589\n",
      "[2025-05-13 10:41:04,143][training][INFO] - Epoch 68, Batch 3900/3907, Loss: 0.372388\n",
      "[2025-05-13 10:41:04,159][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:41:04,159][training][INFO] - Epoch 68 complete. Avg Loss: 0.380288\n",
      "[2025-05-13 10:41:04,205][training][INFO] - Epoch 69, Batch 0/3907, Loss: 0.380364\n",
      "[2025-05-13 10:41:04,601][training][INFO] - Epoch 69, Batch 100/3907, Loss: 0.374169\n",
      "[2025-05-13 10:41:04,988][training][INFO] - Epoch 69, Batch 200/3907, Loss: 0.367851\n",
      "[2025-05-13 10:41:05,375][training][INFO] - Epoch 69, Batch 300/3907, Loss: 0.374073\n",
      "[2025-05-13 10:41:05,762][training][INFO] - Epoch 69, Batch 400/3907, Loss: 0.385532\n",
      "[2025-05-13 10:41:06,152][training][INFO] - Epoch 69, Batch 500/3907, Loss: 0.378993\n",
      "[2025-05-13 10:41:06,538][training][INFO] - Epoch 69, Batch 600/3907, Loss: 0.379981\n",
      "[2025-05-13 10:41:06,925][training][INFO] - Epoch 69, Batch 700/3907, Loss: 0.379411\n",
      "[2025-05-13 10:41:07,313][training][INFO] - Epoch 69, Batch 800/3907, Loss: 0.380135\n",
      "[2025-05-13 10:41:07,700][training][INFO] - Epoch 69, Batch 900/3907, Loss: 0.373609\n",
      "[2025-05-13 10:41:08,091][training][INFO] - Epoch 69, Batch 1000/3907, Loss: 0.370574\n",
      "[2025-05-13 10:41:08,481][training][INFO] - Epoch 69, Batch 1100/3907, Loss: 0.381772\n",
      "[2025-05-13 10:41:08,867][training][INFO] - Epoch 69, Batch 1200/3907, Loss: 0.384540\n",
      "[2025-05-13 10:41:09,257][training][INFO] - Epoch 69, Batch 1300/3907, Loss: 0.386647\n",
      "[2025-05-13 10:41:09,648][training][INFO] - Epoch 69, Batch 1400/3907, Loss: 0.377256\n",
      "[2025-05-13 10:41:10,036][training][INFO] - Epoch 69, Batch 1500/3907, Loss: 0.379107\n",
      "[2025-05-13 10:41:10,428][training][INFO] - Epoch 69, Batch 1600/3907, Loss: 0.382002\n",
      "[2025-05-13 10:41:10,817][training][INFO] - Epoch 69, Batch 1700/3907, Loss: 0.389363\n",
      "[2025-05-13 10:41:11,207][training][INFO] - Epoch 69, Batch 1800/3907, Loss: 0.386625\n",
      "[2025-05-13 10:41:11,595][training][INFO] - Epoch 69, Batch 1900/3907, Loss: 0.380337\n",
      "[2025-05-13 10:41:11,983][training][INFO] - Epoch 69, Batch 2000/3907, Loss: 0.374494\n",
      "[2025-05-13 10:41:12,369][training][INFO] - Epoch 69, Batch 2100/3907, Loss: 0.377627\n",
      "[2025-05-13 10:41:12,759][training][INFO] - Epoch 69, Batch 2200/3907, Loss: 0.380709\n",
      "[2025-05-13 10:41:13,147][training][INFO] - Epoch 69, Batch 2300/3907, Loss: 0.383293\n",
      "[2025-05-13 10:41:13,536][training][INFO] - Epoch 69, Batch 2400/3907, Loss: 0.378659\n",
      "[2025-05-13 10:41:13,926][training][INFO] - Epoch 69, Batch 2500/3907, Loss: 0.380500\n",
      "[2025-05-13 10:41:14,315][training][INFO] - Epoch 69, Batch 2600/3907, Loss: 0.377346\n",
      "[2025-05-13 10:41:14,708][training][INFO] - Epoch 69, Batch 2700/3907, Loss: 0.380979\n",
      "[2025-05-13 10:41:15,095][training][INFO] - Epoch 69, Batch 2800/3907, Loss: 0.381490\n",
      "[2025-05-13 10:41:15,485][training][INFO] - Epoch 69, Batch 2900/3907, Loss: 0.388266\n",
      "[2025-05-13 10:41:15,877][training][INFO] - Epoch 69, Batch 3000/3907, Loss: 0.374230\n",
      "[2025-05-13 10:41:16,268][training][INFO] - Epoch 69, Batch 3100/3907, Loss: 0.373039\n",
      "[2025-05-13 10:41:16,654][training][INFO] - Epoch 69, Batch 3200/3907, Loss: 0.379196\n",
      "[2025-05-13 10:41:17,040][training][INFO] - Epoch 69, Batch 3300/3907, Loss: 0.380250\n",
      "[2025-05-13 10:41:17,433][training][INFO] - Epoch 69, Batch 3400/3907, Loss: 0.375295\n",
      "[2025-05-13 10:41:17,823][training][INFO] - Epoch 69, Batch 3500/3907, Loss: 0.380200\n",
      "[2025-05-13 10:41:18,208][training][INFO] - Epoch 69, Batch 3600/3907, Loss: 0.376567\n",
      "[2025-05-13 10:41:18,596][training][INFO] - Epoch 69, Batch 3700/3907, Loss: 0.384497\n",
      "[2025-05-13 10:41:18,987][training][INFO] - Epoch 69, Batch 3800/3907, Loss: 0.371232\n",
      "[2025-05-13 10:41:19,397][training][INFO] - Epoch 69, Batch 3900/3907, Loss: 0.384221\n",
      "[2025-05-13 10:41:19,414][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:41:19,414][training][INFO] - Epoch 69 complete. Avg Loss: 0.380101\n",
      "[2025-05-13 10:41:19,454][training][INFO] - Epoch 70, Batch 0/3907, Loss: 0.375528\n",
      "[2025-05-13 10:41:19,841][training][INFO] - Epoch 70, Batch 100/3907, Loss: 0.369899\n",
      "[2025-05-13 10:41:20,226][training][INFO] - Epoch 70, Batch 200/3907, Loss: 0.372912\n",
      "[2025-05-13 10:41:20,613][training][INFO] - Epoch 70, Batch 300/3907, Loss: 0.380456\n",
      "[2025-05-13 10:41:21,010][training][INFO] - Epoch 70, Batch 400/3907, Loss: 0.382606\n",
      "[2025-05-13 10:41:21,402][training][INFO] - Epoch 70, Batch 500/3907, Loss: 0.381566\n",
      "[2025-05-13 10:41:21,791][training][INFO] - Epoch 70, Batch 600/3907, Loss: 0.389651\n",
      "[2025-05-13 10:41:22,181][training][INFO] - Epoch 70, Batch 700/3907, Loss: 0.388597\n",
      "[2025-05-13 10:41:22,570][training][INFO] - Epoch 70, Batch 800/3907, Loss: 0.390914\n",
      "[2025-05-13 10:41:22,954][training][INFO] - Epoch 70, Batch 900/3907, Loss: 0.381735\n",
      "[2025-05-13 10:41:23,341][training][INFO] - Epoch 70, Batch 1000/3907, Loss: 0.384593\n",
      "[2025-05-13 10:41:23,731][training][INFO] - Epoch 70, Batch 1100/3907, Loss: 0.378600\n",
      "[2025-05-13 10:41:24,120][training][INFO] - Epoch 70, Batch 1200/3907, Loss: 0.379331\n",
      "[2025-05-13 10:41:24,507][training][INFO] - Epoch 70, Batch 1300/3907, Loss: 0.378500\n",
      "[2025-05-13 10:41:24,895][training][INFO] - Epoch 70, Batch 1400/3907, Loss: 0.379197\n",
      "[2025-05-13 10:41:25,278][training][INFO] - Epoch 70, Batch 1500/3907, Loss: 0.380324\n",
      "[2025-05-13 10:41:25,664][training][INFO] - Epoch 70, Batch 1600/3907, Loss: 0.378698\n",
      "[2025-05-13 10:41:26,051][training][INFO] - Epoch 70, Batch 1700/3907, Loss: 0.375869\n",
      "[2025-05-13 10:41:26,438][training][INFO] - Epoch 70, Batch 1800/3907, Loss: 0.378086\n",
      "[2025-05-13 10:41:26,829][training][INFO] - Epoch 70, Batch 1900/3907, Loss: 0.378943\n",
      "[2025-05-13 10:41:27,222][training][INFO] - Epoch 70, Batch 2000/3907, Loss: 0.376538\n",
      "[2025-05-13 10:41:27,613][training][INFO] - Epoch 70, Batch 2100/3907, Loss: 0.373410\n",
      "[2025-05-13 10:41:28,004][training][INFO] - Epoch 70, Batch 2200/3907, Loss: 0.379253\n",
      "[2025-05-13 10:41:28,394][training][INFO] - Epoch 70, Batch 2300/3907, Loss: 0.380659\n",
      "[2025-05-13 10:41:28,785][training][INFO] - Epoch 70, Batch 2400/3907, Loss: 0.376919\n",
      "[2025-05-13 10:41:29,173][training][INFO] - Epoch 70, Batch 2500/3907, Loss: 0.379329\n",
      "[2025-05-13 10:41:29,563][training][INFO] - Epoch 70, Batch 2600/3907, Loss: 0.379101\n",
      "[2025-05-13 10:41:29,949][training][INFO] - Epoch 70, Batch 2700/3907, Loss: 0.375572\n",
      "[2025-05-13 10:41:30,335][training][INFO] - Epoch 70, Batch 2800/3907, Loss: 0.378796\n",
      "[2025-05-13 10:41:30,726][training][INFO] - Epoch 70, Batch 2900/3907, Loss: 0.378911\n",
      "[2025-05-13 10:41:31,113][training][INFO] - Epoch 70, Batch 3000/3907, Loss: 0.382014\n",
      "[2025-05-13 10:41:31,505][training][INFO] - Epoch 70, Batch 3100/3907, Loss: 0.383315\n",
      "[2025-05-13 10:41:31,893][training][INFO] - Epoch 70, Batch 3200/3907, Loss: 0.377712\n",
      "[2025-05-13 10:41:32,285][training][INFO] - Epoch 70, Batch 3300/3907, Loss: 0.381196\n",
      "[2025-05-13 10:41:32,676][training][INFO] - Epoch 70, Batch 3400/3907, Loss: 0.382907\n",
      "[2025-05-13 10:41:33,065][training][INFO] - Epoch 70, Batch 3500/3907, Loss: 0.377588\n",
      "[2025-05-13 10:41:33,454][training][INFO] - Epoch 70, Batch 3600/3907, Loss: 0.386092\n",
      "[2025-05-13 10:41:33,845][training][INFO] - Epoch 70, Batch 3700/3907, Loss: 0.379628\n",
      "[2025-05-13 10:41:34,238][training][INFO] - Epoch 70, Batch 3800/3907, Loss: 0.388509\n",
      "[2025-05-13 10:41:34,663][training][INFO] - Epoch 70, Batch 3900/3907, Loss: 0.380004\n",
      "[2025-05-13 10:41:34,680][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:41:34,681][training][INFO] - Epoch 70 complete. Avg Loss: 0.379940\n",
      "[2025-05-13 10:41:34,729][training][INFO] - Epoch 71, Batch 0/3907, Loss: 0.377288\n",
      "[2025-05-13 10:41:35,124][training][INFO] - Epoch 71, Batch 100/3907, Loss: 0.384387\n",
      "[2025-05-13 10:41:35,514][training][INFO] - Epoch 71, Batch 200/3907, Loss: 0.379100\n",
      "[2025-05-13 10:41:35,903][training][INFO] - Epoch 71, Batch 300/3907, Loss: 0.384858\n",
      "[2025-05-13 10:41:36,287][training][INFO] - Epoch 71, Batch 400/3907, Loss: 0.373334\n",
      "[2025-05-13 10:41:36,677][training][INFO] - Epoch 71, Batch 500/3907, Loss: 0.378885\n",
      "[2025-05-13 10:41:37,063][training][INFO] - Epoch 71, Batch 600/3907, Loss: 0.390466\n",
      "[2025-05-13 10:41:37,448][training][INFO] - Epoch 71, Batch 700/3907, Loss: 0.386287\n",
      "[2025-05-13 10:41:37,837][training][INFO] - Epoch 71, Batch 800/3907, Loss: 0.381635\n",
      "[2025-05-13 10:41:38,232][training][INFO] - Epoch 71, Batch 900/3907, Loss: 0.377257\n",
      "[2025-05-13 10:41:38,617][training][INFO] - Epoch 71, Batch 1000/3907, Loss: 0.377722\n",
      "[2025-05-13 10:41:39,006][training][INFO] - Epoch 71, Batch 1100/3907, Loss: 0.381283\n",
      "[2025-05-13 10:41:39,393][training][INFO] - Epoch 71, Batch 1200/3907, Loss: 0.378519\n",
      "[2025-05-13 10:41:39,777][training][INFO] - Epoch 71, Batch 1300/3907, Loss: 0.378017\n",
      "[2025-05-13 10:41:40,164][training][INFO] - Epoch 71, Batch 1400/3907, Loss: 0.371483\n",
      "[2025-05-13 10:41:40,550][training][INFO] - Epoch 71, Batch 1500/3907, Loss: 0.386920\n",
      "[2025-05-13 10:41:40,935][training][INFO] - Epoch 71, Batch 1600/3907, Loss: 0.380698\n",
      "[2025-05-13 10:41:41,319][training][INFO] - Epoch 71, Batch 1700/3907, Loss: 0.379246\n",
      "[2025-05-13 10:41:41,707][training][INFO] - Epoch 71, Batch 1800/3907, Loss: 0.377755\n",
      "[2025-05-13 10:41:42,093][training][INFO] - Epoch 71, Batch 1900/3907, Loss: 0.372817\n",
      "[2025-05-13 10:41:42,481][training][INFO] - Epoch 71, Batch 2000/3907, Loss: 0.375688\n",
      "[2025-05-13 10:41:42,866][training][INFO] - Epoch 71, Batch 2100/3907, Loss: 0.381454\n",
      "[2025-05-13 10:41:43,254][training][INFO] - Epoch 71, Batch 2200/3907, Loss: 0.383745\n",
      "[2025-05-13 10:41:43,645][training][INFO] - Epoch 71, Batch 2300/3907, Loss: 0.373946\n",
      "[2025-05-13 10:41:44,035][training][INFO] - Epoch 71, Batch 2400/3907, Loss: 0.385309\n",
      "[2025-05-13 10:41:44,425][training][INFO] - Epoch 71, Batch 2500/3907, Loss: 0.384464\n",
      "[2025-05-13 10:41:44,813][training][INFO] - Epoch 71, Batch 2600/3907, Loss: 0.377177\n",
      "[2025-05-13 10:41:45,204][training][INFO] - Epoch 71, Batch 2700/3907, Loss: 0.378029\n",
      "[2025-05-13 10:41:45,597][training][INFO] - Epoch 71, Batch 2800/3907, Loss: 0.373802\n",
      "[2025-05-13 10:41:45,991][training][INFO] - Epoch 71, Batch 2900/3907, Loss: 0.380954\n",
      "[2025-05-13 10:41:46,385][training][INFO] - Epoch 71, Batch 3000/3907, Loss: 0.386629\n",
      "[2025-05-13 10:41:46,785][training][INFO] - Epoch 71, Batch 3100/3907, Loss: 0.374021\n",
      "[2025-05-13 10:41:47,187][training][INFO] - Epoch 71, Batch 3200/3907, Loss: 0.379872\n",
      "[2025-05-13 10:41:47,587][training][INFO] - Epoch 71, Batch 3300/3907, Loss: 0.386611\n",
      "[2025-05-13 10:41:47,987][training][INFO] - Epoch 71, Batch 3400/3907, Loss: 0.375932\n",
      "[2025-05-13 10:41:48,390][training][INFO] - Epoch 71, Batch 3500/3907, Loss: 0.386599\n",
      "[2025-05-13 10:41:48,805][training][INFO] - Epoch 71, Batch 3600/3907, Loss: 0.378701\n",
      "[2025-05-13 10:41:49,207][training][INFO] - Epoch 71, Batch 3700/3907, Loss: 0.378424\n",
      "[2025-05-13 10:41:49,608][training][INFO] - Epoch 71, Batch 3800/3907, Loss: 0.383220\n",
      "[2025-05-13 10:41:50,026][training][INFO] - Epoch 71, Batch 3900/3907, Loss: 0.390064\n",
      "[2025-05-13 10:41:50,043][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:41:50,043][training][INFO] - Epoch 71 complete. Avg Loss: 0.379707\n",
      "[2025-05-13 10:41:50,093][training][INFO] - Epoch 72, Batch 0/3907, Loss: 0.381635\n",
      "[2025-05-13 10:41:50,493][training][INFO] - Epoch 72, Batch 100/3907, Loss: 0.374363\n",
      "[2025-05-13 10:41:50,892][training][INFO] - Epoch 72, Batch 200/3907, Loss: 0.382965\n",
      "[2025-05-13 10:41:51,292][training][INFO] - Epoch 72, Batch 300/3907, Loss: 0.384312\n",
      "[2025-05-13 10:41:51,693][training][INFO] - Epoch 72, Batch 400/3907, Loss: 0.380036\n",
      "[2025-05-13 10:41:52,093][training][INFO] - Epoch 72, Batch 500/3907, Loss: 0.379197\n",
      "[2025-05-13 10:41:52,492][training][INFO] - Epoch 72, Batch 600/3907, Loss: 0.378536\n",
      "[2025-05-13 10:41:52,891][training][INFO] - Epoch 72, Batch 700/3907, Loss: 0.379798\n",
      "[2025-05-13 10:41:53,293][training][INFO] - Epoch 72, Batch 800/3907, Loss: 0.386234\n",
      "[2025-05-13 10:41:53,697][training][INFO] - Epoch 72, Batch 900/3907, Loss: 0.375421\n",
      "[2025-05-13 10:41:54,102][training][INFO] - Epoch 72, Batch 1000/3907, Loss: 0.376832\n",
      "[2025-05-13 10:41:54,507][training][INFO] - Epoch 72, Batch 1100/3907, Loss: 0.376753\n",
      "[2025-05-13 10:41:54,910][training][INFO] - Epoch 72, Batch 1200/3907, Loss: 0.377359\n",
      "[2025-05-13 10:41:55,315][training][INFO] - Epoch 72, Batch 1300/3907, Loss: 0.385911\n",
      "[2025-05-13 10:41:55,717][training][INFO] - Epoch 72, Batch 1400/3907, Loss: 0.382105\n",
      "[2025-05-13 10:41:56,117][training][INFO] - Epoch 72, Batch 1500/3907, Loss: 0.390488\n",
      "[2025-05-13 10:41:56,517][training][INFO] - Epoch 72, Batch 1600/3907, Loss: 0.390404\n",
      "[2025-05-13 10:41:56,917][training][INFO] - Epoch 72, Batch 1700/3907, Loss: 0.376684\n",
      "[2025-05-13 10:41:57,317][training][INFO] - Epoch 72, Batch 1800/3907, Loss: 0.374314\n",
      "[2025-05-13 10:41:57,716][training][INFO] - Epoch 72, Batch 1900/3907, Loss: 0.382780\n",
      "[2025-05-13 10:41:58,115][training][INFO] - Epoch 72, Batch 2000/3907, Loss: 0.373195\n",
      "[2025-05-13 10:41:58,516][training][INFO] - Epoch 72, Batch 2100/3907, Loss: 0.376250\n",
      "[2025-05-13 10:41:58,915][training][INFO] - Epoch 72, Batch 2200/3907, Loss: 0.375729\n",
      "[2025-05-13 10:41:59,315][training][INFO] - Epoch 72, Batch 2300/3907, Loss: 0.375020\n",
      "[2025-05-13 10:41:59,715][training][INFO] - Epoch 72, Batch 2400/3907, Loss: 0.374839\n",
      "[2025-05-13 10:42:00,114][training][INFO] - Epoch 72, Batch 2500/3907, Loss: 0.374903\n",
      "[2025-05-13 10:42:00,513][training][INFO] - Epoch 72, Batch 2600/3907, Loss: 0.381009\n",
      "[2025-05-13 10:42:00,919][training][INFO] - Epoch 72, Batch 2700/3907, Loss: 0.375616\n",
      "[2025-05-13 10:42:01,320][training][INFO] - Epoch 72, Batch 2800/3907, Loss: 0.381964\n",
      "[2025-05-13 10:42:01,722][training][INFO] - Epoch 72, Batch 2900/3907, Loss: 0.367925\n",
      "[2025-05-13 10:42:02,123][training][INFO] - Epoch 72, Batch 3000/3907, Loss: 0.371270\n",
      "[2025-05-13 10:42:02,523][training][INFO] - Epoch 72, Batch 3100/3907, Loss: 0.373107\n",
      "[2025-05-13 10:42:02,925][training][INFO] - Epoch 72, Batch 3200/3907, Loss: 0.383828\n",
      "[2025-05-13 10:42:03,326][training][INFO] - Epoch 72, Batch 3300/3907, Loss: 0.377048\n",
      "[2025-05-13 10:42:03,727][training][INFO] - Epoch 72, Batch 3400/3907, Loss: 0.377986\n",
      "[2025-05-13 10:42:04,129][training][INFO] - Epoch 72, Batch 3500/3907, Loss: 0.376918\n",
      "[2025-05-13 10:42:04,531][training][INFO] - Epoch 72, Batch 3600/3907, Loss: 0.388650\n",
      "[2025-05-13 10:42:04,930][training][INFO] - Epoch 72, Batch 3700/3907, Loss: 0.368837\n",
      "[2025-05-13 10:42:05,330][training][INFO] - Epoch 72, Batch 3800/3907, Loss: 0.379879\n",
      "[2025-05-13 10:42:05,754][training][INFO] - Epoch 72, Batch 3900/3907, Loss: 0.383635\n",
      "[2025-05-13 10:42:05,770][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:42:05,770][training][INFO] - Epoch 72 complete. Avg Loss: 0.379633\n",
      "[2025-05-13 10:42:05,820][training][INFO] - Epoch 73, Batch 0/3907, Loss: 0.386818\n",
      "[2025-05-13 10:42:06,220][training][INFO] - Epoch 73, Batch 100/3907, Loss: 0.385045\n",
      "[2025-05-13 10:42:06,621][training][INFO] - Epoch 73, Batch 200/3907, Loss: 0.380827\n",
      "[2025-05-13 10:42:07,022][training][INFO] - Epoch 73, Batch 300/3907, Loss: 0.379915\n",
      "[2025-05-13 10:42:07,421][training][INFO] - Epoch 73, Batch 400/3907, Loss: 0.379715\n",
      "[2025-05-13 10:42:07,823][training][INFO] - Epoch 73, Batch 500/3907, Loss: 0.383208\n",
      "[2025-05-13 10:42:08,223][training][INFO] - Epoch 73, Batch 600/3907, Loss: 0.380918\n",
      "[2025-05-13 10:42:08,623][training][INFO] - Epoch 73, Batch 700/3907, Loss: 0.378999\n",
      "[2025-05-13 10:42:09,023][training][INFO] - Epoch 73, Batch 800/3907, Loss: 0.381200\n",
      "[2025-05-13 10:42:09,423][training][INFO] - Epoch 73, Batch 900/3907, Loss: 0.383700\n",
      "[2025-05-13 10:42:09,823][training][INFO] - Epoch 73, Batch 1000/3907, Loss: 0.382747\n",
      "[2025-05-13 10:42:10,223][training][INFO] - Epoch 73, Batch 1100/3907, Loss: 0.375140\n",
      "[2025-05-13 10:42:10,627][training][INFO] - Epoch 73, Batch 1200/3907, Loss: 0.380089\n",
      "[2025-05-13 10:42:11,031][training][INFO] - Epoch 73, Batch 1300/3907, Loss: 0.380189\n",
      "[2025-05-13 10:42:11,435][training][INFO] - Epoch 73, Batch 1400/3907, Loss: 0.379100\n",
      "[2025-05-13 10:42:11,841][training][INFO] - Epoch 73, Batch 1500/3907, Loss: 0.379521\n",
      "[2025-05-13 10:42:12,244][training][INFO] - Epoch 73, Batch 1600/3907, Loss: 0.371739\n",
      "[2025-05-13 10:42:12,643][training][INFO] - Epoch 73, Batch 1700/3907, Loss: 0.375922\n",
      "[2025-05-13 10:42:13,042][training][INFO] - Epoch 73, Batch 1800/3907, Loss: 0.383758\n",
      "[2025-05-13 10:42:13,442][training][INFO] - Epoch 73, Batch 1900/3907, Loss: 0.373849\n",
      "[2025-05-13 10:42:13,841][training][INFO] - Epoch 73, Batch 2000/3907, Loss: 0.376914\n",
      "[2025-05-13 10:42:14,240][training][INFO] - Epoch 73, Batch 2100/3907, Loss: 0.371798\n",
      "[2025-05-13 10:42:14,640][training][INFO] - Epoch 73, Batch 2200/3907, Loss: 0.383848\n",
      "[2025-05-13 10:42:15,043][training][INFO] - Epoch 73, Batch 2300/3907, Loss: 0.378116\n",
      "[2025-05-13 10:42:15,446][training][INFO] - Epoch 73, Batch 2400/3907, Loss: 0.370779\n",
      "[2025-05-13 10:42:15,854][training][INFO] - Epoch 73, Batch 2500/3907, Loss: 0.375268\n",
      "[2025-05-13 10:42:16,257][training][INFO] - Epoch 73, Batch 2600/3907, Loss: 0.381856\n",
      "[2025-05-13 10:42:16,659][training][INFO] - Epoch 73, Batch 2700/3907, Loss: 0.374085\n",
      "[2025-05-13 10:42:17,061][training][INFO] - Epoch 73, Batch 2800/3907, Loss: 0.374009\n",
      "[2025-05-13 10:42:17,462][training][INFO] - Epoch 73, Batch 2900/3907, Loss: 0.377832\n",
      "[2025-05-13 10:42:17,862][training][INFO] - Epoch 73, Batch 3000/3907, Loss: 0.387742\n",
      "[2025-05-13 10:42:18,261][training][INFO] - Epoch 73, Batch 3100/3907, Loss: 0.378484\n",
      "[2025-05-13 10:42:18,661][training][INFO] - Epoch 73, Batch 3200/3907, Loss: 0.373059\n",
      "[2025-05-13 10:42:19,059][training][INFO] - Epoch 73, Batch 3300/3907, Loss: 0.381235\n",
      "[2025-05-13 10:42:19,458][training][INFO] - Epoch 73, Batch 3400/3907, Loss: 0.372481\n",
      "[2025-05-13 10:42:19,858][training][INFO] - Epoch 73, Batch 3500/3907, Loss: 0.380930\n",
      "[2025-05-13 10:42:20,260][training][INFO] - Epoch 73, Batch 3600/3907, Loss: 0.380838\n",
      "[2025-05-13 10:42:20,659][training][INFO] - Epoch 73, Batch 3700/3907, Loss: 0.386594\n",
      "[2025-05-13 10:42:21,059][training][INFO] - Epoch 73, Batch 3800/3907, Loss: 0.371525\n",
      "[2025-05-13 10:42:21,477][training][INFO] - Epoch 73, Batch 3900/3907, Loss: 0.385184\n",
      "[2025-05-13 10:42:21,493][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:42:21,494][training][INFO] - Epoch 73 complete. Avg Loss: 0.379528\n",
      "[2025-05-13 10:42:21,543][training][INFO] - Epoch 74, Batch 0/3907, Loss: 0.386386\n",
      "[2025-05-13 10:42:21,942][training][INFO] - Epoch 74, Batch 100/3907, Loss: 0.380871\n",
      "[2025-05-13 10:42:22,341][training][INFO] - Epoch 74, Batch 200/3907, Loss: 0.382715\n",
      "[2025-05-13 10:42:22,741][training][INFO] - Epoch 74, Batch 300/3907, Loss: 0.376055\n",
      "[2025-05-13 10:42:23,141][training][INFO] - Epoch 74, Batch 400/3907, Loss: 0.377084\n",
      "[2025-05-13 10:42:23,541][training][INFO] - Epoch 74, Batch 500/3907, Loss: 0.383795\n",
      "[2025-05-13 10:42:23,941][training][INFO] - Epoch 74, Batch 600/3907, Loss: 0.376908\n",
      "[2025-05-13 10:42:24,341][training][INFO] - Epoch 74, Batch 700/3907, Loss: 0.375499\n",
      "[2025-05-13 10:42:24,740][training][INFO] - Epoch 74, Batch 800/3907, Loss: 0.378974\n",
      "[2025-05-13 10:42:25,140][training][INFO] - Epoch 74, Batch 900/3907, Loss: 0.376424\n",
      "[2025-05-13 10:42:25,540][training][INFO] - Epoch 74, Batch 1000/3907, Loss: 0.382644\n",
      "[2025-05-13 10:42:25,940][training][INFO] - Epoch 74, Batch 1100/3907, Loss: 0.384308\n",
      "[2025-05-13 10:42:26,340][training][INFO] - Epoch 74, Batch 1200/3907, Loss: 0.376763\n",
      "[2025-05-13 10:42:26,741][training][INFO] - Epoch 74, Batch 1300/3907, Loss: 0.377422\n",
      "[2025-05-13 10:42:27,141][training][INFO] - Epoch 74, Batch 1400/3907, Loss: 0.379112\n",
      "[2025-05-13 10:42:27,540][training][INFO] - Epoch 74, Batch 1500/3907, Loss: 0.374143\n",
      "[2025-05-13 10:42:27,941][training][INFO] - Epoch 74, Batch 1600/3907, Loss: 0.377862\n",
      "[2025-05-13 10:42:28,341][training][INFO] - Epoch 74, Batch 1700/3907, Loss: 0.391861\n",
      "[2025-05-13 10:42:28,741][training][INFO] - Epoch 74, Batch 1800/3907, Loss: 0.371494\n",
      "[2025-05-13 10:42:29,140][training][INFO] - Epoch 74, Batch 1900/3907, Loss: 0.379981\n",
      "[2025-05-13 10:42:29,542][training][INFO] - Epoch 74, Batch 2000/3907, Loss: 0.385509\n",
      "[2025-05-13 10:42:29,945][training][INFO] - Epoch 74, Batch 2100/3907, Loss: 0.383392\n",
      "[2025-05-13 10:42:30,348][training][INFO] - Epoch 74, Batch 2200/3907, Loss: 0.379113\n",
      "[2025-05-13 10:42:30,753][training][INFO] - Epoch 74, Batch 2300/3907, Loss: 0.369129\n",
      "[2025-05-13 10:42:31,155][training][INFO] - Epoch 74, Batch 2400/3907, Loss: 0.380602\n",
      "[2025-05-13 10:42:31,555][training][INFO] - Epoch 74, Batch 2500/3907, Loss: 0.382862\n",
      "[2025-05-13 10:42:31,959][training][INFO] - Epoch 74, Batch 2600/3907, Loss: 0.376128\n",
      "[2025-05-13 10:42:32,362][training][INFO] - Epoch 74, Batch 2700/3907, Loss: 0.376492\n",
      "[2025-05-13 10:42:32,763][training][INFO] - Epoch 74, Batch 2800/3907, Loss: 0.379436\n",
      "[2025-05-13 10:42:33,163][training][INFO] - Epoch 74, Batch 2900/3907, Loss: 0.376171\n",
      "[2025-05-13 10:42:33,563][training][INFO] - Epoch 74, Batch 3000/3907, Loss: 0.368938\n",
      "[2025-05-13 10:42:33,963][training][INFO] - Epoch 74, Batch 3100/3907, Loss: 0.378602\n",
      "[2025-05-13 10:42:34,363][training][INFO] - Epoch 74, Batch 3200/3907, Loss: 0.380771\n",
      "[2025-05-13 10:42:34,765][training][INFO] - Epoch 74, Batch 3300/3907, Loss: 0.384228\n",
      "[2025-05-13 10:42:35,165][training][INFO] - Epoch 74, Batch 3400/3907, Loss: 0.366723\n",
      "[2025-05-13 10:42:35,565][training][INFO] - Epoch 74, Batch 3500/3907, Loss: 0.384894\n",
      "[2025-05-13 10:42:35,964][training][INFO] - Epoch 74, Batch 3600/3907, Loss: 0.380516\n",
      "[2025-05-13 10:42:36,363][training][INFO] - Epoch 74, Batch 3700/3907, Loss: 0.390238\n",
      "[2025-05-13 10:42:36,766][training][INFO] - Epoch 74, Batch 3800/3907, Loss: 0.383344\n",
      "[2025-05-13 10:42:37,190][training][INFO] - Epoch 74, Batch 3900/3907, Loss: 0.390661\n",
      "[2025-05-13 10:42:37,207][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:42:37,207][training][INFO] - Epoch 74 complete. Avg Loss: 0.379276\n",
      "[2025-05-13 10:42:37,260][training][INFO] - Epoch 75, Batch 0/3907, Loss: 0.376432\n",
      "[2025-05-13 10:42:37,662][training][INFO] - Epoch 75, Batch 100/3907, Loss: 0.381170\n",
      "[2025-05-13 10:42:38,066][training][INFO] - Epoch 75, Batch 200/3907, Loss: 0.375082\n",
      "[2025-05-13 10:42:38,469][training][INFO] - Epoch 75, Batch 300/3907, Loss: 0.375838\n",
      "[2025-05-13 10:42:38,869][training][INFO] - Epoch 75, Batch 400/3907, Loss: 0.374527\n",
      "[2025-05-13 10:42:39,270][training][INFO] - Epoch 75, Batch 500/3907, Loss: 0.384552\n",
      "[2025-05-13 10:42:39,671][training][INFO] - Epoch 75, Batch 600/3907, Loss: 0.380269\n",
      "[2025-05-13 10:42:40,071][training][INFO] - Epoch 75, Batch 700/3907, Loss: 0.380505\n",
      "[2025-05-13 10:42:40,470][training][INFO] - Epoch 75, Batch 800/3907, Loss: 0.387170\n",
      "[2025-05-13 10:42:40,869][training][INFO] - Epoch 75, Batch 900/3907, Loss: 0.391148\n",
      "[2025-05-13 10:42:41,269][training][INFO] - Epoch 75, Batch 1000/3907, Loss: 0.381281\n",
      "[2025-05-13 10:42:41,669][training][INFO] - Epoch 75, Batch 1100/3907, Loss: 0.372661\n",
      "[2025-05-13 10:42:42,069][training][INFO] - Epoch 75, Batch 1200/3907, Loss: 0.374475\n",
      "[2025-05-13 10:42:42,468][training][INFO] - Epoch 75, Batch 1300/3907, Loss: 0.382306\n",
      "[2025-05-13 10:42:42,868][training][INFO] - Epoch 75, Batch 1400/3907, Loss: 0.387702\n",
      "[2025-05-13 10:42:43,269][training][INFO] - Epoch 75, Batch 1500/3907, Loss: 0.385067\n",
      "[2025-05-13 10:42:43,669][training][INFO] - Epoch 75, Batch 1600/3907, Loss: 0.375667\n",
      "[2025-05-13 10:42:44,069][training][INFO] - Epoch 75, Batch 1700/3907, Loss: 0.376865\n",
      "[2025-05-13 10:42:44,469][training][INFO] - Epoch 75, Batch 1800/3907, Loss: 0.371568\n",
      "[2025-05-13 10:42:44,871][training][INFO] - Epoch 75, Batch 1900/3907, Loss: 0.377001\n",
      "[2025-05-13 10:42:45,271][training][INFO] - Epoch 75, Batch 2000/3907, Loss: 0.372600\n",
      "[2025-05-13 10:42:45,675][training][INFO] - Epoch 75, Batch 2100/3907, Loss: 0.380900\n",
      "[2025-05-13 10:42:46,076][training][INFO] - Epoch 75, Batch 2200/3907, Loss: 0.378859\n",
      "[2025-05-13 10:42:46,475][training][INFO] - Epoch 75, Batch 2300/3907, Loss: 0.379561\n",
      "[2025-05-13 10:42:46,878][training][INFO] - Epoch 75, Batch 2400/3907, Loss: 0.376131\n",
      "[2025-05-13 10:42:47,278][training][INFO] - Epoch 75, Batch 2500/3907, Loss: 0.383735\n",
      "[2025-05-13 10:42:47,678][training][INFO] - Epoch 75, Batch 2600/3907, Loss: 0.375618\n",
      "[2025-05-13 10:42:48,077][training][INFO] - Epoch 75, Batch 2700/3907, Loss: 0.374420\n",
      "[2025-05-13 10:42:48,478][training][INFO] - Epoch 75, Batch 2800/3907, Loss: 0.379040\n",
      "[2025-05-13 10:42:48,880][training][INFO] - Epoch 75, Batch 2900/3907, Loss: 0.382215\n",
      "[2025-05-13 10:42:49,279][training][INFO] - Epoch 75, Batch 3000/3907, Loss: 0.373175\n",
      "[2025-05-13 10:42:49,679][training][INFO] - Epoch 75, Batch 3100/3907, Loss: 0.387627\n",
      "[2025-05-13 10:42:50,078][training][INFO] - Epoch 75, Batch 3200/3907, Loss: 0.377817\n",
      "[2025-05-13 10:42:50,478][training][INFO] - Epoch 75, Batch 3300/3907, Loss: 0.366399\n",
      "[2025-05-13 10:42:50,876][training][INFO] - Epoch 75, Batch 3400/3907, Loss: 0.385143\n",
      "[2025-05-13 10:42:51,274][training][INFO] - Epoch 75, Batch 3500/3907, Loss: 0.390789\n",
      "[2025-05-13 10:42:51,672][training][INFO] - Epoch 75, Batch 3600/3907, Loss: 0.375051\n",
      "[2025-05-13 10:42:52,071][training][INFO] - Epoch 75, Batch 3700/3907, Loss: 0.372866\n",
      "[2025-05-13 10:42:52,472][training][INFO] - Epoch 75, Batch 3800/3907, Loss: 0.378063\n",
      "[2025-05-13 10:42:52,891][training][INFO] - Epoch 75, Batch 3900/3907, Loss: 0.380099\n",
      "[2025-05-13 10:42:52,908][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:42:52,908][training][INFO] - Epoch 75 complete. Avg Loss: 0.379047\n",
      "[2025-05-13 10:42:52,953][training][INFO] - Epoch 76, Batch 0/3907, Loss: 0.379349\n",
      "[2025-05-13 10:42:53,356][training][INFO] - Epoch 76, Batch 100/3907, Loss: 0.371713\n",
      "[2025-05-13 10:42:53,756][training][INFO] - Epoch 76, Batch 200/3907, Loss: 0.379030\n",
      "[2025-05-13 10:42:54,158][training][INFO] - Epoch 76, Batch 300/3907, Loss: 0.380523\n",
      "[2025-05-13 10:42:54,563][training][INFO] - Epoch 76, Batch 400/3907, Loss: 0.377010\n",
      "[2025-05-13 10:42:54,964][training][INFO] - Epoch 76, Batch 500/3907, Loss: 0.376684\n",
      "[2025-05-13 10:42:55,366][training][INFO] - Epoch 76, Batch 600/3907, Loss: 0.377121\n",
      "[2025-05-13 10:42:55,767][training][INFO] - Epoch 76, Batch 700/3907, Loss: 0.378333\n",
      "[2025-05-13 10:42:56,170][training][INFO] - Epoch 76, Batch 800/3907, Loss: 0.372690\n",
      "[2025-05-13 10:42:56,576][training][INFO] - Epoch 76, Batch 900/3907, Loss: 0.381327\n",
      "[2025-05-13 10:42:56,981][training][INFO] - Epoch 76, Batch 1000/3907, Loss: 0.374341\n",
      "[2025-05-13 10:42:57,385][training][INFO] - Epoch 76, Batch 1100/3907, Loss: 0.371600\n",
      "[2025-05-13 10:42:57,786][training][INFO] - Epoch 76, Batch 1200/3907, Loss: 0.375591\n",
      "[2025-05-13 10:42:58,189][training][INFO] - Epoch 76, Batch 1300/3907, Loss: 0.380738\n",
      "[2025-05-13 10:42:58,591][training][INFO] - Epoch 76, Batch 1400/3907, Loss: 0.384641\n",
      "[2025-05-13 10:42:58,991][training][INFO] - Epoch 76, Batch 1500/3907, Loss: 0.371920\n",
      "[2025-05-13 10:42:59,391][training][INFO] - Epoch 76, Batch 1600/3907, Loss: 0.380250\n",
      "[2025-05-13 10:42:59,791][training][INFO] - Epoch 76, Batch 1700/3907, Loss: 0.378288\n",
      "[2025-05-13 10:43:00,192][training][INFO] - Epoch 76, Batch 1800/3907, Loss: 0.376011\n",
      "[2025-05-13 10:43:00,594][training][INFO] - Epoch 76, Batch 1900/3907, Loss: 0.380464\n",
      "[2025-05-13 10:43:00,999][training][INFO] - Epoch 76, Batch 2000/3907, Loss: 0.378274\n",
      "[2025-05-13 10:43:01,401][training][INFO] - Epoch 76, Batch 2100/3907, Loss: 0.375464\n",
      "[2025-05-13 10:43:01,801][training][INFO] - Epoch 76, Batch 2200/3907, Loss: 0.374892\n",
      "[2025-05-13 10:43:02,204][training][INFO] - Epoch 76, Batch 2300/3907, Loss: 0.389053\n",
      "[2025-05-13 10:43:02,607][training][INFO] - Epoch 76, Batch 2400/3907, Loss: 0.392613\n",
      "[2025-05-13 10:43:03,008][training][INFO] - Epoch 76, Batch 2500/3907, Loss: 0.379298\n",
      "[2025-05-13 10:43:03,410][training][INFO] - Epoch 76, Batch 2600/3907, Loss: 0.375789\n",
      "[2025-05-13 10:43:03,810][training][INFO] - Epoch 76, Batch 2700/3907, Loss: 0.366597\n",
      "[2025-05-13 10:43:04,210][training][INFO] - Epoch 76, Batch 2800/3907, Loss: 0.374116\n",
      "[2025-05-13 10:43:04,610][training][INFO] - Epoch 76, Batch 2900/3907, Loss: 0.373529\n",
      "[2025-05-13 10:43:05,013][training][INFO] - Epoch 76, Batch 3000/3907, Loss: 0.375775\n",
      "[2025-05-13 10:43:05,415][training][INFO] - Epoch 76, Batch 3100/3907, Loss: 0.380321\n",
      "[2025-05-13 10:43:05,818][training][INFO] - Epoch 76, Batch 3200/3907, Loss: 0.370181\n",
      "[2025-05-13 10:43:06,219][training][INFO] - Epoch 76, Batch 3300/3907, Loss: 0.383319\n",
      "[2025-05-13 10:43:06,623][training][INFO] - Epoch 76, Batch 3400/3907, Loss: 0.373813\n",
      "[2025-05-13 10:43:07,025][training][INFO] - Epoch 76, Batch 3500/3907, Loss: 0.381153\n",
      "[2025-05-13 10:43:07,427][training][INFO] - Epoch 76, Batch 3600/3907, Loss: 0.374147\n",
      "[2025-05-13 10:43:07,827][training][INFO] - Epoch 76, Batch 3700/3907, Loss: 0.384633\n",
      "[2025-05-13 10:43:08,226][training][INFO] - Epoch 76, Batch 3800/3907, Loss: 0.379117\n",
      "[2025-05-13 10:43:08,646][training][INFO] - Epoch 76, Batch 3900/3907, Loss: 0.374491\n",
      "[2025-05-13 10:43:08,664][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:43:08,664][training][INFO] - Epoch 76 complete. Avg Loss: 0.379049\n",
      "[2025-05-13 10:43:08,714][training][INFO] - Epoch 77, Batch 0/3907, Loss: 0.381722\n",
      "[2025-05-13 10:43:09,116][training][INFO] - Epoch 77, Batch 100/3907, Loss: 0.375081\n",
      "[2025-05-13 10:43:09,516][training][INFO] - Epoch 77, Batch 200/3907, Loss: 0.376149\n",
      "[2025-05-13 10:43:09,917][training][INFO] - Epoch 77, Batch 300/3907, Loss: 0.378775\n",
      "[2025-05-13 10:43:10,317][training][INFO] - Epoch 77, Batch 400/3907, Loss: 0.383684\n",
      "[2025-05-13 10:43:10,716][training][INFO] - Epoch 77, Batch 500/3907, Loss: 0.373464\n",
      "[2025-05-13 10:43:11,116][training][INFO] - Epoch 77, Batch 600/3907, Loss: 0.382508\n",
      "[2025-05-13 10:43:11,516][training][INFO] - Epoch 77, Batch 700/3907, Loss: 0.375188\n",
      "[2025-05-13 10:43:11,916][training][INFO] - Epoch 77, Batch 800/3907, Loss: 0.384845\n",
      "[2025-05-13 10:43:12,316][training][INFO] - Epoch 77, Batch 900/3907, Loss: 0.379115\n",
      "[2025-05-13 10:43:12,717][training][INFO] - Epoch 77, Batch 1000/3907, Loss: 0.370273\n",
      "[2025-05-13 10:43:13,119][training][INFO] - Epoch 77, Batch 1100/3907, Loss: 0.379372\n",
      "[2025-05-13 10:43:13,520][training][INFO] - Epoch 77, Batch 1200/3907, Loss: 0.376585\n",
      "[2025-05-13 10:43:13,920][training][INFO] - Epoch 77, Batch 1300/3907, Loss: 0.383302\n",
      "[2025-05-13 10:43:14,319][training][INFO] - Epoch 77, Batch 1400/3907, Loss: 0.370568\n",
      "[2025-05-13 10:43:14,723][training][INFO] - Epoch 77, Batch 1500/3907, Loss: 0.386661\n",
      "[2025-05-13 10:43:15,126][training][INFO] - Epoch 77, Batch 1600/3907, Loss: 0.381406\n",
      "[2025-05-13 10:43:15,534][training][INFO] - Epoch 77, Batch 1700/3907, Loss: 0.380188\n",
      "[2025-05-13 10:43:15,947][training][INFO] - Epoch 77, Batch 1800/3907, Loss: 0.382721\n",
      "[2025-05-13 10:43:16,347][training][INFO] - Epoch 77, Batch 1900/3907, Loss: 0.375471\n",
      "[2025-05-13 10:43:16,748][training][INFO] - Epoch 77, Batch 2000/3907, Loss: 0.376432\n",
      "[2025-05-13 10:43:17,149][training][INFO] - Epoch 77, Batch 2100/3907, Loss: 0.376066\n",
      "[2025-05-13 10:43:17,549][training][INFO] - Epoch 77, Batch 2200/3907, Loss: 0.374834\n",
      "[2025-05-13 10:43:17,951][training][INFO] - Epoch 77, Batch 2300/3907, Loss: 0.379690\n",
      "[2025-05-13 10:43:18,350][training][INFO] - Epoch 77, Batch 2400/3907, Loss: 0.379167\n",
      "[2025-05-13 10:43:18,749][training][INFO] - Epoch 77, Batch 2500/3907, Loss: 0.379770\n",
      "[2025-05-13 10:43:19,149][training][INFO] - Epoch 77, Batch 2600/3907, Loss: 0.378706\n",
      "[2025-05-13 10:43:19,552][training][INFO] - Epoch 77, Batch 2700/3907, Loss: 0.384947\n",
      "[2025-05-13 10:43:19,957][training][INFO] - Epoch 77, Batch 2800/3907, Loss: 0.378369\n",
      "[2025-05-13 10:43:20,359][training][INFO] - Epoch 77, Batch 2900/3907, Loss: 0.390851\n",
      "[2025-05-13 10:43:20,760][training][INFO] - Epoch 77, Batch 3000/3907, Loss: 0.373683\n",
      "[2025-05-13 10:43:21,160][training][INFO] - Epoch 77, Batch 3100/3907, Loss: 0.370823\n",
      "[2025-05-13 10:43:21,561][training][INFO] - Epoch 77, Batch 3200/3907, Loss: 0.383149\n",
      "[2025-05-13 10:43:21,962][training][INFO] - Epoch 77, Batch 3300/3907, Loss: 0.382078\n",
      "[2025-05-13 10:43:22,364][training][INFO] - Epoch 77, Batch 3400/3907, Loss: 0.368949\n",
      "[2025-05-13 10:43:22,766][training][INFO] - Epoch 77, Batch 3500/3907, Loss: 0.378127\n",
      "[2025-05-13 10:43:23,166][training][INFO] - Epoch 77, Batch 3600/3907, Loss: 0.379886\n",
      "[2025-05-13 10:43:23,567][training][INFO] - Epoch 77, Batch 3700/3907, Loss: 0.384440\n",
      "[2025-05-13 10:43:23,970][training][INFO] - Epoch 77, Batch 3800/3907, Loss: 0.382229\n",
      "[2025-05-13 10:43:24,393][training][INFO] - Epoch 77, Batch 3900/3907, Loss: 0.375603\n",
      "[2025-05-13 10:43:24,410][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:43:24,410][training][INFO] - Epoch 77 complete. Avg Loss: 0.378894\n",
      "[2025-05-13 10:43:24,460][training][INFO] - Epoch 78, Batch 0/3907, Loss: 0.373122\n",
      "[2025-05-13 10:43:24,863][training][INFO] - Epoch 78, Batch 100/3907, Loss: 0.382171\n",
      "[2025-05-13 10:43:25,264][training][INFO] - Epoch 78, Batch 200/3907, Loss: 0.379107\n",
      "[2025-05-13 10:43:25,665][training][INFO] - Epoch 78, Batch 300/3907, Loss: 0.372943\n",
      "[2025-05-13 10:43:26,069][training][INFO] - Epoch 78, Batch 400/3907, Loss: 0.378332\n",
      "[2025-05-13 10:43:26,470][training][INFO] - Epoch 78, Batch 500/3907, Loss: 0.385294\n",
      "[2025-05-13 10:43:26,871][training][INFO] - Epoch 78, Batch 600/3907, Loss: 0.387281\n",
      "[2025-05-13 10:43:27,271][training][INFO] - Epoch 78, Batch 700/3907, Loss: 0.376044\n",
      "[2025-05-13 10:43:27,672][training][INFO] - Epoch 78, Batch 800/3907, Loss: 0.374752\n",
      "[2025-05-13 10:43:28,073][training][INFO] - Epoch 78, Batch 900/3907, Loss: 0.372253\n",
      "[2025-05-13 10:43:28,473][training][INFO] - Epoch 78, Batch 1000/3907, Loss: 0.372916\n",
      "[2025-05-13 10:43:28,874][training][INFO] - Epoch 78, Batch 1100/3907, Loss: 0.383140\n",
      "[2025-05-13 10:43:29,274][training][INFO] - Epoch 78, Batch 1200/3907, Loss: 0.376477\n",
      "[2025-05-13 10:43:29,674][training][INFO] - Epoch 78, Batch 1300/3907, Loss: 0.378128\n",
      "[2025-05-13 10:43:30,075][training][INFO] - Epoch 78, Batch 1400/3907, Loss: 0.388490\n",
      "[2025-05-13 10:43:30,477][training][INFO] - Epoch 78, Batch 1500/3907, Loss: 0.374419\n",
      "[2025-05-13 10:43:30,880][training][INFO] - Epoch 78, Batch 1600/3907, Loss: 0.380567\n",
      "[2025-05-13 10:43:31,286][training][INFO] - Epoch 78, Batch 1700/3907, Loss: 0.381966\n",
      "[2025-05-13 10:43:31,685][training][INFO] - Epoch 78, Batch 1800/3907, Loss: 0.380597\n",
      "[2025-05-13 10:43:32,085][training][INFO] - Epoch 78, Batch 1900/3907, Loss: 0.376020\n",
      "[2025-05-13 10:43:32,486][training][INFO] - Epoch 78, Batch 2000/3907, Loss: 0.378284\n",
      "[2025-05-13 10:43:32,886][training][INFO] - Epoch 78, Batch 2100/3907, Loss: 0.385425\n",
      "[2025-05-13 10:43:33,287][training][INFO] - Epoch 78, Batch 2200/3907, Loss: 0.374868\n",
      "[2025-05-13 10:43:33,687][training][INFO] - Epoch 78, Batch 2300/3907, Loss: 0.384401\n",
      "[2025-05-13 10:43:34,090][training][INFO] - Epoch 78, Batch 2400/3907, Loss: 0.381107\n",
      "[2025-05-13 10:43:34,492][training][INFO] - Epoch 78, Batch 2500/3907, Loss: 0.380656\n",
      "[2025-05-13 10:43:34,892][training][INFO] - Epoch 78, Batch 2600/3907, Loss: 0.385696\n",
      "[2025-05-13 10:43:35,291][training][INFO] - Epoch 78, Batch 2700/3907, Loss: 0.372624\n",
      "[2025-05-13 10:43:35,691][training][INFO] - Epoch 78, Batch 2800/3907, Loss: 0.370026\n",
      "[2025-05-13 10:43:36,090][training][INFO] - Epoch 78, Batch 2900/3907, Loss: 0.384571\n",
      "[2025-05-13 10:43:36,490][training][INFO] - Epoch 78, Batch 3000/3907, Loss: 0.371391\n",
      "[2025-05-13 10:43:36,892][training][INFO] - Epoch 78, Batch 3100/3907, Loss: 0.379464\n",
      "[2025-05-13 10:43:37,296][training][INFO] - Epoch 78, Batch 3200/3907, Loss: 0.374630\n",
      "[2025-05-13 10:43:37,696][training][INFO] - Epoch 78, Batch 3300/3907, Loss: 0.390414\n",
      "[2025-05-13 10:43:38,097][training][INFO] - Epoch 78, Batch 3400/3907, Loss: 0.380661\n",
      "[2025-05-13 10:43:38,497][training][INFO] - Epoch 78, Batch 3500/3907, Loss: 0.381814\n",
      "[2025-05-13 10:43:38,896][training][INFO] - Epoch 78, Batch 3600/3907, Loss: 0.375352\n",
      "[2025-05-13 10:43:39,295][training][INFO] - Epoch 78, Batch 3700/3907, Loss: 0.384111\n",
      "[2025-05-13 10:43:39,695][training][INFO] - Epoch 78, Batch 3800/3907, Loss: 0.382839\n",
      "[2025-05-13 10:43:40,113][training][INFO] - Epoch 78, Batch 3900/3907, Loss: 0.376023\n",
      "[2025-05-13 10:43:40,130][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:43:40,130][training][INFO] - Epoch 78 complete. Avg Loss: 0.378844\n",
      "[2025-05-13 10:43:40,177][training][INFO] - Epoch 79, Batch 0/3907, Loss: 0.379227\n",
      "[2025-05-13 10:43:40,580][training][INFO] - Epoch 79, Batch 100/3907, Loss: 0.387450\n",
      "[2025-05-13 10:43:40,980][training][INFO] - Epoch 79, Batch 200/3907, Loss: 0.382165\n",
      "[2025-05-13 10:43:41,380][training][INFO] - Epoch 79, Batch 300/3907, Loss: 0.384044\n",
      "[2025-05-13 10:43:41,782][training][INFO] - Epoch 79, Batch 400/3907, Loss: 0.373962\n",
      "[2025-05-13 10:43:42,182][training][INFO] - Epoch 79, Batch 500/3907, Loss: 0.389399\n",
      "[2025-05-13 10:43:42,582][training][INFO] - Epoch 79, Batch 600/3907, Loss: 0.385207\n",
      "[2025-05-13 10:43:42,982][training][INFO] - Epoch 79, Batch 700/3907, Loss: 0.378575\n",
      "[2025-05-13 10:43:43,385][training][INFO] - Epoch 79, Batch 800/3907, Loss: 0.373880\n",
      "[2025-05-13 10:43:43,788][training][INFO] - Epoch 79, Batch 900/3907, Loss: 0.376933\n",
      "[2025-05-13 10:43:44,191][training][INFO] - Epoch 79, Batch 1000/3907, Loss: 0.365779\n",
      "[2025-05-13 10:43:44,595][training][INFO] - Epoch 79, Batch 1100/3907, Loss: 0.385274\n",
      "[2025-05-13 10:43:45,000][training][INFO] - Epoch 79, Batch 1200/3907, Loss: 0.375809\n",
      "[2025-05-13 10:43:45,400][training][INFO] - Epoch 79, Batch 1300/3907, Loss: 0.375116\n",
      "[2025-05-13 10:43:45,803][training][INFO] - Epoch 79, Batch 1400/3907, Loss: 0.380549\n",
      "[2025-05-13 10:43:46,205][training][INFO] - Epoch 79, Batch 1500/3907, Loss: 0.387925\n",
      "[2025-05-13 10:43:46,610][training][INFO] - Epoch 79, Batch 1600/3907, Loss: 0.380026\n",
      "[2025-05-13 10:43:47,011][training][INFO] - Epoch 79, Batch 1700/3907, Loss: 0.366361\n",
      "[2025-05-13 10:43:47,412][training][INFO] - Epoch 79, Batch 1800/3907, Loss: 0.382094\n",
      "[2025-05-13 10:43:47,813][training][INFO] - Epoch 79, Batch 1900/3907, Loss: 0.380553\n",
      "[2025-05-13 10:43:48,213][training][INFO] - Epoch 79, Batch 2000/3907, Loss: 0.378926\n",
      "[2025-05-13 10:43:48,613][training][INFO] - Epoch 79, Batch 2100/3907, Loss: 0.370655\n",
      "[2025-05-13 10:43:49,014][training][INFO] - Epoch 79, Batch 2200/3907, Loss: 0.377248\n",
      "[2025-05-13 10:43:49,415][training][INFO] - Epoch 79, Batch 2300/3907, Loss: 0.370961\n",
      "[2025-05-13 10:43:49,817][training][INFO] - Epoch 79, Batch 2400/3907, Loss: 0.376564\n",
      "[2025-05-13 10:43:50,220][training][INFO] - Epoch 79, Batch 2500/3907, Loss: 0.380672\n",
      "[2025-05-13 10:43:50,619][training][INFO] - Epoch 79, Batch 2600/3907, Loss: 0.374301\n",
      "[2025-05-13 10:43:51,020][training][INFO] - Epoch 79, Batch 2700/3907, Loss: 0.379162\n",
      "[2025-05-13 10:43:51,420][training][INFO] - Epoch 79, Batch 2800/3907, Loss: 0.377386\n",
      "[2025-05-13 10:43:51,823][training][INFO] - Epoch 79, Batch 2900/3907, Loss: 0.370800\n",
      "[2025-05-13 10:43:52,229][training][INFO] - Epoch 79, Batch 3000/3907, Loss: 0.370172\n",
      "[2025-05-13 10:43:52,633][training][INFO] - Epoch 79, Batch 3100/3907, Loss: 0.373996\n",
      "[2025-05-13 10:43:53,038][training][INFO] - Epoch 79, Batch 3200/3907, Loss: 0.381809\n",
      "[2025-05-13 10:43:53,441][training][INFO] - Epoch 79, Batch 3300/3907, Loss: 0.377785\n",
      "[2025-05-13 10:43:53,845][training][INFO] - Epoch 79, Batch 3400/3907, Loss: 0.382050\n",
      "[2025-05-13 10:43:54,249][training][INFO] - Epoch 79, Batch 3500/3907, Loss: 0.367763\n",
      "[2025-05-13 10:43:54,653][training][INFO] - Epoch 79, Batch 3600/3907, Loss: 0.384797\n",
      "[2025-05-13 10:43:55,056][training][INFO] - Epoch 79, Batch 3700/3907, Loss: 0.378661\n",
      "[2025-05-13 10:43:55,458][training][INFO] - Epoch 79, Batch 3800/3907, Loss: 0.373762\n",
      "[2025-05-13 10:43:55,880][training][INFO] - Epoch 79, Batch 3900/3907, Loss: 0.370301\n",
      "[2025-05-13 10:43:55,897][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:43:55,897][training][INFO] - Epoch 79 complete. Avg Loss: 0.378654\n",
      "[2025-05-13 10:43:55,948][training][INFO] - Epoch 80, Batch 0/3907, Loss: 0.380654\n",
      "[2025-05-13 10:43:56,350][training][INFO] - Epoch 80, Batch 100/3907, Loss: 0.381620\n",
      "[2025-05-13 10:43:56,756][training][INFO] - Epoch 80, Batch 200/3907, Loss: 0.380531\n",
      "[2025-05-13 10:43:57,155][training][INFO] - Epoch 80, Batch 300/3907, Loss: 0.379040\n",
      "[2025-05-13 10:43:57,554][training][INFO] - Epoch 80, Batch 400/3907, Loss: 0.375512\n",
      "[2025-05-13 10:43:57,954][training][INFO] - Epoch 80, Batch 500/3907, Loss: 0.370717\n",
      "[2025-05-13 10:43:58,357][training][INFO] - Epoch 80, Batch 600/3907, Loss: 0.377779\n",
      "[2025-05-13 10:43:58,762][training][INFO] - Epoch 80, Batch 700/3907, Loss: 0.382556\n",
      "[2025-05-13 10:43:59,166][training][INFO] - Epoch 80, Batch 800/3907, Loss: 0.372583\n",
      "[2025-05-13 10:43:59,566][training][INFO] - Epoch 80, Batch 900/3907, Loss: 0.384048\n",
      "[2025-05-13 10:43:59,965][training][INFO] - Epoch 80, Batch 1000/3907, Loss: 0.368793\n",
      "[2025-05-13 10:44:00,365][training][INFO] - Epoch 80, Batch 1100/3907, Loss: 0.370256\n",
      "[2025-05-13 10:44:00,767][training][INFO] - Epoch 80, Batch 1200/3907, Loss: 0.381917\n",
      "[2025-05-13 10:44:01,171][training][INFO] - Epoch 80, Batch 1300/3907, Loss: 0.374679\n",
      "[2025-05-13 10:44:01,569][training][INFO] - Epoch 80, Batch 1400/3907, Loss: 0.390500\n",
      "[2025-05-13 10:44:01,970][training][INFO] - Epoch 80, Batch 1500/3907, Loss: 0.378341\n",
      "[2025-05-13 10:44:02,372][training][INFO] - Epoch 80, Batch 1600/3907, Loss: 0.385348\n",
      "[2025-05-13 10:44:02,773][training][INFO] - Epoch 80, Batch 1700/3907, Loss: 0.380897\n",
      "[2025-05-13 10:44:03,175][training][INFO] - Epoch 80, Batch 1800/3907, Loss: 0.379824\n",
      "[2025-05-13 10:44:03,575][training][INFO] - Epoch 80, Batch 1900/3907, Loss: 0.374638\n",
      "[2025-05-13 10:44:03,974][training][INFO] - Epoch 80, Batch 2000/3907, Loss: 0.381006\n",
      "[2025-05-13 10:44:04,373][training][INFO] - Epoch 80, Batch 2100/3907, Loss: 0.374730\n",
      "[2025-05-13 10:44:04,772][training][INFO] - Epoch 80, Batch 2200/3907, Loss: 0.381562\n",
      "[2025-05-13 10:44:05,172][training][INFO] - Epoch 80, Batch 2300/3907, Loss: 0.373684\n",
      "[2025-05-13 10:44:05,571][training][INFO] - Epoch 80, Batch 2400/3907, Loss: 0.378069\n",
      "[2025-05-13 10:44:05,973][training][INFO] - Epoch 80, Batch 2500/3907, Loss: 0.383812\n",
      "[2025-05-13 10:44:06,373][training][INFO] - Epoch 80, Batch 2600/3907, Loss: 0.368324\n",
      "[2025-05-13 10:44:06,775][training][INFO] - Epoch 80, Batch 2700/3907, Loss: 0.373868\n",
      "[2025-05-13 10:44:07,176][training][INFO] - Epoch 80, Batch 2800/3907, Loss: 0.375353\n",
      "[2025-05-13 10:44:07,576][training][INFO] - Epoch 80, Batch 2900/3907, Loss: 0.376327\n",
      "[2025-05-13 10:44:07,976][training][INFO] - Epoch 80, Batch 3000/3907, Loss: 0.385735\n",
      "[2025-05-13 10:44:08,376][training][INFO] - Epoch 80, Batch 3100/3907, Loss: 0.380010\n",
      "[2025-05-13 10:44:08,778][training][INFO] - Epoch 80, Batch 3200/3907, Loss: 0.379090\n",
      "[2025-05-13 10:44:09,180][training][INFO] - Epoch 80, Batch 3300/3907, Loss: 0.382190\n",
      "[2025-05-13 10:44:09,579][training][INFO] - Epoch 80, Batch 3400/3907, Loss: 0.377649\n",
      "[2025-05-13 10:44:09,978][training][INFO] - Epoch 80, Batch 3500/3907, Loss: 0.380934\n",
      "[2025-05-13 10:44:10,379][training][INFO] - Epoch 80, Batch 3600/3907, Loss: 0.373993\n",
      "[2025-05-13 10:44:10,780][training][INFO] - Epoch 80, Batch 3700/3907, Loss: 0.377397\n",
      "[2025-05-13 10:44:11,180][training][INFO] - Epoch 80, Batch 3800/3907, Loss: 0.380495\n",
      "[2025-05-13 10:44:11,600][training][INFO] - Epoch 80, Batch 3900/3907, Loss: 0.380529\n",
      "[2025-05-13 10:44:11,617][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:44:11,617][training][INFO] - Epoch 80 complete. Avg Loss: 0.378556\n",
      "[2025-05-13 10:44:11,624][training][INFO] - Saved checkpoint to /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7/checkpoint_epoch_80.pt\n",
      "[2025-05-13 10:44:11,675][training][INFO] - Epoch 81, Batch 0/3907, Loss: 0.383796\n",
      "[2025-05-13 10:44:12,076][training][INFO] - Epoch 81, Batch 100/3907, Loss: 0.384532\n",
      "[2025-05-13 10:44:12,476][training][INFO] - Epoch 81, Batch 200/3907, Loss: 0.383831\n",
      "[2025-05-13 10:44:12,877][training][INFO] - Epoch 81, Batch 300/3907, Loss: 0.381136\n",
      "[2025-05-13 10:44:13,276][training][INFO] - Epoch 81, Batch 400/3907, Loss: 0.374520\n",
      "[2025-05-13 10:44:13,674][training][INFO] - Epoch 81, Batch 500/3907, Loss: 0.377500\n",
      "[2025-05-13 10:44:14,075][training][INFO] - Epoch 81, Batch 600/3907, Loss: 0.381135\n",
      "[2025-05-13 10:44:14,478][training][INFO] - Epoch 81, Batch 700/3907, Loss: 0.375570\n",
      "[2025-05-13 10:44:14,880][training][INFO] - Epoch 81, Batch 800/3907, Loss: 0.376114\n",
      "[2025-05-13 10:44:15,281][training][INFO] - Epoch 81, Batch 900/3907, Loss: 0.377953\n",
      "[2025-05-13 10:44:15,685][training][INFO] - Epoch 81, Batch 1000/3907, Loss: 0.369983\n",
      "[2025-05-13 10:44:16,090][training][INFO] - Epoch 81, Batch 1100/3907, Loss: 0.374785\n",
      "[2025-05-13 10:44:16,490][training][INFO] - Epoch 81, Batch 1200/3907, Loss: 0.382853\n",
      "[2025-05-13 10:44:16,892][training][INFO] - Epoch 81, Batch 1300/3907, Loss: 0.375555\n",
      "[2025-05-13 10:44:17,294][training][INFO] - Epoch 81, Batch 1400/3907, Loss: 0.381643\n",
      "[2025-05-13 10:44:17,695][training][INFO] - Epoch 81, Batch 1500/3907, Loss: 0.372593\n",
      "[2025-05-13 10:44:18,099][training][INFO] - Epoch 81, Batch 1600/3907, Loss: 0.377867\n",
      "[2025-05-13 10:44:18,501][training][INFO] - Epoch 81, Batch 1700/3907, Loss: 0.385635\n",
      "[2025-05-13 10:44:18,902][training][INFO] - Epoch 81, Batch 1800/3907, Loss: 0.375687\n",
      "[2025-05-13 10:44:19,302][training][INFO] - Epoch 81, Batch 1900/3907, Loss: 0.384353\n",
      "[2025-05-13 10:44:19,703][training][INFO] - Epoch 81, Batch 2000/3907, Loss: 0.379277\n",
      "[2025-05-13 10:44:20,105][training][INFO] - Epoch 81, Batch 2100/3907, Loss: 0.373985\n",
      "[2025-05-13 10:44:20,505][training][INFO] - Epoch 81, Batch 2200/3907, Loss: 0.370991\n",
      "[2025-05-13 10:44:20,903][training][INFO] - Epoch 81, Batch 2300/3907, Loss: 0.373755\n",
      "[2025-05-13 10:44:21,297][training][INFO] - Epoch 81, Batch 2400/3907, Loss: 0.369556\n",
      "[2025-05-13 10:44:21,683][training][INFO] - Epoch 81, Batch 2500/3907, Loss: 0.379224\n",
      "[2025-05-13 10:44:22,070][training][INFO] - Epoch 81, Batch 2600/3907, Loss: 0.382302\n",
      "[2025-05-13 10:44:22,455][training][INFO] - Epoch 81, Batch 2700/3907, Loss: 0.378444\n",
      "[2025-05-13 10:44:22,842][training][INFO] - Epoch 81, Batch 2800/3907, Loss: 0.372333\n",
      "[2025-05-13 10:44:23,232][training][INFO] - Epoch 81, Batch 2900/3907, Loss: 0.375950\n",
      "[2025-05-13 10:44:23,621][training][INFO] - Epoch 81, Batch 3000/3907, Loss: 0.382864\n",
      "[2025-05-13 10:44:24,011][training][INFO] - Epoch 81, Batch 3100/3907, Loss: 0.373896\n",
      "[2025-05-13 10:44:24,397][training][INFO] - Epoch 81, Batch 3200/3907, Loss: 0.380494\n",
      "[2025-05-13 10:44:24,782][training][INFO] - Epoch 81, Batch 3300/3907, Loss: 0.373482\n",
      "[2025-05-13 10:44:25,173][training][INFO] - Epoch 81, Batch 3400/3907, Loss: 0.374251\n",
      "[2025-05-13 10:44:25,563][training][INFO] - Epoch 81, Batch 3500/3907, Loss: 0.381498\n",
      "[2025-05-13 10:44:25,948][training][INFO] - Epoch 81, Batch 3600/3907, Loss: 0.377708\n",
      "[2025-05-13 10:44:26,336][training][INFO] - Epoch 81, Batch 3700/3907, Loss: 0.379145\n",
      "[2025-05-13 10:44:26,724][training][INFO] - Epoch 81, Batch 3800/3907, Loss: 0.384046\n",
      "[2025-05-13 10:44:27,132][training][INFO] - Epoch 81, Batch 3900/3907, Loss: 0.380895\n",
      "[2025-05-13 10:44:27,149][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:44:27,149][training][INFO] - Epoch 81 complete. Avg Loss: 0.378471\n",
      "[2025-05-13 10:44:27,193][training][INFO] - Epoch 82, Batch 0/3907, Loss: 0.383110\n",
      "[2025-05-13 10:44:27,581][training][INFO] - Epoch 82, Batch 100/3907, Loss: 0.376122\n",
      "[2025-05-13 10:44:27,966][training][INFO] - Epoch 82, Batch 200/3907, Loss: 0.382297\n",
      "[2025-05-13 10:44:28,351][training][INFO] - Epoch 82, Batch 300/3907, Loss: 0.365237\n",
      "[2025-05-13 10:44:28,736][training][INFO] - Epoch 82, Batch 400/3907, Loss: 0.382447\n",
      "[2025-05-13 10:44:29,123][training][INFO] - Epoch 82, Batch 500/3907, Loss: 0.379030\n",
      "[2025-05-13 10:44:29,509][training][INFO] - Epoch 82, Batch 600/3907, Loss: 0.384908\n",
      "[2025-05-13 10:44:29,892][training][INFO] - Epoch 82, Batch 700/3907, Loss: 0.378460\n",
      "[2025-05-13 10:44:30,281][training][INFO] - Epoch 82, Batch 800/3907, Loss: 0.379795\n",
      "[2025-05-13 10:44:30,674][training][INFO] - Epoch 82, Batch 900/3907, Loss: 0.371267\n",
      "[2025-05-13 10:44:31,061][training][INFO] - Epoch 82, Batch 1000/3907, Loss: 0.379861\n",
      "[2025-05-13 10:44:31,444][training][INFO] - Epoch 82, Batch 1100/3907, Loss: 0.379513\n",
      "[2025-05-13 10:44:31,830][training][INFO] - Epoch 82, Batch 1200/3907, Loss: 0.379882\n",
      "[2025-05-13 10:44:32,215][training][INFO] - Epoch 82, Batch 1300/3907, Loss: 0.376678\n",
      "[2025-05-13 10:44:32,601][training][INFO] - Epoch 82, Batch 1400/3907, Loss: 0.383294\n",
      "[2025-05-13 10:44:32,986][training][INFO] - Epoch 82, Batch 1500/3907, Loss: 0.378577\n",
      "[2025-05-13 10:44:33,372][training][INFO] - Epoch 82, Batch 1600/3907, Loss: 0.377768\n",
      "[2025-05-13 10:44:33,757][training][INFO] - Epoch 82, Batch 1700/3907, Loss: 0.382797\n",
      "[2025-05-13 10:44:34,140][training][INFO] - Epoch 82, Batch 1800/3907, Loss: 0.387613\n",
      "[2025-05-13 10:44:34,527][training][INFO] - Epoch 82, Batch 1900/3907, Loss: 0.387482\n",
      "[2025-05-13 10:44:34,914][training][INFO] - Epoch 82, Batch 2000/3907, Loss: 0.382598\n",
      "[2025-05-13 10:44:35,304][training][INFO] - Epoch 82, Batch 2100/3907, Loss: 0.381209\n",
      "[2025-05-13 10:44:35,697][training][INFO] - Epoch 82, Batch 2200/3907, Loss: 0.385973\n",
      "[2025-05-13 10:44:36,084][training][INFO] - Epoch 82, Batch 2300/3907, Loss: 0.376784\n",
      "[2025-05-13 10:44:36,471][training][INFO] - Epoch 82, Batch 2400/3907, Loss: 0.371248\n",
      "[2025-05-13 10:44:36,861][training][INFO] - Epoch 82, Batch 2500/3907, Loss: 0.378825\n",
      "[2025-05-13 10:44:37,247][training][INFO] - Epoch 82, Batch 2600/3907, Loss: 0.379106\n",
      "[2025-05-13 10:44:37,633][training][INFO] - Epoch 82, Batch 2700/3907, Loss: 0.381450\n",
      "[2025-05-13 10:44:38,020][training][INFO] - Epoch 82, Batch 2800/3907, Loss: 0.380171\n",
      "[2025-05-13 10:44:38,409][training][INFO] - Epoch 82, Batch 2900/3907, Loss: 0.366118\n",
      "[2025-05-13 10:44:38,796][training][INFO] - Epoch 82, Batch 3000/3907, Loss: 0.375691\n",
      "[2025-05-13 10:44:39,181][training][INFO] - Epoch 82, Batch 3100/3907, Loss: 0.371681\n",
      "[2025-05-13 10:44:39,566][training][INFO] - Epoch 82, Batch 3200/3907, Loss: 0.381299\n",
      "[2025-05-13 10:44:39,949][training][INFO] - Epoch 82, Batch 3300/3907, Loss: 0.385687\n",
      "[2025-05-13 10:44:40,336][training][INFO] - Epoch 82, Batch 3400/3907, Loss: 0.377060\n",
      "[2025-05-13 10:44:40,723][training][INFO] - Epoch 82, Batch 3500/3907, Loss: 0.382663\n",
      "[2025-05-13 10:44:41,109][training][INFO] - Epoch 82, Batch 3600/3907, Loss: 0.371367\n",
      "[2025-05-13 10:44:41,496][training][INFO] - Epoch 82, Batch 3700/3907, Loss: 0.384287\n",
      "[2025-05-13 10:44:41,882][training][INFO] - Epoch 82, Batch 3800/3907, Loss: 0.380309\n",
      "[2025-05-13 10:44:42,284][training][INFO] - Epoch 82, Batch 3900/3907, Loss: 0.383638\n",
      "[2025-05-13 10:44:42,301][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:44:42,301][training][INFO] - Epoch 82 complete. Avg Loss: 0.378327\n",
      "[2025-05-13 10:44:42,346][training][INFO] - Epoch 83, Batch 0/3907, Loss: 0.374432\n",
      "[2025-05-13 10:44:42,734][training][INFO] - Epoch 83, Batch 100/3907, Loss: 0.379227\n",
      "[2025-05-13 10:44:43,121][training][INFO] - Epoch 83, Batch 200/3907, Loss: 0.378309\n",
      "[2025-05-13 10:44:43,509][training][INFO] - Epoch 83, Batch 300/3907, Loss: 0.375568\n",
      "[2025-05-13 10:44:43,897][training][INFO] - Epoch 83, Batch 400/3907, Loss: 0.377377\n",
      "[2025-05-13 10:44:44,285][training][INFO] - Epoch 83, Batch 500/3907, Loss: 0.380759\n",
      "[2025-05-13 10:44:44,667][training][INFO] - Epoch 83, Batch 600/3907, Loss: 0.385151\n",
      "[2025-05-13 10:44:45,052][training][INFO] - Epoch 83, Batch 700/3907, Loss: 0.374310\n",
      "[2025-05-13 10:44:45,436][training][INFO] - Epoch 83, Batch 800/3907, Loss: 0.372267\n",
      "[2025-05-13 10:44:45,827][training][INFO] - Epoch 83, Batch 900/3907, Loss: 0.377899\n",
      "[2025-05-13 10:44:46,217][training][INFO] - Epoch 83, Batch 1000/3907, Loss: 0.380387\n",
      "[2025-05-13 10:44:46,604][training][INFO] - Epoch 83, Batch 1100/3907, Loss: 0.377883\n",
      "[2025-05-13 10:44:46,994][training][INFO] - Epoch 83, Batch 1200/3907, Loss: 0.379547\n",
      "[2025-05-13 10:44:47,381][training][INFO] - Epoch 83, Batch 1300/3907, Loss: 0.376198\n",
      "[2025-05-13 10:44:47,768][training][INFO] - Epoch 83, Batch 1400/3907, Loss: 0.370076\n",
      "[2025-05-13 10:44:48,155][training][INFO] - Epoch 83, Batch 1500/3907, Loss: 0.387560\n",
      "[2025-05-13 10:44:48,544][training][INFO] - Epoch 83, Batch 1600/3907, Loss: 0.384017\n",
      "[2025-05-13 10:44:48,936][training][INFO] - Epoch 83, Batch 1700/3907, Loss: 0.381991\n",
      "[2025-05-13 10:44:49,331][training][INFO] - Epoch 83, Batch 1800/3907, Loss: 0.376787\n",
      "[2025-05-13 10:44:49,717][training][INFO] - Epoch 83, Batch 1900/3907, Loss: 0.376936\n",
      "[2025-05-13 10:44:50,104][training][INFO] - Epoch 83, Batch 2000/3907, Loss: 0.375732\n",
      "[2025-05-13 10:44:50,492][training][INFO] - Epoch 83, Batch 2100/3907, Loss: 0.382422\n",
      "[2025-05-13 10:44:50,878][training][INFO] - Epoch 83, Batch 2200/3907, Loss: 0.377757\n",
      "[2025-05-13 10:44:51,264][training][INFO] - Epoch 83, Batch 2300/3907, Loss: 0.374593\n",
      "[2025-05-13 10:44:51,652][training][INFO] - Epoch 83, Batch 2400/3907, Loss: 0.366355\n",
      "[2025-05-13 10:44:52,037][training][INFO] - Epoch 83, Batch 2500/3907, Loss: 0.380122\n",
      "[2025-05-13 10:44:52,423][training][INFO] - Epoch 83, Batch 2600/3907, Loss: 0.383645\n",
      "[2025-05-13 10:44:52,809][training][INFO] - Epoch 83, Batch 2700/3907, Loss: 0.379745\n",
      "[2025-05-13 10:44:53,196][training][INFO] - Epoch 83, Batch 2800/3907, Loss: 0.370762\n",
      "[2025-05-13 10:44:53,587][training][INFO] - Epoch 83, Batch 2900/3907, Loss: 0.376314\n",
      "[2025-05-13 10:44:53,972][training][INFO] - Epoch 83, Batch 3000/3907, Loss: 0.382357\n",
      "[2025-05-13 10:44:54,361][training][INFO] - Epoch 83, Batch 3100/3907, Loss: 0.382825\n",
      "[2025-05-13 10:44:54,747][training][INFO] - Epoch 83, Batch 3200/3907, Loss: 0.368072\n",
      "[2025-05-13 10:44:55,133][training][INFO] - Epoch 83, Batch 3300/3907, Loss: 0.379712\n",
      "[2025-05-13 10:44:55,519][training][INFO] - Epoch 83, Batch 3400/3907, Loss: 0.374410\n",
      "[2025-05-13 10:44:55,907][training][INFO] - Epoch 83, Batch 3500/3907, Loss: 0.375766\n",
      "[2025-05-13 10:44:56,292][training][INFO] - Epoch 83, Batch 3600/3907, Loss: 0.374785\n",
      "[2025-05-13 10:44:56,678][training][INFO] - Epoch 83, Batch 3700/3907, Loss: 0.382723\n",
      "[2025-05-13 10:44:57,067][training][INFO] - Epoch 83, Batch 3800/3907, Loss: 0.370587\n",
      "[2025-05-13 10:44:57,475][training][INFO] - Epoch 83, Batch 3900/3907, Loss: 0.381487\n",
      "[2025-05-13 10:44:57,491][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:44:57,491][training][INFO] - Epoch 83 complete. Avg Loss: 0.378222\n",
      "[2025-05-13 10:44:57,536][training][INFO] - Epoch 84, Batch 0/3907, Loss: 0.379108\n",
      "[2025-05-13 10:44:57,923][training][INFO] - Epoch 84, Batch 100/3907, Loss: 0.385020\n",
      "[2025-05-13 10:44:58,309][training][INFO] - Epoch 84, Batch 200/3907, Loss: 0.379659\n",
      "[2025-05-13 10:44:58,695][training][INFO] - Epoch 84, Batch 300/3907, Loss: 0.378313\n",
      "[2025-05-13 10:44:59,081][training][INFO] - Epoch 84, Batch 400/3907, Loss: 0.382896\n",
      "[2025-05-13 10:44:59,467][training][INFO] - Epoch 84, Batch 500/3907, Loss: 0.378432\n",
      "[2025-05-13 10:44:59,855][training][INFO] - Epoch 84, Batch 600/3907, Loss: 0.379986\n",
      "[2025-05-13 10:45:00,244][training][INFO] - Epoch 84, Batch 700/3907, Loss: 0.368071\n",
      "[2025-05-13 10:45:00,637][training][INFO] - Epoch 84, Batch 800/3907, Loss: 0.374922\n",
      "[2025-05-13 10:45:01,028][training][INFO] - Epoch 84, Batch 900/3907, Loss: 0.372274\n",
      "[2025-05-13 10:45:01,416][training][INFO] - Epoch 84, Batch 1000/3907, Loss: 0.377473\n",
      "[2025-05-13 10:45:01,803][training][INFO] - Epoch 84, Batch 1100/3907, Loss: 0.379381\n",
      "[2025-05-13 10:45:02,187][training][INFO] - Epoch 84, Batch 1200/3907, Loss: 0.381063\n",
      "[2025-05-13 10:45:02,571][training][INFO] - Epoch 84, Batch 1300/3907, Loss: 0.380945\n",
      "[2025-05-13 10:45:02,959][training][INFO] - Epoch 84, Batch 1400/3907, Loss: 0.368627\n",
      "[2025-05-13 10:45:03,348][training][INFO] - Epoch 84, Batch 1500/3907, Loss: 0.382607\n",
      "[2025-05-13 10:45:03,735][training][INFO] - Epoch 84, Batch 1600/3907, Loss: 0.373891\n",
      "[2025-05-13 10:45:04,120][training][INFO] - Epoch 84, Batch 1700/3907, Loss: 0.376839\n",
      "[2025-05-13 10:45:04,507][training][INFO] - Epoch 84, Batch 1800/3907, Loss: 0.382570\n",
      "[2025-05-13 10:45:04,895][training][INFO] - Epoch 84, Batch 1900/3907, Loss: 0.381314\n",
      "[2025-05-13 10:45:05,282][training][INFO] - Epoch 84, Batch 2000/3907, Loss: 0.377154\n",
      "[2025-05-13 10:45:05,670][training][INFO] - Epoch 84, Batch 2100/3907, Loss: 0.380239\n",
      "[2025-05-13 10:45:06,053][training][INFO] - Epoch 84, Batch 2200/3907, Loss: 0.380647\n",
      "[2025-05-13 10:45:06,441][training][INFO] - Epoch 84, Batch 2300/3907, Loss: 0.392268\n",
      "[2025-05-13 10:45:06,830][training][INFO] - Epoch 84, Batch 2400/3907, Loss: 0.373809\n",
      "[2025-05-13 10:45:07,216][training][INFO] - Epoch 84, Batch 2500/3907, Loss: 0.377099\n",
      "[2025-05-13 10:45:07,606][training][INFO] - Epoch 84, Batch 2600/3907, Loss: 0.376098\n",
      "[2025-05-13 10:45:07,992][training][INFO] - Epoch 84, Batch 2700/3907, Loss: 0.383484\n",
      "[2025-05-13 10:45:08,379][training][INFO] - Epoch 84, Batch 2800/3907, Loss: 0.381077\n",
      "[2025-05-13 10:45:08,764][training][INFO] - Epoch 84, Batch 2900/3907, Loss: 0.382959\n",
      "[2025-05-13 10:45:09,151][training][INFO] - Epoch 84, Batch 3000/3907, Loss: 0.386830\n",
      "[2025-05-13 10:45:09,533][training][INFO] - Epoch 84, Batch 3100/3907, Loss: 0.377298\n",
      "[2025-05-13 10:45:09,917][training][INFO] - Epoch 84, Batch 3200/3907, Loss: 0.372594\n",
      "[2025-05-13 10:45:10,303][training][INFO] - Epoch 84, Batch 3300/3907, Loss: 0.380779\n",
      "[2025-05-13 10:45:10,690][training][INFO] - Epoch 84, Batch 3400/3907, Loss: 0.374509\n",
      "[2025-05-13 10:45:11,076][training][INFO] - Epoch 84, Batch 3500/3907, Loss: 0.376529\n",
      "[2025-05-13 10:45:11,464][training][INFO] - Epoch 84, Batch 3600/3907, Loss: 0.380333\n",
      "[2025-05-13 10:45:11,853][training][INFO] - Epoch 84, Batch 3700/3907, Loss: 0.375593\n",
      "[2025-05-13 10:45:12,241][training][INFO] - Epoch 84, Batch 3800/3907, Loss: 0.374198\n",
      "[2025-05-13 10:45:12,650][training][INFO] - Epoch 84, Batch 3900/3907, Loss: 0.372993\n",
      "[2025-05-13 10:45:12,667][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:45:12,667][training][INFO] - Epoch 84 complete. Avg Loss: 0.378129\n",
      "[2025-05-13 10:45:12,695][training][INFO] - Epoch 85, Batch 0/3907, Loss: 0.375958\n",
      "[2025-05-13 10:45:13,099][training][INFO] - Epoch 85, Batch 100/3907, Loss: 0.386868\n",
      "[2025-05-13 10:45:13,485][training][INFO] - Epoch 85, Batch 200/3907, Loss: 0.377498\n",
      "[2025-05-13 10:45:13,871][training][INFO] - Epoch 85, Batch 300/3907, Loss: 0.379639\n",
      "[2025-05-13 10:45:14,256][training][INFO] - Epoch 85, Batch 400/3907, Loss: 0.371841\n",
      "[2025-05-13 10:45:14,645][training][INFO] - Epoch 85, Batch 500/3907, Loss: 0.381760\n",
      "[2025-05-13 10:45:15,035][training][INFO] - Epoch 85, Batch 600/3907, Loss: 0.384947\n",
      "[2025-05-13 10:45:15,427][training][INFO] - Epoch 85, Batch 700/3907, Loss: 0.383369\n",
      "[2025-05-13 10:45:15,818][training][INFO] - Epoch 85, Batch 800/3907, Loss: 0.378530\n",
      "[2025-05-13 10:45:16,209][training][INFO] - Epoch 85, Batch 900/3907, Loss: 0.381327\n",
      "[2025-05-13 10:45:16,595][training][INFO] - Epoch 85, Batch 1000/3907, Loss: 0.383435\n",
      "[2025-05-13 10:45:16,981][training][INFO] - Epoch 85, Batch 1100/3907, Loss: 0.382652\n",
      "[2025-05-13 10:45:17,367][training][INFO] - Epoch 85, Batch 1200/3907, Loss: 0.367599\n",
      "[2025-05-13 10:45:17,753][training][INFO] - Epoch 85, Batch 1300/3907, Loss: 0.380228\n",
      "[2025-05-13 10:45:18,139][training][INFO] - Epoch 85, Batch 1400/3907, Loss: 0.373648\n",
      "[2025-05-13 10:45:18,527][training][INFO] - Epoch 85, Batch 1500/3907, Loss: 0.383656\n",
      "[2025-05-13 10:45:18,914][training][INFO] - Epoch 85, Batch 1600/3907, Loss: 0.376526\n",
      "[2025-05-13 10:45:19,299][training][INFO] - Epoch 85, Batch 1700/3907, Loss: 0.374775\n",
      "[2025-05-13 10:45:19,683][training][INFO] - Epoch 85, Batch 1800/3907, Loss: 0.382591\n",
      "[2025-05-13 10:45:20,070][training][INFO] - Epoch 85, Batch 1900/3907, Loss: 0.377306\n",
      "[2025-05-13 10:45:20,456][training][INFO] - Epoch 85, Batch 2000/3907, Loss: 0.375256\n",
      "[2025-05-13 10:45:20,844][training][INFO] - Epoch 85, Batch 2100/3907, Loss: 0.374195\n",
      "[2025-05-13 10:45:21,240][training][INFO] - Epoch 85, Batch 2200/3907, Loss: 0.381068\n",
      "[2025-05-13 10:45:21,628][training][INFO] - Epoch 85, Batch 2300/3907, Loss: 0.378397\n",
      "[2025-05-13 10:45:22,016][training][INFO] - Epoch 85, Batch 2400/3907, Loss: 0.376402\n",
      "[2025-05-13 10:45:22,406][training][INFO] - Epoch 85, Batch 2500/3907, Loss: 0.379794\n",
      "[2025-05-13 10:45:22,794][training][INFO] - Epoch 85, Batch 2600/3907, Loss: 0.381914\n",
      "[2025-05-13 10:45:23,182][training][INFO] - Epoch 85, Batch 2700/3907, Loss: 0.379703\n",
      "[2025-05-13 10:45:23,577][training][INFO] - Epoch 85, Batch 2800/3907, Loss: 0.377073\n",
      "[2025-05-13 10:45:23,965][training][INFO] - Epoch 85, Batch 2900/3907, Loss: 0.370662\n",
      "[2025-05-13 10:45:24,354][training][INFO] - Epoch 85, Batch 3000/3907, Loss: 0.374045\n",
      "[2025-05-13 10:45:24,743][training][INFO] - Epoch 85, Batch 3100/3907, Loss: 0.383981\n",
      "[2025-05-13 10:45:25,132][training][INFO] - Epoch 85, Batch 3200/3907, Loss: 0.384872\n",
      "[2025-05-13 10:45:25,521][training][INFO] - Epoch 85, Batch 3300/3907, Loss: 0.371359\n",
      "[2025-05-13 10:45:25,907][training][INFO] - Epoch 85, Batch 3400/3907, Loss: 0.367763\n",
      "[2025-05-13 10:45:26,293][training][INFO] - Epoch 85, Batch 3500/3907, Loss: 0.377310\n",
      "[2025-05-13 10:45:26,680][training][INFO] - Epoch 85, Batch 3600/3907, Loss: 0.374368\n",
      "[2025-05-13 10:45:27,080][training][INFO] - Epoch 85, Batch 3700/3907, Loss: 0.384049\n",
      "[2025-05-13 10:45:27,470][training][INFO] - Epoch 85, Batch 3800/3907, Loss: 0.371147\n",
      "[2025-05-13 10:45:27,877][training][INFO] - Epoch 85, Batch 3900/3907, Loss: 0.375073\n",
      "[2025-05-13 10:45:27,894][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:45:27,894][training][INFO] - Epoch 85 complete. Avg Loss: 0.377973\n",
      "[2025-05-13 10:45:27,940][training][INFO] - Epoch 86, Batch 0/3907, Loss: 0.385683\n",
      "[2025-05-13 10:45:28,326][training][INFO] - Epoch 86, Batch 100/3907, Loss: 0.371892\n",
      "[2025-05-13 10:45:28,709][training][INFO] - Epoch 86, Batch 200/3907, Loss: 0.377755\n",
      "[2025-05-13 10:45:29,095][training][INFO] - Epoch 86, Batch 300/3907, Loss: 0.375673\n",
      "[2025-05-13 10:45:29,482][training][INFO] - Epoch 86, Batch 400/3907, Loss: 0.385637\n",
      "[2025-05-13 10:45:29,870][training][INFO] - Epoch 86, Batch 500/3907, Loss: 0.371794\n",
      "[2025-05-13 10:45:30,255][training][INFO] - Epoch 86, Batch 600/3907, Loss: 0.376987\n",
      "[2025-05-13 10:45:30,645][training][INFO] - Epoch 86, Batch 700/3907, Loss: 0.378203\n",
      "[2025-05-13 10:45:31,032][training][INFO] - Epoch 86, Batch 800/3907, Loss: 0.377119\n",
      "[2025-05-13 10:45:31,418][training][INFO] - Epoch 86, Batch 900/3907, Loss: 0.383778\n",
      "[2025-05-13 10:45:31,805][training][INFO] - Epoch 86, Batch 1000/3907, Loss: 0.376312\n",
      "[2025-05-13 10:45:32,190][training][INFO] - Epoch 86, Batch 1100/3907, Loss: 0.378997\n",
      "[2025-05-13 10:45:32,575][training][INFO] - Epoch 86, Batch 1200/3907, Loss: 0.370746\n",
      "[2025-05-13 10:45:32,961][training][INFO] - Epoch 86, Batch 1300/3907, Loss: 0.384328\n",
      "[2025-05-13 10:45:33,346][training][INFO] - Epoch 86, Batch 1400/3907, Loss: 0.382461\n",
      "[2025-05-13 10:45:33,731][training][INFO] - Epoch 86, Batch 1500/3907, Loss: 0.371889\n",
      "[2025-05-13 10:45:34,119][training][INFO] - Epoch 86, Batch 1600/3907, Loss: 0.382214\n",
      "[2025-05-13 10:45:34,505][training][INFO] - Epoch 86, Batch 1700/3907, Loss: 0.389245\n",
      "[2025-05-13 10:45:34,890][training][INFO] - Epoch 86, Batch 1800/3907, Loss: 0.364557\n",
      "[2025-05-13 10:45:35,278][training][INFO] - Epoch 86, Batch 1900/3907, Loss: 0.383110\n",
      "[2025-05-13 10:45:35,665][training][INFO] - Epoch 86, Batch 2000/3907, Loss: 0.376483\n",
      "[2025-05-13 10:45:36,053][training][INFO] - Epoch 86, Batch 2100/3907, Loss: 0.378827\n",
      "[2025-05-13 10:45:36,440][training][INFO] - Epoch 86, Batch 2200/3907, Loss: 0.384823\n",
      "[2025-05-13 10:45:36,827][training][INFO] - Epoch 86, Batch 2300/3907, Loss: 0.382621\n",
      "[2025-05-13 10:45:37,216][training][INFO] - Epoch 86, Batch 2400/3907, Loss: 0.382218\n",
      "[2025-05-13 10:45:37,604][training][INFO] - Epoch 86, Batch 2500/3907, Loss: 0.385913\n",
      "[2025-05-13 10:45:37,991][training][INFO] - Epoch 86, Batch 2600/3907, Loss: 0.378959\n",
      "[2025-05-13 10:45:38,378][training][INFO] - Epoch 86, Batch 2700/3907, Loss: 0.385357\n",
      "[2025-05-13 10:45:38,765][training][INFO] - Epoch 86, Batch 2800/3907, Loss: 0.370327\n",
      "[2025-05-13 10:45:39,150][training][INFO] - Epoch 86, Batch 2900/3907, Loss: 0.370869\n",
      "[2025-05-13 10:45:39,539][training][INFO] - Epoch 86, Batch 3000/3907, Loss: 0.371359\n",
      "[2025-05-13 10:45:39,931][training][INFO] - Epoch 86, Batch 3100/3907, Loss: 0.385353\n",
      "[2025-05-13 10:45:40,319][training][INFO] - Epoch 86, Batch 3200/3907, Loss: 0.373751\n",
      "[2025-05-13 10:45:40,705][training][INFO] - Epoch 86, Batch 3300/3907, Loss: 0.369828\n",
      "[2025-05-13 10:45:41,091][training][INFO] - Epoch 86, Batch 3400/3907, Loss: 0.372353\n",
      "[2025-05-13 10:45:41,477][training][INFO] - Epoch 86, Batch 3500/3907, Loss: 0.379035\n",
      "[2025-05-13 10:45:41,862][training][INFO] - Epoch 86, Batch 3600/3907, Loss: 0.387511\n",
      "[2025-05-13 10:45:42,249][training][INFO] - Epoch 86, Batch 3700/3907, Loss: 0.382273\n",
      "[2025-05-13 10:45:42,637][training][INFO] - Epoch 86, Batch 3800/3907, Loss: 0.381877\n",
      "[2025-05-13 10:45:43,046][training][INFO] - Epoch 86, Batch 3900/3907, Loss: 0.372635\n",
      "[2025-05-13 10:45:43,063][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:45:43,064][training][INFO] - Epoch 86 complete. Avg Loss: 0.377818\n",
      "[2025-05-13 10:45:43,107][training][INFO] - Epoch 87, Batch 0/3907, Loss: 0.378037\n",
      "[2025-05-13 10:45:43,499][training][INFO] - Epoch 87, Batch 100/3907, Loss: 0.375405\n",
      "[2025-05-13 10:45:43,886][training][INFO] - Epoch 87, Batch 200/3907, Loss: 0.376902\n",
      "[2025-05-13 10:45:44,272][training][INFO] - Epoch 87, Batch 300/3907, Loss: 0.376370\n",
      "[2025-05-13 10:45:44,659][training][INFO] - Epoch 87, Batch 400/3907, Loss: 0.384438\n",
      "[2025-05-13 10:45:45,047][training][INFO] - Epoch 87, Batch 500/3907, Loss: 0.377089\n",
      "[2025-05-13 10:45:45,433][training][INFO] - Epoch 87, Batch 600/3907, Loss: 0.382166\n",
      "[2025-05-13 10:45:45,823][training][INFO] - Epoch 87, Batch 700/3907, Loss: 0.382044\n",
      "[2025-05-13 10:45:46,209][training][INFO] - Epoch 87, Batch 800/3907, Loss: 0.381077\n",
      "[2025-05-13 10:45:46,593][training][INFO] - Epoch 87, Batch 900/3907, Loss: 0.378805\n",
      "[2025-05-13 10:45:46,978][training][INFO] - Epoch 87, Batch 1000/3907, Loss: 0.378005\n",
      "[2025-05-13 10:45:47,361][training][INFO] - Epoch 87, Batch 1100/3907, Loss: 0.379963\n",
      "[2025-05-13 10:45:47,751][training][INFO] - Epoch 87, Batch 1200/3907, Loss: 0.380898\n",
      "[2025-05-13 10:45:48,136][training][INFO] - Epoch 87, Batch 1300/3907, Loss: 0.378601\n",
      "[2025-05-13 10:45:48,522][training][INFO] - Epoch 87, Batch 1400/3907, Loss: 0.372500\n",
      "[2025-05-13 10:45:48,908][training][INFO] - Epoch 87, Batch 1500/3907, Loss: 0.379991\n",
      "[2025-05-13 10:45:49,297][training][INFO] - Epoch 87, Batch 1600/3907, Loss: 0.379740\n",
      "[2025-05-13 10:45:49,684][training][INFO] - Epoch 87, Batch 1700/3907, Loss: 0.378261\n",
      "[2025-05-13 10:45:50,071][training][INFO] - Epoch 87, Batch 1800/3907, Loss: 0.379322\n",
      "[2025-05-13 10:45:50,459][training][INFO] - Epoch 87, Batch 1900/3907, Loss: 0.379570\n",
      "[2025-05-13 10:45:50,843][training][INFO] - Epoch 87, Batch 2000/3907, Loss: 0.370579\n",
      "[2025-05-13 10:45:51,231][training][INFO] - Epoch 87, Batch 2100/3907, Loss: 0.373829\n",
      "[2025-05-13 10:45:51,617][training][INFO] - Epoch 87, Batch 2200/3907, Loss: 0.382349\n",
      "[2025-05-13 10:45:52,005][training][INFO] - Epoch 87, Batch 2300/3907, Loss: 0.378705\n",
      "[2025-05-13 10:45:52,391][training][INFO] - Epoch 87, Batch 2400/3907, Loss: 0.376663\n",
      "[2025-05-13 10:45:52,779][training][INFO] - Epoch 87, Batch 2500/3907, Loss: 0.373577\n",
      "[2025-05-13 10:45:53,168][training][INFO] - Epoch 87, Batch 2600/3907, Loss: 0.364005\n",
      "[2025-05-13 10:45:53,556][training][INFO] - Epoch 87, Batch 2700/3907, Loss: 0.384775\n",
      "[2025-05-13 10:45:53,942][training][INFO] - Epoch 87, Batch 2800/3907, Loss: 0.385343\n",
      "[2025-05-13 10:45:54,330][training][INFO] - Epoch 87, Batch 2900/3907, Loss: 0.387171\n",
      "[2025-05-13 10:45:54,714][training][INFO] - Epoch 87, Batch 3000/3907, Loss: 0.378909\n",
      "[2025-05-13 10:45:55,102][training][INFO] - Epoch 87, Batch 3100/3907, Loss: 0.380289\n",
      "[2025-05-13 10:45:55,489][training][INFO] - Epoch 87, Batch 3200/3907, Loss: 0.370229\n",
      "[2025-05-13 10:45:55,877][training][INFO] - Epoch 87, Batch 3300/3907, Loss: 0.375721\n",
      "[2025-05-13 10:45:56,262][training][INFO] - Epoch 87, Batch 3400/3907, Loss: 0.375642\n",
      "[2025-05-13 10:45:56,648][training][INFO] - Epoch 87, Batch 3500/3907, Loss: 0.377325\n",
      "[2025-05-13 10:45:57,037][training][INFO] - Epoch 87, Batch 3600/3907, Loss: 0.376077\n",
      "[2025-05-13 10:45:57,424][training][INFO] - Epoch 87, Batch 3700/3907, Loss: 0.378473\n",
      "[2025-05-13 10:45:57,812][training][INFO] - Epoch 87, Batch 3800/3907, Loss: 0.374605\n",
      "[2025-05-13 10:45:58,218][training][INFO] - Epoch 87, Batch 3900/3907, Loss: 0.382751\n",
      "[2025-05-13 10:45:58,235][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:45:58,235][training][INFO] - Epoch 87 complete. Avg Loss: 0.377758\n",
      "[2025-05-13 10:45:58,276][training][INFO] - Epoch 88, Batch 0/3907, Loss: 0.370317\n",
      "[2025-05-13 10:45:58,666][training][INFO] - Epoch 88, Batch 100/3907, Loss: 0.379820\n",
      "[2025-05-13 10:45:59,052][training][INFO] - Epoch 88, Batch 200/3907, Loss: 0.378182\n",
      "[2025-05-13 10:45:59,437][training][INFO] - Epoch 88, Batch 300/3907, Loss: 0.384799\n",
      "[2025-05-13 10:45:59,824][training][INFO] - Epoch 88, Batch 400/3907, Loss: 0.376173\n",
      "[2025-05-13 10:46:00,214][training][INFO] - Epoch 88, Batch 500/3907, Loss: 0.378920\n",
      "[2025-05-13 10:46:00,602][training][INFO] - Epoch 88, Batch 600/3907, Loss: 0.369372\n",
      "[2025-05-13 10:46:00,991][training][INFO] - Epoch 88, Batch 700/3907, Loss: 0.373339\n",
      "[2025-05-13 10:46:01,380][training][INFO] - Epoch 88, Batch 800/3907, Loss: 0.382872\n",
      "[2025-05-13 10:46:01,766][training][INFO] - Epoch 88, Batch 900/3907, Loss: 0.378511\n",
      "[2025-05-13 10:46:02,152][training][INFO] - Epoch 88, Batch 1000/3907, Loss: 0.372120\n",
      "[2025-05-13 10:46:02,536][training][INFO] - Epoch 88, Batch 1100/3907, Loss: 0.369737\n",
      "[2025-05-13 10:46:02,924][training][INFO] - Epoch 88, Batch 1200/3907, Loss: 0.378258\n",
      "[2025-05-13 10:46:03,307][training][INFO] - Epoch 88, Batch 1300/3907, Loss: 0.373025\n",
      "[2025-05-13 10:46:03,694][training][INFO] - Epoch 88, Batch 1400/3907, Loss: 0.374668\n",
      "[2025-05-13 10:46:04,082][training][INFO] - Epoch 88, Batch 1500/3907, Loss: 0.381941\n",
      "[2025-05-13 10:46:04,468][training][INFO] - Epoch 88, Batch 1600/3907, Loss: 0.381516\n",
      "[2025-05-13 10:46:04,856][training][INFO] - Epoch 88, Batch 1700/3907, Loss: 0.381720\n",
      "[2025-05-13 10:46:05,243][training][INFO] - Epoch 88, Batch 1800/3907, Loss: 0.377363\n",
      "[2025-05-13 10:46:05,632][training][INFO] - Epoch 88, Batch 1900/3907, Loss: 0.380065\n",
      "[2025-05-13 10:46:06,022][training][INFO] - Epoch 88, Batch 2000/3907, Loss: 0.370987\n",
      "[2025-05-13 10:46:06,410][training][INFO] - Epoch 88, Batch 2100/3907, Loss: 0.370965\n",
      "[2025-05-13 10:46:06,798][training][INFO] - Epoch 88, Batch 2200/3907, Loss: 0.380712\n",
      "[2025-05-13 10:46:07,189][training][INFO] - Epoch 88, Batch 2300/3907, Loss: 0.377151\n",
      "[2025-05-13 10:46:07,586][training][INFO] - Epoch 88, Batch 2400/3907, Loss: 0.375009\n",
      "[2025-05-13 10:46:07,981][training][INFO] - Epoch 88, Batch 2500/3907, Loss: 0.373497\n",
      "[2025-05-13 10:46:08,374][training][INFO] - Epoch 88, Batch 2600/3907, Loss: 0.372810\n",
      "[2025-05-13 10:46:08,767][training][INFO] - Epoch 88, Batch 2700/3907, Loss: 0.378018\n",
      "[2025-05-13 10:46:09,159][training][INFO] - Epoch 88, Batch 2800/3907, Loss: 0.384589\n",
      "[2025-05-13 10:46:09,550][training][INFO] - Epoch 88, Batch 2900/3907, Loss: 0.392024\n",
      "[2025-05-13 10:46:09,938][training][INFO] - Epoch 88, Batch 3000/3907, Loss: 0.388376\n",
      "[2025-05-13 10:46:10,330][training][INFO] - Epoch 88, Batch 3100/3907, Loss: 0.380420\n",
      "[2025-05-13 10:46:10,721][training][INFO] - Epoch 88, Batch 3200/3907, Loss: 0.383069\n",
      "[2025-05-13 10:46:11,114][training][INFO] - Epoch 88, Batch 3300/3907, Loss: 0.378588\n",
      "[2025-05-13 10:46:11,506][training][INFO] - Epoch 88, Batch 3400/3907, Loss: 0.384828\n",
      "[2025-05-13 10:46:11,891][training][INFO] - Epoch 88, Batch 3500/3907, Loss: 0.377632\n",
      "[2025-05-13 10:46:12,281][training][INFO] - Epoch 88, Batch 3600/3907, Loss: 0.371026\n",
      "[2025-05-13 10:46:12,676][training][INFO] - Epoch 88, Batch 3700/3907, Loss: 0.382405\n",
      "[2025-05-13 10:46:13,075][training][INFO] - Epoch 88, Batch 3800/3907, Loss: 0.374051\n",
      "[2025-05-13 10:46:13,489][training][INFO] - Epoch 88, Batch 3900/3907, Loss: 0.379079\n",
      "[2025-05-13 10:46:13,506][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:46:13,506][training][INFO] - Epoch 88 complete. Avg Loss: 0.377696\n",
      "[2025-05-13 10:46:13,550][training][INFO] - Epoch 89, Batch 0/3907, Loss: 0.380222\n",
      "[2025-05-13 10:46:13,939][training][INFO] - Epoch 89, Batch 100/3907, Loss: 0.386074\n",
      "[2025-05-13 10:46:14,327][training][INFO] - Epoch 89, Batch 200/3907, Loss: 0.370929\n",
      "[2025-05-13 10:46:14,718][training][INFO] - Epoch 89, Batch 300/3907, Loss: 0.378878\n",
      "[2025-05-13 10:46:15,104][training][INFO] - Epoch 89, Batch 400/3907, Loss: 0.374627\n",
      "[2025-05-13 10:46:15,493][training][INFO] - Epoch 89, Batch 500/3907, Loss: 0.379410\n",
      "[2025-05-13 10:46:15,885][training][INFO] - Epoch 89, Batch 600/3907, Loss: 0.384793\n",
      "[2025-05-13 10:46:16,276][training][INFO] - Epoch 89, Batch 700/3907, Loss: 0.380269\n",
      "[2025-05-13 10:46:16,664][training][INFO] - Epoch 89, Batch 800/3907, Loss: 0.378392\n",
      "[2025-05-13 10:46:17,050][training][INFO] - Epoch 89, Batch 900/3907, Loss: 0.374939\n",
      "[2025-05-13 10:46:17,435][training][INFO] - Epoch 89, Batch 1000/3907, Loss: 0.383156\n",
      "[2025-05-13 10:46:17,823][training][INFO] - Epoch 89, Batch 1100/3907, Loss: 0.380699\n",
      "[2025-05-13 10:46:18,210][training][INFO] - Epoch 89, Batch 1200/3907, Loss: 0.377640\n",
      "[2025-05-13 10:46:18,598][training][INFO] - Epoch 89, Batch 1300/3907, Loss: 0.381332\n",
      "[2025-05-13 10:46:18,985][training][INFO] - Epoch 89, Batch 1400/3907, Loss: 0.367792\n",
      "[2025-05-13 10:46:19,375][training][INFO] - Epoch 89, Batch 1500/3907, Loss: 0.378564\n",
      "[2025-05-13 10:46:19,766][training][INFO] - Epoch 89, Batch 1600/3907, Loss: 0.383865\n",
      "[2025-05-13 10:46:20,149][training][INFO] - Epoch 89, Batch 1700/3907, Loss: 0.368827\n",
      "[2025-05-13 10:46:20,536][training][INFO] - Epoch 89, Batch 1800/3907, Loss: 0.368212\n",
      "[2025-05-13 10:46:20,921][training][INFO] - Epoch 89, Batch 1900/3907, Loss: 0.371112\n",
      "[2025-05-13 10:46:21,304][training][INFO] - Epoch 89, Batch 2000/3907, Loss: 0.378827\n",
      "[2025-05-13 10:46:21,692][training][INFO] - Epoch 89, Batch 2100/3907, Loss: 0.372650\n",
      "[2025-05-13 10:46:22,080][training][INFO] - Epoch 89, Batch 2200/3907, Loss: 0.381418\n",
      "[2025-05-13 10:46:22,465][training][INFO] - Epoch 89, Batch 2300/3907, Loss: 0.382691\n",
      "[2025-05-13 10:46:22,854][training][INFO] - Epoch 89, Batch 2400/3907, Loss: 0.385938\n",
      "[2025-05-13 10:46:23,241][training][INFO] - Epoch 89, Batch 2500/3907, Loss: 0.376934\n",
      "[2025-05-13 10:46:23,630][training][INFO] - Epoch 89, Batch 2600/3907, Loss: 0.374864\n",
      "[2025-05-13 10:46:24,017][training][INFO] - Epoch 89, Batch 2700/3907, Loss: 0.382840\n",
      "[2025-05-13 10:46:24,404][training][INFO] - Epoch 89, Batch 2800/3907, Loss: 0.378247\n",
      "[2025-05-13 10:46:24,791][training][INFO] - Epoch 89, Batch 2900/3907, Loss: 0.378659\n",
      "[2025-05-13 10:46:25,176][training][INFO] - Epoch 89, Batch 3000/3907, Loss: 0.382748\n",
      "[2025-05-13 10:46:25,561][training][INFO] - Epoch 89, Batch 3100/3907, Loss: 0.388264\n",
      "[2025-05-13 10:46:25,948][training][INFO] - Epoch 89, Batch 3200/3907, Loss: 0.364314\n",
      "[2025-05-13 10:46:26,333][training][INFO] - Epoch 89, Batch 3300/3907, Loss: 0.376460\n",
      "[2025-05-13 10:46:26,718][training][INFO] - Epoch 89, Batch 3400/3907, Loss: 0.383454\n",
      "[2025-05-13 10:46:27,105][training][INFO] - Epoch 89, Batch 3500/3907, Loss: 0.370495\n",
      "[2025-05-13 10:46:27,495][training][INFO] - Epoch 89, Batch 3600/3907, Loss: 0.383520\n",
      "[2025-05-13 10:46:27,882][training][INFO] - Epoch 89, Batch 3700/3907, Loss: 0.384709\n",
      "[2025-05-13 10:46:28,272][training][INFO] - Epoch 89, Batch 3800/3907, Loss: 0.377693\n",
      "[2025-05-13 10:46:28,676][training][INFO] - Epoch 89, Batch 3900/3907, Loss: 0.374422\n",
      "[2025-05-13 10:46:28,693][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:46:28,693][training][INFO] - Epoch 89 complete. Avg Loss: 0.377565\n",
      "[2025-05-13 10:46:28,737][training][INFO] - Epoch 90, Batch 0/3907, Loss: 0.386059\n",
      "[2025-05-13 10:46:29,132][training][INFO] - Epoch 90, Batch 100/3907, Loss: 0.374678\n",
      "[2025-05-13 10:46:29,527][training][INFO] - Epoch 90, Batch 200/3907, Loss: 0.379839\n",
      "[2025-05-13 10:46:29,922][training][INFO] - Epoch 90, Batch 300/3907, Loss: 0.385859\n",
      "[2025-05-13 10:46:30,316][training][INFO] - Epoch 90, Batch 400/3907, Loss: 0.382475\n",
      "[2025-05-13 10:46:30,709][training][INFO] - Epoch 90, Batch 500/3907, Loss: 0.382138\n",
      "[2025-05-13 10:46:31,101][training][INFO] - Epoch 90, Batch 600/3907, Loss: 0.378117\n",
      "[2025-05-13 10:46:31,494][training][INFO] - Epoch 90, Batch 700/3907, Loss: 0.374380\n",
      "[2025-05-13 10:46:31,890][training][INFO] - Epoch 90, Batch 800/3907, Loss: 0.369120\n",
      "[2025-05-13 10:46:32,281][training][INFO] - Epoch 90, Batch 900/3907, Loss: 0.375271\n",
      "[2025-05-13 10:46:32,670][training][INFO] - Epoch 90, Batch 1000/3907, Loss: 0.377629\n",
      "[2025-05-13 10:46:33,059][training][INFO] - Epoch 90, Batch 1100/3907, Loss: 0.382039\n",
      "[2025-05-13 10:46:33,449][training][INFO] - Epoch 90, Batch 1200/3907, Loss: 0.376019\n",
      "[2025-05-13 10:46:33,841][training][INFO] - Epoch 90, Batch 1300/3907, Loss: 0.374160\n",
      "[2025-05-13 10:46:34,234][training][INFO] - Epoch 90, Batch 1400/3907, Loss: 0.370384\n",
      "[2025-05-13 10:46:34,623][training][INFO] - Epoch 90, Batch 1500/3907, Loss: 0.374381\n",
      "[2025-05-13 10:46:35,015][training][INFO] - Epoch 90, Batch 1600/3907, Loss: 0.380856\n",
      "[2025-05-13 10:46:35,408][training][INFO] - Epoch 90, Batch 1700/3907, Loss: 0.389258\n",
      "[2025-05-13 10:46:35,803][training][INFO] - Epoch 90, Batch 1800/3907, Loss: 0.378841\n",
      "[2025-05-13 10:46:36,194][training][INFO] - Epoch 90, Batch 1900/3907, Loss: 0.377628\n",
      "[2025-05-13 10:46:36,581][training][INFO] - Epoch 90, Batch 2000/3907, Loss: 0.376499\n",
      "[2025-05-13 10:46:36,974][training][INFO] - Epoch 90, Batch 2100/3907, Loss: 0.375711\n",
      "[2025-05-13 10:46:37,360][training][INFO] - Epoch 90, Batch 2200/3907, Loss: 0.385687\n",
      "[2025-05-13 10:46:37,750][training][INFO] - Epoch 90, Batch 2300/3907, Loss: 0.379199\n",
      "[2025-05-13 10:46:38,145][training][INFO] - Epoch 90, Batch 2400/3907, Loss: 0.381630\n",
      "[2025-05-13 10:46:38,532][training][INFO] - Epoch 90, Batch 2500/3907, Loss: 0.380928\n",
      "[2025-05-13 10:46:38,921][training][INFO] - Epoch 90, Batch 2600/3907, Loss: 0.382750\n",
      "[2025-05-13 10:46:39,307][training][INFO] - Epoch 90, Batch 2700/3907, Loss: 0.383683\n",
      "[2025-05-13 10:46:39,695][training][INFO] - Epoch 90, Batch 2800/3907, Loss: 0.384241\n",
      "[2025-05-13 10:46:40,084][training][INFO] - Epoch 90, Batch 2900/3907, Loss: 0.377417\n",
      "[2025-05-13 10:46:40,473][training][INFO] - Epoch 90, Batch 3000/3907, Loss: 0.380593\n",
      "[2025-05-13 10:46:40,859][training][INFO] - Epoch 90, Batch 3100/3907, Loss: 0.388586\n",
      "[2025-05-13 10:46:41,249][training][INFO] - Epoch 90, Batch 3200/3907, Loss: 0.380869\n",
      "[2025-05-13 10:46:41,639][training][INFO] - Epoch 90, Batch 3300/3907, Loss: 0.375268\n",
      "[2025-05-13 10:46:42,027][training][INFO] - Epoch 90, Batch 3400/3907, Loss: 0.372069\n",
      "[2025-05-13 10:46:42,412][training][INFO] - Epoch 90, Batch 3500/3907, Loss: 0.372068\n",
      "[2025-05-13 10:46:42,798][training][INFO] - Epoch 90, Batch 3600/3907, Loss: 0.370324\n",
      "[2025-05-13 10:46:43,184][training][INFO] - Epoch 90, Batch 3700/3907, Loss: 0.378828\n",
      "[2025-05-13 10:46:43,575][training][INFO] - Epoch 90, Batch 3800/3907, Loss: 0.372697\n",
      "[2025-05-13 10:46:43,980][training][INFO] - Epoch 90, Batch 3900/3907, Loss: 0.376316\n",
      "[2025-05-13 10:46:43,997][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:46:43,997][training][INFO] - Epoch 90 complete. Avg Loss: 0.377522\n",
      "[2025-05-13 10:46:44,026][training][INFO] - Epoch 91, Batch 0/3907, Loss: 0.378967\n",
      "[2025-05-13 10:46:44,425][training][INFO] - Epoch 91, Batch 100/3907, Loss: 0.373782\n",
      "[2025-05-13 10:46:44,813][training][INFO] - Epoch 91, Batch 200/3907, Loss: 0.373796\n",
      "[2025-05-13 10:46:45,203][training][INFO] - Epoch 91, Batch 300/3907, Loss: 0.377037\n",
      "[2025-05-13 10:46:45,592][training][INFO] - Epoch 91, Batch 400/3907, Loss: 0.373388\n",
      "[2025-05-13 10:46:45,981][training][INFO] - Epoch 91, Batch 500/3907, Loss: 0.374059\n",
      "[2025-05-13 10:46:46,371][training][INFO] - Epoch 91, Batch 600/3907, Loss: 0.373238\n",
      "[2025-05-13 10:46:46,755][training][INFO] - Epoch 91, Batch 700/3907, Loss: 0.377512\n",
      "[2025-05-13 10:46:47,143][training][INFO] - Epoch 91, Batch 800/3907, Loss: 0.377864\n",
      "[2025-05-13 10:46:47,530][training][INFO] - Epoch 91, Batch 900/3907, Loss: 0.373725\n",
      "[2025-05-13 10:46:47,915][training][INFO] - Epoch 91, Batch 1000/3907, Loss: 0.374833\n",
      "[2025-05-13 10:46:48,300][training][INFO] - Epoch 91, Batch 1100/3907, Loss: 0.380599\n",
      "[2025-05-13 10:46:48,686][training][INFO] - Epoch 91, Batch 1200/3907, Loss: 0.371571\n",
      "[2025-05-13 10:46:49,076][training][INFO] - Epoch 91, Batch 1300/3907, Loss: 0.385590\n",
      "[2025-05-13 10:46:49,462][training][INFO] - Epoch 91, Batch 1400/3907, Loss: 0.378388\n",
      "[2025-05-13 10:46:49,849][training][INFO] - Epoch 91, Batch 1500/3907, Loss: 0.371291\n",
      "[2025-05-13 10:46:50,240][training][INFO] - Epoch 91, Batch 1600/3907, Loss: 0.375236\n",
      "[2025-05-13 10:46:50,625][training][INFO] - Epoch 91, Batch 1700/3907, Loss: 0.375887\n",
      "[2025-05-13 10:46:51,013][training][INFO] - Epoch 91, Batch 1800/3907, Loss: 0.377172\n",
      "[2025-05-13 10:46:51,403][training][INFO] - Epoch 91, Batch 1900/3907, Loss: 0.383840\n",
      "[2025-05-13 10:46:51,788][training][INFO] - Epoch 91, Batch 2000/3907, Loss: 0.376360\n",
      "[2025-05-13 10:46:52,176][training][INFO] - Epoch 91, Batch 2100/3907, Loss: 0.381548\n",
      "[2025-05-13 10:46:52,562][training][INFO] - Epoch 91, Batch 2200/3907, Loss: 0.369341\n",
      "[2025-05-13 10:46:52,950][training][INFO] - Epoch 91, Batch 2300/3907, Loss: 0.382813\n",
      "[2025-05-13 10:46:53,335][training][INFO] - Epoch 91, Batch 2400/3907, Loss: 0.379508\n",
      "[2025-05-13 10:46:53,722][training][INFO] - Epoch 91, Batch 2500/3907, Loss: 0.388146\n",
      "[2025-05-13 10:46:54,108][training][INFO] - Epoch 91, Batch 2600/3907, Loss: 0.382059\n",
      "[2025-05-13 10:46:54,493][training][INFO] - Epoch 91, Batch 2700/3907, Loss: 0.375223\n",
      "[2025-05-13 10:46:54,878][training][INFO] - Epoch 91, Batch 2800/3907, Loss: 0.384548\n",
      "[2025-05-13 10:46:55,265][training][INFO] - Epoch 91, Batch 2900/3907, Loss: 0.373475\n",
      "[2025-05-13 10:46:55,652][training][INFO] - Epoch 91, Batch 3000/3907, Loss: 0.379149\n",
      "[2025-05-13 10:46:56,037][training][INFO] - Epoch 91, Batch 3100/3907, Loss: 0.383740\n",
      "[2025-05-13 10:46:56,422][training][INFO] - Epoch 91, Batch 3200/3907, Loss: 0.381625\n",
      "[2025-05-13 10:46:56,813][training][INFO] - Epoch 91, Batch 3300/3907, Loss: 0.378065\n",
      "[2025-05-13 10:46:57,202][training][INFO] - Epoch 91, Batch 3400/3907, Loss: 0.372357\n",
      "[2025-05-13 10:46:57,590][training][INFO] - Epoch 91, Batch 3500/3907, Loss: 0.374365\n",
      "[2025-05-13 10:46:57,976][training][INFO] - Epoch 91, Batch 3600/3907, Loss: 0.379323\n",
      "[2025-05-13 10:46:58,363][training][INFO] - Epoch 91, Batch 3700/3907, Loss: 0.384981\n",
      "[2025-05-13 10:46:58,751][training][INFO] - Epoch 91, Batch 3800/3907, Loss: 0.369708\n",
      "[2025-05-13 10:46:59,158][training][INFO] - Epoch 91, Batch 3900/3907, Loss: 0.373624\n",
      "[2025-05-13 10:46:59,175][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:46:59,175][training][INFO] - Epoch 91 complete. Avg Loss: 0.377427\n",
      "[2025-05-13 10:46:59,217][training][INFO] - Epoch 92, Batch 0/3907, Loss: 0.380349\n",
      "[2025-05-13 10:46:59,609][training][INFO] - Epoch 92, Batch 100/3907, Loss: 0.380954\n",
      "[2025-05-13 10:47:00,003][training][INFO] - Epoch 92, Batch 200/3907, Loss: 0.363022\n",
      "[2025-05-13 10:47:00,405][training][INFO] - Epoch 92, Batch 300/3907, Loss: 0.382639\n",
      "[2025-05-13 10:47:00,804][training][INFO] - Epoch 92, Batch 400/3907, Loss: 0.377532\n",
      "[2025-05-13 10:47:01,197][training][INFO] - Epoch 92, Batch 500/3907, Loss: 0.375334\n",
      "[2025-05-13 10:47:01,586][training][INFO] - Epoch 92, Batch 600/3907, Loss: 0.377340\n",
      "[2025-05-13 10:47:01,973][training][INFO] - Epoch 92, Batch 700/3907, Loss: 0.379860\n",
      "[2025-05-13 10:47:02,359][training][INFO] - Epoch 92, Batch 800/3907, Loss: 0.379379\n",
      "[2025-05-13 10:47:02,746][training][INFO] - Epoch 92, Batch 900/3907, Loss: 0.379876\n",
      "[2025-05-13 10:47:03,134][training][INFO] - Epoch 92, Batch 1000/3907, Loss: 0.385091\n",
      "[2025-05-13 10:47:03,526][training][INFO] - Epoch 92, Batch 1100/3907, Loss: 0.378014\n",
      "[2025-05-13 10:47:03,922][training][INFO] - Epoch 92, Batch 1200/3907, Loss: 0.382762\n",
      "[2025-05-13 10:47:04,311][training][INFO] - Epoch 92, Batch 1300/3907, Loss: 0.376892\n",
      "[2025-05-13 10:47:04,701][training][INFO] - Epoch 92, Batch 1400/3907, Loss: 0.373239\n",
      "[2025-05-13 10:47:05,091][training][INFO] - Epoch 92, Batch 1500/3907, Loss: 0.384470\n",
      "[2025-05-13 10:47:05,479][training][INFO] - Epoch 92, Batch 1600/3907, Loss: 0.370904\n",
      "[2025-05-13 10:47:05,867][training][INFO] - Epoch 92, Batch 1700/3907, Loss: 0.383590\n",
      "[2025-05-13 10:47:06,257][training][INFO] - Epoch 92, Batch 1800/3907, Loss: 0.378124\n",
      "[2025-05-13 10:47:06,650][training][INFO] - Epoch 92, Batch 1900/3907, Loss: 0.373655\n",
      "[2025-05-13 10:47:07,043][training][INFO] - Epoch 92, Batch 2000/3907, Loss: 0.370864\n",
      "[2025-05-13 10:47:07,430][training][INFO] - Epoch 92, Batch 2100/3907, Loss: 0.374263\n",
      "[2025-05-13 10:47:07,817][training][INFO] - Epoch 92, Batch 2200/3907, Loss: 0.382990\n",
      "[2025-05-13 10:47:08,205][training][INFO] - Epoch 92, Batch 2300/3907, Loss: 0.376455\n",
      "[2025-05-13 10:47:08,596][training][INFO] - Epoch 92, Batch 2400/3907, Loss: 0.376386\n",
      "[2025-05-13 10:47:08,986][training][INFO] - Epoch 92, Batch 2500/3907, Loss: 0.379661\n",
      "[2025-05-13 10:47:09,372][training][INFO] - Epoch 92, Batch 2600/3907, Loss: 0.371294\n",
      "[2025-05-13 10:47:09,765][training][INFO] - Epoch 92, Batch 2700/3907, Loss: 0.362452\n",
      "[2025-05-13 10:47:10,152][training][INFO] - Epoch 92, Batch 2800/3907, Loss: 0.370991\n",
      "[2025-05-13 10:47:10,541][training][INFO] - Epoch 92, Batch 2900/3907, Loss: 0.384070\n",
      "[2025-05-13 10:47:10,932][training][INFO] - Epoch 92, Batch 3000/3907, Loss: 0.385157\n",
      "[2025-05-13 10:47:11,320][training][INFO] - Epoch 92, Batch 3100/3907, Loss: 0.379919\n",
      "[2025-05-13 10:47:11,708][training][INFO] - Epoch 92, Batch 3200/3907, Loss: 0.368569\n",
      "[2025-05-13 10:47:12,097][training][INFO] - Epoch 92, Batch 3300/3907, Loss: 0.374466\n",
      "[2025-05-13 10:47:12,481][training][INFO] - Epoch 92, Batch 3400/3907, Loss: 0.378758\n",
      "[2025-05-13 10:47:12,867][training][INFO] - Epoch 92, Batch 3500/3907, Loss: 0.372384\n",
      "[2025-05-13 10:47:13,257][training][INFO] - Epoch 92, Batch 3600/3907, Loss: 0.376308\n",
      "[2025-05-13 10:47:13,647][training][INFO] - Epoch 92, Batch 3700/3907, Loss: 0.367741\n",
      "[2025-05-13 10:47:14,032][training][INFO] - Epoch 92, Batch 3800/3907, Loss: 0.374659\n",
      "[2025-05-13 10:47:14,438][training][INFO] - Epoch 92, Batch 3900/3907, Loss: 0.384458\n",
      "[2025-05-13 10:47:14,455][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:47:14,455][training][INFO] - Epoch 92 complete. Avg Loss: 0.377370\n",
      "[2025-05-13 10:47:14,499][training][INFO] - Epoch 93, Batch 0/3907, Loss: 0.374523\n",
      "[2025-05-13 10:47:14,892][training][INFO] - Epoch 93, Batch 100/3907, Loss: 0.371331\n",
      "[2025-05-13 10:47:15,282][training][INFO] - Epoch 93, Batch 200/3907, Loss: 0.377627\n",
      "[2025-05-13 10:47:15,673][training][INFO] - Epoch 93, Batch 300/3907, Loss: 0.382381\n",
      "[2025-05-13 10:47:16,062][training][INFO] - Epoch 93, Batch 400/3907, Loss: 0.381036\n",
      "[2025-05-13 10:47:16,447][training][INFO] - Epoch 93, Batch 500/3907, Loss: 0.378573\n",
      "[2025-05-13 10:47:16,837][training][INFO] - Epoch 93, Batch 600/3907, Loss: 0.375183\n",
      "[2025-05-13 10:47:17,227][training][INFO] - Epoch 93, Batch 700/3907, Loss: 0.372525\n",
      "[2025-05-13 10:47:17,614][training][INFO] - Epoch 93, Batch 800/3907, Loss: 0.384209\n",
      "[2025-05-13 10:47:18,000][training][INFO] - Epoch 93, Batch 900/3907, Loss: 0.387274\n",
      "[2025-05-13 10:47:18,387][training][INFO] - Epoch 93, Batch 1000/3907, Loss: 0.377680\n",
      "[2025-05-13 10:47:18,777][training][INFO] - Epoch 93, Batch 1100/3907, Loss: 0.385731\n",
      "[2025-05-13 10:47:19,178][training][INFO] - Epoch 93, Batch 1200/3907, Loss: 0.372188\n",
      "[2025-05-13 10:47:19,568][training][INFO] - Epoch 93, Batch 1300/3907, Loss: 0.376634\n",
      "[2025-05-13 10:47:19,957][training][INFO] - Epoch 93, Batch 1400/3907, Loss: 0.386249\n",
      "[2025-05-13 10:47:20,350][training][INFO] - Epoch 93, Batch 1500/3907, Loss: 0.380954\n",
      "[2025-05-13 10:47:20,743][training][INFO] - Epoch 93, Batch 1600/3907, Loss: 0.387635\n",
      "[2025-05-13 10:47:21,132][training][INFO] - Epoch 93, Batch 1700/3907, Loss: 0.376586\n",
      "[2025-05-13 10:47:21,520][training][INFO] - Epoch 93, Batch 1800/3907, Loss: 0.387630\n",
      "[2025-05-13 10:47:21,908][training][INFO] - Epoch 93, Batch 1900/3907, Loss: 0.377059\n",
      "[2025-05-13 10:47:22,293][training][INFO] - Epoch 93, Batch 2000/3907, Loss: 0.379608\n",
      "[2025-05-13 10:47:22,683][training][INFO] - Epoch 93, Batch 2100/3907, Loss: 0.366651\n",
      "[2025-05-13 10:47:23,076][training][INFO] - Epoch 93, Batch 2200/3907, Loss: 0.372505\n",
      "[2025-05-13 10:47:23,471][training][INFO] - Epoch 93, Batch 2300/3907, Loss: 0.376295\n",
      "[2025-05-13 10:47:23,866][training][INFO] - Epoch 93, Batch 2400/3907, Loss: 0.376919\n",
      "[2025-05-13 10:47:24,260][training][INFO] - Epoch 93, Batch 2500/3907, Loss: 0.365160\n",
      "[2025-05-13 10:47:24,656][training][INFO] - Epoch 93, Batch 2600/3907, Loss: 0.374930\n",
      "[2025-05-13 10:47:25,050][training][INFO] - Epoch 93, Batch 2700/3907, Loss: 0.370202\n",
      "[2025-05-13 10:47:25,437][training][INFO] - Epoch 93, Batch 2800/3907, Loss: 0.376313\n",
      "[2025-05-13 10:47:25,828][training][INFO] - Epoch 93, Batch 2900/3907, Loss: 0.374020\n",
      "[2025-05-13 10:47:26,217][training][INFO] - Epoch 93, Batch 3000/3907, Loss: 0.377414\n",
      "[2025-05-13 10:47:26,608][training][INFO] - Epoch 93, Batch 3100/3907, Loss: 0.370552\n",
      "[2025-05-13 10:47:27,006][training][INFO] - Epoch 93, Batch 3200/3907, Loss: 0.377134\n",
      "[2025-05-13 10:47:27,392][training][INFO] - Epoch 93, Batch 3300/3907, Loss: 0.370747\n",
      "[2025-05-13 10:47:27,782][training][INFO] - Epoch 93, Batch 3400/3907, Loss: 0.373521\n",
      "[2025-05-13 10:47:28,168][training][INFO] - Epoch 93, Batch 3500/3907, Loss: 0.369339\n",
      "[2025-05-13 10:47:28,557][training][INFO] - Epoch 93, Batch 3600/3907, Loss: 0.376763\n",
      "[2025-05-13 10:47:28,946][training][INFO] - Epoch 93, Batch 3700/3907, Loss: 0.370512\n",
      "[2025-05-13 10:47:29,332][training][INFO] - Epoch 93, Batch 3800/3907, Loss: 0.372479\n",
      "[2025-05-13 10:47:29,744][training][INFO] - Epoch 93, Batch 3900/3907, Loss: 0.376035\n",
      "[2025-05-13 10:47:29,762][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:47:29,762][training][INFO] - Epoch 93 complete. Avg Loss: 0.377178\n",
      "[2025-05-13 10:47:29,806][training][INFO] - Epoch 94, Batch 0/3907, Loss: 0.384159\n",
      "[2025-05-13 10:47:30,196][training][INFO] - Epoch 94, Batch 100/3907, Loss: 0.378460\n",
      "[2025-05-13 10:47:30,580][training][INFO] - Epoch 94, Batch 200/3907, Loss: 0.375950\n",
      "[2025-05-13 10:47:30,974][training][INFO] - Epoch 94, Batch 300/3907, Loss: 0.365145\n",
      "[2025-05-13 10:47:31,360][training][INFO] - Epoch 94, Batch 400/3907, Loss: 0.386134\n",
      "[2025-05-13 10:47:31,746][training][INFO] - Epoch 94, Batch 500/3907, Loss: 0.358283\n",
      "[2025-05-13 10:47:32,129][training][INFO] - Epoch 94, Batch 600/3907, Loss: 0.379551\n",
      "[2025-05-13 10:47:32,514][training][INFO] - Epoch 94, Batch 700/3907, Loss: 0.378447\n",
      "[2025-05-13 10:47:32,902][training][INFO] - Epoch 94, Batch 800/3907, Loss: 0.372262\n",
      "[2025-05-13 10:47:33,288][training][INFO] - Epoch 94, Batch 900/3907, Loss: 0.369973\n",
      "[2025-05-13 10:47:33,673][training][INFO] - Epoch 94, Batch 1000/3907, Loss: 0.376595\n",
      "[2025-05-13 10:47:34,058][training][INFO] - Epoch 94, Batch 1100/3907, Loss: 0.383664\n",
      "[2025-05-13 10:47:34,444][training][INFO] - Epoch 94, Batch 1200/3907, Loss: 0.377358\n",
      "[2025-05-13 10:47:34,829][training][INFO] - Epoch 94, Batch 1300/3907, Loss: 0.371176\n",
      "[2025-05-13 10:47:35,217][training][INFO] - Epoch 94, Batch 1400/3907, Loss: 0.383841\n",
      "[2025-05-13 10:47:35,602][training][INFO] - Epoch 94, Batch 1500/3907, Loss: 0.375142\n",
      "[2025-05-13 10:47:35,987][training][INFO] - Epoch 94, Batch 1600/3907, Loss: 0.387483\n",
      "[2025-05-13 10:47:36,373][training][INFO] - Epoch 94, Batch 1700/3907, Loss: 0.368153\n",
      "[2025-05-13 10:47:36,764][training][INFO] - Epoch 94, Batch 1800/3907, Loss: 0.375148\n",
      "[2025-05-13 10:47:37,151][training][INFO] - Epoch 94, Batch 1900/3907, Loss: 0.367685\n",
      "[2025-05-13 10:47:37,538][training][INFO] - Epoch 94, Batch 2000/3907, Loss: 0.375818\n",
      "[2025-05-13 10:47:37,923][training][INFO] - Epoch 94, Batch 2100/3907, Loss: 0.369708\n",
      "[2025-05-13 10:47:38,312][training][INFO] - Epoch 94, Batch 2200/3907, Loss: 0.378274\n",
      "[2025-05-13 10:47:38,699][training][INFO] - Epoch 94, Batch 2300/3907, Loss: 0.367689\n",
      "[2025-05-13 10:47:39,082][training][INFO] - Epoch 94, Batch 2400/3907, Loss: 0.379142\n",
      "[2025-05-13 10:47:39,468][training][INFO] - Epoch 94, Batch 2500/3907, Loss: 0.376384\n",
      "[2025-05-13 10:47:39,851][training][INFO] - Epoch 94, Batch 2600/3907, Loss: 0.382880\n",
      "[2025-05-13 10:47:40,237][training][INFO] - Epoch 94, Batch 2700/3907, Loss: 0.382112\n",
      "[2025-05-13 10:47:40,621][training][INFO] - Epoch 94, Batch 2800/3907, Loss: 0.374197\n",
      "[2025-05-13 10:47:41,008][training][INFO] - Epoch 94, Batch 2900/3907, Loss: 0.374637\n",
      "[2025-05-13 10:47:41,396][training][INFO] - Epoch 94, Batch 3000/3907, Loss: 0.376615\n",
      "[2025-05-13 10:47:41,783][training][INFO] - Epoch 94, Batch 3100/3907, Loss: 0.377367\n",
      "[2025-05-13 10:47:42,167][training][INFO] - Epoch 94, Batch 3200/3907, Loss: 0.372845\n",
      "[2025-05-13 10:47:42,551][training][INFO] - Epoch 94, Batch 3300/3907, Loss: 0.387718\n",
      "[2025-05-13 10:47:42,938][training][INFO] - Epoch 94, Batch 3400/3907, Loss: 0.375444\n",
      "[2025-05-13 10:47:43,325][training][INFO] - Epoch 94, Batch 3500/3907, Loss: 0.380586\n",
      "[2025-05-13 10:47:43,709][training][INFO] - Epoch 94, Batch 3600/3907, Loss: 0.372896\n",
      "[2025-05-13 10:47:44,090][training][INFO] - Epoch 94, Batch 3700/3907, Loss: 0.377607\n",
      "[2025-05-13 10:47:44,475][training][INFO] - Epoch 94, Batch 3800/3907, Loss: 0.371453\n",
      "[2025-05-13 10:47:44,882][training][INFO] - Epoch 94, Batch 3900/3907, Loss: 0.383268\n",
      "[2025-05-13 10:47:44,899][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:47:44,899][training][INFO] - Epoch 94 complete. Avg Loss: 0.377070\n",
      "[2025-05-13 10:47:44,943][training][INFO] - Epoch 95, Batch 0/3907, Loss: 0.381877\n",
      "[2025-05-13 10:47:45,331][training][INFO] - Epoch 95, Batch 100/3907, Loss: 0.382878\n",
      "[2025-05-13 10:47:45,722][training][INFO] - Epoch 95, Batch 200/3907, Loss: 0.367846\n",
      "[2025-05-13 10:47:46,110][training][INFO] - Epoch 95, Batch 300/3907, Loss: 0.367855\n",
      "[2025-05-13 10:47:46,496][training][INFO] - Epoch 95, Batch 400/3907, Loss: 0.379432\n",
      "[2025-05-13 10:47:46,884][training][INFO] - Epoch 95, Batch 500/3907, Loss: 0.377464\n",
      "[2025-05-13 10:47:47,273][training][INFO] - Epoch 95, Batch 600/3907, Loss: 0.373085\n",
      "[2025-05-13 10:47:47,658][training][INFO] - Epoch 95, Batch 700/3907, Loss: 0.372246\n",
      "[2025-05-13 10:47:48,044][training][INFO] - Epoch 95, Batch 800/3907, Loss: 0.373728\n",
      "[2025-05-13 10:47:48,429][training][INFO] - Epoch 95, Batch 900/3907, Loss: 0.378243\n",
      "[2025-05-13 10:47:48,814][training][INFO] - Epoch 95, Batch 1000/3907, Loss: 0.379407\n",
      "[2025-05-13 10:47:49,209][training][INFO] - Epoch 95, Batch 1100/3907, Loss: 0.379685\n",
      "[2025-05-13 10:47:49,608][training][INFO] - Epoch 95, Batch 1200/3907, Loss: 0.375076\n",
      "[2025-05-13 10:47:49,997][training][INFO] - Epoch 95, Batch 1300/3907, Loss: 0.381066\n",
      "[2025-05-13 10:47:50,381][training][INFO] - Epoch 95, Batch 1400/3907, Loss: 0.381902\n",
      "[2025-05-13 10:47:50,770][training][INFO] - Epoch 95, Batch 1500/3907, Loss: 0.366342\n",
      "[2025-05-13 10:47:51,159][training][INFO] - Epoch 95, Batch 1600/3907, Loss: 0.378210\n",
      "[2025-05-13 10:47:51,545][training][INFO] - Epoch 95, Batch 1700/3907, Loss: 0.369862\n",
      "[2025-05-13 10:47:51,932][training][INFO] - Epoch 95, Batch 1800/3907, Loss: 0.376739\n",
      "[2025-05-13 10:47:52,317][training][INFO] - Epoch 95, Batch 1900/3907, Loss: 0.382776\n",
      "[2025-05-13 10:47:52,701][training][INFO] - Epoch 95, Batch 2000/3907, Loss: 0.387391\n",
      "[2025-05-13 10:47:53,084][training][INFO] - Epoch 95, Batch 2100/3907, Loss: 0.383878\n",
      "[2025-05-13 10:47:53,469][training][INFO] - Epoch 95, Batch 2200/3907, Loss: 0.381155\n",
      "[2025-05-13 10:47:53,857][training][INFO] - Epoch 95, Batch 2300/3907, Loss: 0.371073\n",
      "[2025-05-13 10:47:54,242][training][INFO] - Epoch 95, Batch 2400/3907, Loss: 0.373390\n",
      "[2025-05-13 10:47:54,630][training][INFO] - Epoch 95, Batch 2500/3907, Loss: 0.366824\n",
      "[2025-05-13 10:47:55,017][training][INFO] - Epoch 95, Batch 2600/3907, Loss: 0.385900\n",
      "[2025-05-13 10:47:55,407][training][INFO] - Epoch 95, Batch 2700/3907, Loss: 0.377598\n",
      "[2025-05-13 10:47:55,792][training][INFO] - Epoch 95, Batch 2800/3907, Loss: 0.377178\n",
      "[2025-05-13 10:47:56,179][training][INFO] - Epoch 95, Batch 2900/3907, Loss: 0.367137\n",
      "[2025-05-13 10:47:56,567][training][INFO] - Epoch 95, Batch 3000/3907, Loss: 0.376781\n",
      "[2025-05-13 10:47:56,954][training][INFO] - Epoch 95, Batch 3100/3907, Loss: 0.375779\n",
      "[2025-05-13 10:47:57,348][training][INFO] - Epoch 95, Batch 3200/3907, Loss: 0.362731\n",
      "[2025-05-13 10:47:57,739][training][INFO] - Epoch 95, Batch 3300/3907, Loss: 0.378574\n",
      "[2025-05-13 10:47:58,129][training][INFO] - Epoch 95, Batch 3400/3907, Loss: 0.370391\n",
      "[2025-05-13 10:47:58,522][training][INFO] - Epoch 95, Batch 3500/3907, Loss: 0.376647\n",
      "[2025-05-13 10:47:58,909][training][INFO] - Epoch 95, Batch 3600/3907, Loss: 0.373040\n",
      "[2025-05-13 10:47:59,300][training][INFO] - Epoch 95, Batch 3700/3907, Loss: 0.383346\n",
      "[2025-05-13 10:47:59,689][training][INFO] - Epoch 95, Batch 3800/3907, Loss: 0.381115\n",
      "[2025-05-13 10:48:00,098][training][INFO] - Epoch 95, Batch 3900/3907, Loss: 0.379711\n",
      "[2025-05-13 10:48:00,115][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:48:00,115][training][INFO] - Epoch 95 complete. Avg Loss: 0.376974\n",
      "[2025-05-13 10:48:00,154][training][INFO] - Epoch 96, Batch 0/3907, Loss: 0.380495\n",
      "[2025-05-13 10:48:00,547][training][INFO] - Epoch 96, Batch 100/3907, Loss: 0.374160\n",
      "[2025-05-13 10:48:00,939][training][INFO] - Epoch 96, Batch 200/3907, Loss: 0.377291\n",
      "[2025-05-13 10:48:01,326][training][INFO] - Epoch 96, Batch 300/3907, Loss: 0.384108\n",
      "[2025-05-13 10:48:01,719][training][INFO] - Epoch 96, Batch 400/3907, Loss: 0.376049\n",
      "[2025-05-13 10:48:02,106][training][INFO] - Epoch 96, Batch 500/3907, Loss: 0.372348\n",
      "[2025-05-13 10:48:02,494][training][INFO] - Epoch 96, Batch 600/3907, Loss: 0.369733\n",
      "[2025-05-13 10:48:02,879][training][INFO] - Epoch 96, Batch 700/3907, Loss: 0.374569\n",
      "[2025-05-13 10:48:03,266][training][INFO] - Epoch 96, Batch 800/3907, Loss: 0.374916\n",
      "[2025-05-13 10:48:03,654][training][INFO] - Epoch 96, Batch 900/3907, Loss: 0.382888\n",
      "[2025-05-13 10:48:04,041][training][INFO] - Epoch 96, Batch 1000/3907, Loss: 0.376682\n",
      "[2025-05-13 10:48:04,426][training][INFO] - Epoch 96, Batch 1100/3907, Loss: 0.376422\n",
      "[2025-05-13 10:48:04,813][training][INFO] - Epoch 96, Batch 1200/3907, Loss: 0.377953\n",
      "[2025-05-13 10:48:05,197][training][INFO] - Epoch 96, Batch 1300/3907, Loss: 0.377413\n",
      "[2025-05-13 10:48:05,583][training][INFO] - Epoch 96, Batch 1400/3907, Loss: 0.381526\n",
      "[2025-05-13 10:48:05,971][training][INFO] - Epoch 96, Batch 1500/3907, Loss: 0.377615\n",
      "[2025-05-13 10:48:06,356][training][INFO] - Epoch 96, Batch 1600/3907, Loss: 0.382360\n",
      "[2025-05-13 10:48:06,744][training][INFO] - Epoch 96, Batch 1700/3907, Loss: 0.383444\n",
      "[2025-05-13 10:48:07,131][training][INFO] - Epoch 96, Batch 1800/3907, Loss: 0.374790\n",
      "[2025-05-13 10:48:07,518][training][INFO] - Epoch 96, Batch 1900/3907, Loss: 0.378318\n",
      "[2025-05-13 10:48:07,903][training][INFO] - Epoch 96, Batch 2000/3907, Loss: 0.374793\n",
      "[2025-05-13 10:48:08,288][training][INFO] - Epoch 96, Batch 2100/3907, Loss: 0.375870\n",
      "[2025-05-13 10:48:08,677][training][INFO] - Epoch 96, Batch 2200/3907, Loss: 0.377509\n",
      "[2025-05-13 10:48:09,061][training][INFO] - Epoch 96, Batch 2300/3907, Loss: 0.374352\n",
      "[2025-05-13 10:48:09,448][training][INFO] - Epoch 96, Batch 2400/3907, Loss: 0.384492\n",
      "[2025-05-13 10:48:09,837][training][INFO] - Epoch 96, Batch 2500/3907, Loss: 0.375063\n",
      "[2025-05-13 10:48:10,222][training][INFO] - Epoch 96, Batch 2600/3907, Loss: 0.378533\n",
      "[2025-05-13 10:48:10,608][training][INFO] - Epoch 96, Batch 2700/3907, Loss: 0.370981\n",
      "[2025-05-13 10:48:10,993][training][INFO] - Epoch 96, Batch 2800/3907, Loss: 0.370817\n",
      "[2025-05-13 10:48:11,378][training][INFO] - Epoch 96, Batch 2900/3907, Loss: 0.371580\n",
      "[2025-05-13 10:48:11,764][training][INFO] - Epoch 96, Batch 3000/3907, Loss: 0.371947\n",
      "[2025-05-13 10:48:12,151][training][INFO] - Epoch 96, Batch 3100/3907, Loss: 0.371919\n",
      "[2025-05-13 10:48:12,538][training][INFO] - Epoch 96, Batch 3200/3907, Loss: 0.367704\n",
      "[2025-05-13 10:48:12,926][training][INFO] - Epoch 96, Batch 3300/3907, Loss: 0.376166\n",
      "[2025-05-13 10:48:13,310][training][INFO] - Epoch 96, Batch 3400/3907, Loss: 0.377595\n",
      "[2025-05-13 10:48:13,697][training][INFO] - Epoch 96, Batch 3500/3907, Loss: 0.375883\n",
      "[2025-05-13 10:48:14,083][training][INFO] - Epoch 96, Batch 3600/3907, Loss: 0.383002\n",
      "[2025-05-13 10:48:14,469][training][INFO] - Epoch 96, Batch 3700/3907, Loss: 0.370195\n",
      "[2025-05-13 10:48:14,855][training][INFO] - Epoch 96, Batch 3800/3907, Loss: 0.377842\n",
      "[2025-05-13 10:48:15,262][training][INFO] - Epoch 96, Batch 3900/3907, Loss: 0.376389\n",
      "[2025-05-13 10:48:15,279][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:48:15,279][training][INFO] - Epoch 96 complete. Avg Loss: 0.376903\n",
      "[2025-05-13 10:48:15,323][training][INFO] - Epoch 97, Batch 0/3907, Loss: 0.378353\n",
      "[2025-05-13 10:48:15,716][training][INFO] - Epoch 97, Batch 100/3907, Loss: 0.381082\n",
      "[2025-05-13 10:48:16,105][training][INFO] - Epoch 97, Batch 200/3907, Loss: 0.370082\n",
      "[2025-05-13 10:48:16,494][training][INFO] - Epoch 97, Batch 300/3907, Loss: 0.368627\n",
      "[2025-05-13 10:48:16,879][training][INFO] - Epoch 97, Batch 400/3907, Loss: 0.379162\n",
      "[2025-05-13 10:48:17,265][training][INFO] - Epoch 97, Batch 500/3907, Loss: 0.378642\n",
      "[2025-05-13 10:48:17,648][training][INFO] - Epoch 97, Batch 600/3907, Loss: 0.372040\n",
      "[2025-05-13 10:48:18,038][training][INFO] - Epoch 97, Batch 700/3907, Loss: 0.373985\n",
      "[2025-05-13 10:48:18,426][training][INFO] - Epoch 97, Batch 800/3907, Loss: 0.374241\n",
      "[2025-05-13 10:48:18,813][training][INFO] - Epoch 97, Batch 900/3907, Loss: 0.375810\n",
      "[2025-05-13 10:48:19,200][training][INFO] - Epoch 97, Batch 1000/3907, Loss: 0.373532\n",
      "[2025-05-13 10:48:19,586][training][INFO] - Epoch 97, Batch 1100/3907, Loss: 0.376918\n",
      "[2025-05-13 10:48:19,972][training][INFO] - Epoch 97, Batch 1200/3907, Loss: 0.374340\n",
      "[2025-05-13 10:48:20,361][training][INFO] - Epoch 97, Batch 1300/3907, Loss: 0.377356\n",
      "[2025-05-13 10:48:20,746][training][INFO] - Epoch 97, Batch 1400/3907, Loss: 0.382998\n",
      "[2025-05-13 10:48:21,132][training][INFO] - Epoch 97, Batch 1500/3907, Loss: 0.382668\n",
      "[2025-05-13 10:48:21,521][training][INFO] - Epoch 97, Batch 1600/3907, Loss: 0.377284\n",
      "[2025-05-13 10:48:21,911][training][INFO] - Epoch 97, Batch 1700/3907, Loss: 0.374183\n",
      "[2025-05-13 10:48:22,299][training][INFO] - Epoch 97, Batch 1800/3907, Loss: 0.371790\n",
      "[2025-05-13 10:48:22,686][training][INFO] - Epoch 97, Batch 1900/3907, Loss: 0.380739\n",
      "[2025-05-13 10:48:23,074][training][INFO] - Epoch 97, Batch 2000/3907, Loss: 0.383488\n",
      "[2025-05-13 10:48:23,462][training][INFO] - Epoch 97, Batch 2100/3907, Loss: 0.372746\n",
      "[2025-05-13 10:48:23,852][training][INFO] - Epoch 97, Batch 2200/3907, Loss: 0.372515\n",
      "[2025-05-13 10:48:24,237][training][INFO] - Epoch 97, Batch 2300/3907, Loss: 0.379526\n",
      "[2025-05-13 10:48:24,624][training][INFO] - Epoch 97, Batch 2400/3907, Loss: 0.375207\n",
      "[2025-05-13 10:48:25,007][training][INFO] - Epoch 97, Batch 2500/3907, Loss: 0.380498\n",
      "[2025-05-13 10:48:25,391][training][INFO] - Epoch 97, Batch 2600/3907, Loss: 0.375955\n",
      "[2025-05-13 10:48:25,776][training][INFO] - Epoch 97, Batch 2700/3907, Loss: 0.372148\n",
      "[2025-05-13 10:48:26,162][training][INFO] - Epoch 97, Batch 2800/3907, Loss: 0.377105\n",
      "[2025-05-13 10:48:26,549][training][INFO] - Epoch 97, Batch 2900/3907, Loss: 0.388130\n",
      "[2025-05-13 10:48:26,938][training][INFO] - Epoch 97, Batch 3000/3907, Loss: 0.379011\n",
      "[2025-05-13 10:48:27,331][training][INFO] - Epoch 97, Batch 3100/3907, Loss: 0.364124\n",
      "[2025-05-13 10:48:27,719][training][INFO] - Epoch 97, Batch 3200/3907, Loss: 0.386164\n",
      "[2025-05-13 10:48:28,110][training][INFO] - Epoch 97, Batch 3300/3907, Loss: 0.376024\n",
      "[2025-05-13 10:48:28,498][training][INFO] - Epoch 97, Batch 3400/3907, Loss: 0.371334\n",
      "[2025-05-13 10:48:28,885][training][INFO] - Epoch 97, Batch 3500/3907, Loss: 0.386112\n",
      "[2025-05-13 10:48:29,271][training][INFO] - Epoch 97, Batch 3600/3907, Loss: 0.369581\n",
      "[2025-05-13 10:48:29,660][training][INFO] - Epoch 97, Batch 3700/3907, Loss: 0.371384\n",
      "[2025-05-13 10:48:30,049][training][INFO] - Epoch 97, Batch 3800/3907, Loss: 0.375895\n",
      "[2025-05-13 10:48:30,462][training][INFO] - Epoch 97, Batch 3900/3907, Loss: 0.367380\n",
      "[2025-05-13 10:48:30,479][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:48:30,480][training][INFO] - Epoch 97 complete. Avg Loss: 0.376876\n",
      "[2025-05-13 10:48:30,513][training][INFO] - Epoch 98, Batch 0/3907, Loss: 0.370629\n",
      "[2025-05-13 10:48:30,912][training][INFO] - Epoch 98, Batch 100/3907, Loss: 0.383699\n",
      "[2025-05-13 10:48:31,303][training][INFO] - Epoch 98, Batch 200/3907, Loss: 0.372009\n",
      "[2025-05-13 10:48:31,690][training][INFO] - Epoch 98, Batch 300/3907, Loss: 0.373020\n",
      "[2025-05-13 10:48:32,075][training][INFO] - Epoch 98, Batch 400/3907, Loss: 0.374188\n",
      "[2025-05-13 10:48:32,460][training][INFO] - Epoch 98, Batch 500/3907, Loss: 0.368882\n",
      "[2025-05-13 10:48:32,848][training][INFO] - Epoch 98, Batch 600/3907, Loss: 0.377641\n",
      "[2025-05-13 10:48:33,234][training][INFO] - Epoch 98, Batch 700/3907, Loss: 0.377026\n",
      "[2025-05-13 10:48:33,621][training][INFO] - Epoch 98, Batch 800/3907, Loss: 0.378651\n",
      "[2025-05-13 10:48:34,007][training][INFO] - Epoch 98, Batch 900/3907, Loss: 0.377348\n",
      "[2025-05-13 10:48:34,392][training][INFO] - Epoch 98, Batch 1000/3907, Loss: 0.376422\n",
      "[2025-05-13 10:48:34,781][training][INFO] - Epoch 98, Batch 1100/3907, Loss: 0.382801\n",
      "[2025-05-13 10:48:35,166][training][INFO] - Epoch 98, Batch 1200/3907, Loss: 0.375273\n",
      "[2025-05-13 10:48:35,555][training][INFO] - Epoch 98, Batch 1300/3907, Loss: 0.370874\n",
      "[2025-05-13 10:48:35,943][training][INFO] - Epoch 98, Batch 1400/3907, Loss: 0.378446\n",
      "[2025-05-13 10:48:36,328][training][INFO] - Epoch 98, Batch 1500/3907, Loss: 0.378144\n",
      "[2025-05-13 10:48:36,722][training][INFO] - Epoch 98, Batch 1600/3907, Loss: 0.371184\n",
      "[2025-05-13 10:48:37,111][training][INFO] - Epoch 98, Batch 1700/3907, Loss: 0.382426\n",
      "[2025-05-13 10:48:37,495][training][INFO] - Epoch 98, Batch 1800/3907, Loss: 0.366913\n",
      "[2025-05-13 10:48:37,885][training][INFO] - Epoch 98, Batch 1900/3907, Loss: 0.378163\n",
      "[2025-05-13 10:48:38,271][training][INFO] - Epoch 98, Batch 2000/3907, Loss: 0.368536\n",
      "[2025-05-13 10:48:38,661][training][INFO] - Epoch 98, Batch 2100/3907, Loss: 0.375272\n",
      "[2025-05-13 10:48:39,050][training][INFO] - Epoch 98, Batch 2200/3907, Loss: 0.376298\n",
      "[2025-05-13 10:48:39,439][training][INFO] - Epoch 98, Batch 2300/3907, Loss: 0.367097\n",
      "[2025-05-13 10:48:39,824][training][INFO] - Epoch 98, Batch 2400/3907, Loss: 0.370832\n",
      "[2025-05-13 10:48:40,210][training][INFO] - Epoch 98, Batch 2500/3907, Loss: 0.375095\n",
      "[2025-05-13 10:48:40,596][training][INFO] - Epoch 98, Batch 2600/3907, Loss: 0.375066\n",
      "[2025-05-13 10:48:40,983][training][INFO] - Epoch 98, Batch 2700/3907, Loss: 0.377120\n",
      "[2025-05-13 10:48:41,367][training][INFO] - Epoch 98, Batch 2800/3907, Loss: 0.374305\n",
      "[2025-05-13 10:48:41,752][training][INFO] - Epoch 98, Batch 2900/3907, Loss: 0.381391\n",
      "[2025-05-13 10:48:42,137][training][INFO] - Epoch 98, Batch 3000/3907, Loss: 0.377761\n",
      "[2025-05-13 10:48:42,526][training][INFO] - Epoch 98, Batch 3100/3907, Loss: 0.375715\n",
      "[2025-05-13 10:48:42,911][training][INFO] - Epoch 98, Batch 3200/3907, Loss: 0.385891\n",
      "[2025-05-13 10:48:43,297][training][INFO] - Epoch 98, Batch 3300/3907, Loss: 0.382458\n",
      "[2025-05-13 10:48:43,683][training][INFO] - Epoch 98, Batch 3400/3907, Loss: 0.374529\n",
      "[2025-05-13 10:48:44,069][training][INFO] - Epoch 98, Batch 3500/3907, Loss: 0.385141\n",
      "[2025-05-13 10:48:44,459][training][INFO] - Epoch 98, Batch 3600/3907, Loss: 0.375694\n",
      "[2025-05-13 10:48:44,846][training][INFO] - Epoch 98, Batch 3700/3907, Loss: 0.374269\n",
      "[2025-05-13 10:48:45,233][training][INFO] - Epoch 98, Batch 3800/3907, Loss: 0.377850\n",
      "[2025-05-13 10:48:45,644][training][INFO] - Epoch 98, Batch 3900/3907, Loss: 0.372528\n",
      "[2025-05-13 10:48:45,662][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:48:45,662][training][INFO] - Epoch 98 complete. Avg Loss: 0.376745\n",
      "[2025-05-13 10:48:45,704][training][INFO] - Epoch 99, Batch 0/3907, Loss: 0.374716\n",
      "[2025-05-13 10:48:46,095][training][INFO] - Epoch 99, Batch 100/3907, Loss: 0.382602\n",
      "[2025-05-13 10:48:46,483][training][INFO] - Epoch 99, Batch 200/3907, Loss: 0.379571\n",
      "[2025-05-13 10:48:46,872][training][INFO] - Epoch 99, Batch 300/3907, Loss: 0.374872\n",
      "[2025-05-13 10:48:47,259][training][INFO] - Epoch 99, Batch 400/3907, Loss: 0.372943\n",
      "[2025-05-13 10:48:47,645][training][INFO] - Epoch 99, Batch 500/3907, Loss: 0.376037\n",
      "[2025-05-13 10:48:48,035][training][INFO] - Epoch 99, Batch 600/3907, Loss: 0.374848\n",
      "[2025-05-13 10:48:48,422][training][INFO] - Epoch 99, Batch 700/3907, Loss: 0.380039\n",
      "[2025-05-13 10:48:48,808][training][INFO] - Epoch 99, Batch 800/3907, Loss: 0.379706\n",
      "[2025-05-13 10:48:49,195][training][INFO] - Epoch 99, Batch 900/3907, Loss: 0.383993\n",
      "[2025-05-13 10:48:49,583][training][INFO] - Epoch 99, Batch 1000/3907, Loss: 0.382697\n",
      "[2025-05-13 10:48:49,977][training][INFO] - Epoch 99, Batch 1100/3907, Loss: 0.375491\n",
      "[2025-05-13 10:48:50,363][training][INFO] - Epoch 99, Batch 1200/3907, Loss: 0.378560\n",
      "[2025-05-13 10:48:50,756][training][INFO] - Epoch 99, Batch 1300/3907, Loss: 0.377805\n",
      "[2025-05-13 10:48:51,144][training][INFO] - Epoch 99, Batch 1400/3907, Loss: 0.375995\n",
      "[2025-05-13 10:48:51,531][training][INFO] - Epoch 99, Batch 1500/3907, Loss: 0.380066\n",
      "[2025-05-13 10:48:51,916][training][INFO] - Epoch 99, Batch 1600/3907, Loss: 0.378765\n",
      "[2025-05-13 10:48:52,305][training][INFO] - Epoch 99, Batch 1700/3907, Loss: 0.376262\n",
      "[2025-05-13 10:48:52,692][training][INFO] - Epoch 99, Batch 1800/3907, Loss: 0.373809\n",
      "[2025-05-13 10:48:53,075][training][INFO] - Epoch 99, Batch 1900/3907, Loss: 0.368359\n",
      "[2025-05-13 10:48:53,465][training][INFO] - Epoch 99, Batch 2000/3907, Loss: 0.372158\n",
      "[2025-05-13 10:48:53,852][training][INFO] - Epoch 99, Batch 2100/3907, Loss: 0.380515\n",
      "[2025-05-13 10:48:54,238][training][INFO] - Epoch 99, Batch 2200/3907, Loss: 0.376599\n",
      "[2025-05-13 10:48:54,627][training][INFO] - Epoch 99, Batch 2300/3907, Loss: 0.370702\n",
      "[2025-05-13 10:48:55,012][training][INFO] - Epoch 99, Batch 2400/3907, Loss: 0.379991\n",
      "[2025-05-13 10:48:55,397][training][INFO] - Epoch 99, Batch 2500/3907, Loss: 0.380003\n",
      "[2025-05-13 10:48:55,786][training][INFO] - Epoch 99, Batch 2600/3907, Loss: 0.382787\n",
      "[2025-05-13 10:48:56,173][training][INFO] - Epoch 99, Batch 2700/3907, Loss: 0.381540\n",
      "[2025-05-13 10:48:56,562][training][INFO] - Epoch 99, Batch 2800/3907, Loss: 0.377793\n",
      "[2025-05-13 10:48:56,947][training][INFO] - Epoch 99, Batch 2900/3907, Loss: 0.379837\n",
      "[2025-05-13 10:48:57,338][training][INFO] - Epoch 99, Batch 3000/3907, Loss: 0.377855\n",
      "[2025-05-13 10:48:57,729][training][INFO] - Epoch 99, Batch 3100/3907, Loss: 0.381800\n",
      "[2025-05-13 10:48:58,119][training][INFO] - Epoch 99, Batch 3200/3907, Loss: 0.370652\n",
      "[2025-05-13 10:48:58,506][training][INFO] - Epoch 99, Batch 3300/3907, Loss: 0.365414\n",
      "[2025-05-13 10:48:58,896][training][INFO] - Epoch 99, Batch 3400/3907, Loss: 0.384134\n",
      "[2025-05-13 10:48:59,285][training][INFO] - Epoch 99, Batch 3500/3907, Loss: 0.364054\n",
      "[2025-05-13 10:48:59,672][training][INFO] - Epoch 99, Batch 3600/3907, Loss: 0.370009\n",
      "[2025-05-13 10:49:00,062][training][INFO] - Epoch 99, Batch 3700/3907, Loss: 0.375316\n",
      "[2025-05-13 10:49:00,450][training][INFO] - Epoch 99, Batch 3800/3907, Loss: 0.376171\n",
      "[2025-05-13 10:49:00,861][training][INFO] - Epoch 99, Batch 3900/3907, Loss: 0.376272\n",
      "[2025-05-13 10:49:00,877][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:49:00,878][training][INFO] - Epoch 99 complete. Avg Loss: 0.376658\n",
      "[2025-05-13 10:49:00,923][training][INFO] - Epoch 100, Batch 0/3907, Loss: 0.370751\n",
      "[2025-05-13 10:49:01,315][training][INFO] - Epoch 100, Batch 100/3907, Loss: 0.384291\n",
      "[2025-05-13 10:49:01,703][training][INFO] - Epoch 100, Batch 200/3907, Loss: 0.367474\n",
      "[2025-05-13 10:49:02,090][training][INFO] - Epoch 100, Batch 300/3907, Loss: 0.369204\n",
      "[2025-05-13 10:49:02,477][training][INFO] - Epoch 100, Batch 400/3907, Loss: 0.373828\n",
      "[2025-05-13 10:49:02,864][training][INFO] - Epoch 100, Batch 500/3907, Loss: 0.379932\n",
      "[2025-05-13 10:49:03,254][training][INFO] - Epoch 100, Batch 600/3907, Loss: 0.385109\n",
      "[2025-05-13 10:49:03,640][training][INFO] - Epoch 100, Batch 700/3907, Loss: 0.367116\n",
      "[2025-05-13 10:49:04,027][training][INFO] - Epoch 100, Batch 800/3907, Loss: 0.381215\n",
      "[2025-05-13 10:49:04,415][training][INFO] - Epoch 100, Batch 900/3907, Loss: 0.371639\n",
      "[2025-05-13 10:49:04,805][training][INFO] - Epoch 100, Batch 1000/3907, Loss: 0.379921\n",
      "[2025-05-13 10:49:05,189][training][INFO] - Epoch 100, Batch 1100/3907, Loss: 0.369991\n",
      "[2025-05-13 10:49:05,577][training][INFO] - Epoch 100, Batch 1200/3907, Loss: 0.377741\n",
      "[2025-05-13 10:49:05,966][training][INFO] - Epoch 100, Batch 1300/3907, Loss: 0.374611\n",
      "[2025-05-13 10:49:06,350][training][INFO] - Epoch 100, Batch 1400/3907, Loss: 0.377836\n",
      "[2025-05-13 10:49:06,739][training][INFO] - Epoch 100, Batch 1500/3907, Loss: 0.379066\n",
      "[2025-05-13 10:49:07,131][training][INFO] - Epoch 100, Batch 1600/3907, Loss: 0.383255\n",
      "[2025-05-13 10:49:07,516][training][INFO] - Epoch 100, Batch 1700/3907, Loss: 0.376659\n",
      "[2025-05-13 10:49:07,901][training][INFO] - Epoch 100, Batch 1800/3907, Loss: 0.384231\n",
      "[2025-05-13 10:49:08,286][training][INFO] - Epoch 100, Batch 1900/3907, Loss: 0.383600\n",
      "[2025-05-13 10:49:08,673][training][INFO] - Epoch 100, Batch 2000/3907, Loss: 0.374879\n",
      "[2025-05-13 10:49:09,058][training][INFO] - Epoch 100, Batch 2100/3907, Loss: 0.376782\n",
      "[2025-05-13 10:49:09,445][training][INFO] - Epoch 100, Batch 2200/3907, Loss: 0.377086\n",
      "[2025-05-13 10:49:09,832][training][INFO] - Epoch 100, Batch 2300/3907, Loss: 0.381971\n",
      "[2025-05-13 10:49:10,222][training][INFO] - Epoch 100, Batch 2400/3907, Loss: 0.369211\n",
      "[2025-05-13 10:49:10,608][training][INFO] - Epoch 100, Batch 2500/3907, Loss: 0.377237\n",
      "[2025-05-13 10:49:10,994][training][INFO] - Epoch 100, Batch 2600/3907, Loss: 0.373644\n",
      "[2025-05-13 10:49:11,383][training][INFO] - Epoch 100, Batch 2700/3907, Loss: 0.378525\n",
      "[2025-05-13 10:49:11,770][training][INFO] - Epoch 100, Batch 2800/3907, Loss: 0.383775\n",
      "[2025-05-13 10:49:12,154][training][INFO] - Epoch 100, Batch 2900/3907, Loss: 0.383284\n",
      "[2025-05-13 10:49:12,540][training][INFO] - Epoch 100, Batch 3000/3907, Loss: 0.371372\n",
      "[2025-05-13 10:49:12,925][training][INFO] - Epoch 100, Batch 3100/3907, Loss: 0.377603\n",
      "[2025-05-13 10:49:13,313][training][INFO] - Epoch 100, Batch 3200/3907, Loss: 0.380507\n",
      "[2025-05-13 10:49:13,698][training][INFO] - Epoch 100, Batch 3300/3907, Loss: 0.383401\n",
      "[2025-05-13 10:49:14,092][training][INFO] - Epoch 100, Batch 3400/3907, Loss: 0.383514\n",
      "[2025-05-13 10:49:14,482][training][INFO] - Epoch 100, Batch 3500/3907, Loss: 0.381806\n",
      "[2025-05-13 10:49:14,871][training][INFO] - Epoch 100, Batch 3600/3907, Loss: 0.376499\n",
      "[2025-05-13 10:49:15,260][training][INFO] - Epoch 100, Batch 3700/3907, Loss: 0.377187\n",
      "[2025-05-13 10:49:15,652][training][INFO] - Epoch 100, Batch 3800/3907, Loss: 0.379590\n",
      "[2025-05-13 10:49:16,062][training][INFO] - Epoch 100, Batch 3900/3907, Loss: 0.376489\n",
      "[2025-05-13 10:49:16,078][training][INFO] - Learning rate: 0.000200\n",
      "[2025-05-13 10:49:16,078][training][INFO] - Epoch 100 complete. Avg Loss: 0.376670\n",
      "[2025-05-13 10:49:16,085][training][INFO] - Saved checkpoint to /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7/checkpoint_epoch_100.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "[2025-05-13 10:49:16,156][training][INFO] - Evaluation Loss: 0.377331\n",
      "[2025-05-13 10:49:16,336][training][INFO] - Original samples shape: torch.Size([256, 100, 3])\n",
      "[2025-05-13 10:49:16,337][training][INFO] - Generated samples shape: torch.Size([256, 100, 3])\n",
      "[2025-05-13 10:49:21,194][training][INFO] - New best model saved to /orcd/data/omarabu/001/gokul/DistributionEmbeddings/outputs/multinomial_069a4b4f71c36569e9352b49f606eff7/best_model.pt\n",
      "[2025-05-13 10:49:21,195][training][INFO] - Training completed in 1545.05 seconds\n",
      "[2025-05-13 10:49:21,196][__main__][INFO] - Training completed. Best epoch: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               batch/epoch \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                batch/loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: batch/reconstruction_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                batch/step \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch/epoch \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch/eval_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch/learning_rate \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch/train_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               batch/epoch 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                batch/loss 0.37649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: batch/reconstruction_loss 0.37649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                batch/step 390693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                best_epoch 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 best_loss 0.37733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               config_hash 069a4b4f71c36569e935...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch/epoch 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           epoch/eval_loss 0.37733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch/learning_rate 0.0002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch/train_loss 0.37667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           final_eval_loss 0.37733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          final_train_loss 0.37667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                total_time 1545.05378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run \u001b[33msplendid-bush-539\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ggowri-harvard-university/distribution-embeddings/runs/xme9sjjf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ggowri-harvard-university/distribution-embeddings\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 6 media file(s), 0 artifact file(s) and 2 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250513_102315-xme9sjjf/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python3 main.py experiment=multinomial encoder=gnn generator=ddpm model=diffusion_mlp dataset.n_sets=1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc7981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
