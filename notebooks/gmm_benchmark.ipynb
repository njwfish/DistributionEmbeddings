{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76818e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[0])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ff9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiment_utils import get_all_experiments_info, load_best_model\n",
    "import torch\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from notebooks.mnist_classifier.mnist_tiny_cnn import TinyCNN\n",
    "\n",
    "from mixer.mixer import SetMixer\n",
    "from datasets.mnist import MNISTDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import product\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets.distribution_datasets import GaussianMixtureModelDataset\n",
    "from utils.gmm_utils import fit_gmm_batch\n",
    "\n",
    "from ot.gmm import gmm_ot_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982aed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "configs = get_all_experiments_info('/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/', False)\n",
    "cfg = [\n",
    "    c for c in configs if 'gmm_sys' in c['name']\n",
    "    and c['config']['training']['num_epochs'] == 200\n",
    "]\n",
    "\n",
    "# load model and move to device\n",
    "def load_model(cfg, path, device):\n",
    "    enc = hydra.utils.instantiate(cfg['encoder'])\n",
    "    gen = hydra.utils.instantiate(cfg['generator'])\n",
    "    state = load_best_model(path)\n",
    "    enc.load_state_dict(state['encoder_state_dict'])\n",
    "    gen.model.load_state_dict(state['generator_state_dict'])\n",
    "    enc.eval()\n",
    "    gen.eval()\n",
    "    enc.to(device)\n",
    "    gen.to(device)\n",
    "    return enc, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97010975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'gmm_systematic_exp_fb3f1bafd8959cfb09ed0b676e2c143a',\n",
       "  'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/gmm_systematic_exp_fb3f1bafd8959cfb09ed0b676e2c143a',\n",
       "  'config': {'dataset': {'_target_': 'datasets.distribution_datasets.MultivariateNormalDistributionDataset', 'n_sets': 50000, 'set_size': '${experiment.set_size}', 'data_shape': [2], 'seed': '${seed}', 'prior_mu': [0, 5], 'prior_cov_df': 10, 'prior_cov_scale': 1}, 'encoder': {'_target_': 'encoder.encoders.DistributionEncoderTx', 'in_dim': '${dataset.data_shape[0]}', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'set_size': '${experiment.set_size}', 'layers': 2, 'heads': 4}, 'model': {'_target_': 'layers.MLP', 'in_dims': [2, 32, 1], 'hidden_dim': 128, 'out_dim': 2, 'layers': 4}, 'generator': {'_target_': 'generator.ddpm.DDPM', 'model': '${model}', 'betas': [0.0001, 0.02], 'n_T': 400, 'drop_prob': 0.1, 'noise_shape': '${dataset.data_shape}'}, 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': '${experiment.lr}', 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ConstantLR', '_partial_': True}, 'training': {'_target_': 'training.Trainer', 'num_epochs': 200, 'log_interval': 100, 'save_interval': 20, 'eval_interval': 100, 'early_stopping': False, 'patience': 5, 'use_tqdm': False}, 'mixer': {'_target_': 'mixer.mixer.SetMixer', 'k': '${experiment.n_mix}', 'alpha': '${experiment.alpha}'}, 'wandb': {'_target_': 'wandb_utils.setup_wandb', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'mode': 'online', 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'experiment': {'wandb': {'_target_': 'wandb_utils.setup_wandb', 'mode': 'disabled', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'name': 'gmm_systematic_exp', 'description': 'Gaussian Mixture Model experiment with distribution embeddings', 'latent_dim': 32, 'hidden_dim': 128, 'set_size': 300, 'batch_size': 256, 'n_mix': 3, 'lr': 0.0002, 'alpha': 1.0}, 'loss': {'_target_': 'loss.default.LossManager', 'mask_context_prob': 0.0}, 'experiment_name': '${experiment.name}', 'seed': 42, 'device': 'cuda'},\n",
       "  'config_hash': 'fb3f1bafd8959cfb09ed0b676e2c143a',\n",
       "  'encoder': 'DistributionEncoderTx',\n",
       "  'generator': 'DDPM',\n",
       "  'model': 'MLP',\n",
       "  'dataset': 'MultivariateNormalDistributionDataset',\n",
       "  'latent_dim': 'N/A',\n",
       "  'training_epochs': 'N/A',\n",
       "  'batch_size': 'N/A',\n",
       "  'checkpoints': [],\n",
       "  'best_model': None,\n",
       "  'best_loss': 'N/A',\n",
       "  'best_epoch': 'N/A'},\n",
       " {'name': 'gmm_systematic_exp_ffd7b9e0d9e46d0cf2e49d9fb5facccd',\n",
       "  'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/gmm_systematic_exp_ffd7b9e0d9e46d0cf2e49d9fb5facccd',\n",
       "  'config': {'dataset': {'_target_': 'datasets.distribution_datasets.MultivariateNormalDistributionDataset', 'n_sets': 50000, 'set_size': '${experiment.set_size}', 'data_shape': [2], 'seed': '${seed}', 'prior_mu': [0, 5], 'prior_cov_df': 10, 'prior_cov_scale': 1}, 'encoder': {'_target_': 'encoder.encoders.DistributionEncoderGNN', 'in_dim': '${dataset.data_shape[0]}', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'set_size': '${experiment.set_size}', 'layers': 2, 'fc_layers': 2}, 'model': {'_target_': 'model.cvae.SimpleVAE', 'in_dim': 2, 'latent_dim': 32, 'hidden_dim': 128, 'vae_latent_dim': 32, 'fc_layers': 4}, 'generator': {'_target_': 'generator.cvae.CVAE', 'model': '${model}', 'latent_dim': '${experiment.latent_dim}', 'noise_shape': '${experiment.latent_dim}', 'beta': 1.0, 'kl_anneal_steps': 0, 'kl_anneal_start': 0.0}, 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': '${experiment.lr}', 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ConstantLR', '_partial_': True}, 'training': {'_target_': 'training.Trainer', 'num_epochs': 200, 'log_interval': 100, 'save_interval': 20, 'eval_interval': 100, 'early_stopping': False, 'patience': 5, 'use_tqdm': False}, 'mixer': {'_target_': 'mixer.mixer.SetMixer', 'k': '${experiment.n_mix}', 'alpha': '${experiment.alpha}'}, 'wandb': {'_target_': 'wandb_utils.setup_wandb', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'mode': 'online', 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'experiment': {'wandb': {'_target_': 'wandb_utils.setup_wandb', 'mode': 'disabled', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'name': 'gmm_systematic_exp', 'description': 'Gaussian Mixture Model experiment with distribution embeddings', 'latent_dim': 32, 'hidden_dim': 128, 'set_size': 300, 'batch_size': 256, 'n_mix': 3, 'lr': 0.0002, 'alpha': 1.0}, 'loss': {'_target_': 'loss.default.LossManager', 'mask_context_prob': 0.0}, 'experiment_name': '${experiment.name}', 'seed': 42, 'device': 'cuda'},\n",
       "  'config_hash': 'ffd7b9e0d9e46d0cf2e49d9fb5facccd',\n",
       "  'encoder': 'DistributionEncoderGNN',\n",
       "  'generator': 'CVAE',\n",
       "  'model': 'SimpleVAE',\n",
       "  'dataset': 'MultivariateNormalDistributionDataset',\n",
       "  'latent_dim': 'N/A',\n",
       "  'training_epochs': 'N/A',\n",
       "  'batch_size': 'N/A',\n",
       "  'checkpoints': [],\n",
       "  'best_model': None,\n",
       "  'best_loss': 'N/A',\n",
       "  'best_epoch': 'N/A'},\n",
       " {'name': 'gmm_systematic_exp_ae68613e0026280d6c1750f5f240b0e4',\n",
       "  'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/gmm_systematic_exp_ae68613e0026280d6c1750f5f240b0e4',\n",
       "  'config': {'dataset': {'_target_': 'datasets.distribution_datasets.MultivariateNormalDistributionDataset', 'n_sets': 50000, 'set_size': '${experiment.set_size}', 'data_shape': [2], 'seed': '${seed}', 'prior_mu': [0, 5], 'prior_cov_df': 10, 'prior_cov_scale': 1}, 'encoder': {'_target_': 'encoder.wormhole_encoder.WormholeEncoder', 'data_shape': '${dataset.data_shape}', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'set_size': '${experiment.set_size}', 'layers': 2, 'heads': 4}, 'model': {'_target_': 'types.NoneType'}, 'generator': {'_target_': 'generator.wormhole.WormholeGenerator', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'data_shape': '${dataset.data_shape}', 'set_size': '${dataset.set_size}', 'layers': 2, 'heads': 4}, 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': '${experiment.lr}', 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ConstantLR', '_partial_': True}, 'training': {'_target_': 'training.Trainer', 'num_epochs': 200, 'log_interval': 100, 'save_interval': 20, 'eval_interval': 100, 'early_stopping': False, 'patience': 5, 'use_tqdm': False}, 'mixer': {'_target_': 'mixer.mixer.SetMixer', 'k': '${experiment.n_mix}', 'alpha': '${experiment.alpha}'}, 'wandb': {'_target_': 'wandb_utils.setup_wandb', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'mode': 'online', 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'experiment': {'wandb': {'_target_': 'wandb_utils.setup_wandb', 'mode': 'disabled', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'name': 'gmm_systematic_exp', 'description': 'Gaussian Mixture Model experiment with distribution embeddings', 'latent_dim': 32, 'hidden_dim': 128, 'set_size': 300, 'batch_size': 256, 'n_mix': 3, 'lr': 0.0002, 'alpha': 1.0}, 'loss': {'_target_': 'loss.default.LossManager', 'mask_context_prob': 0.0}, 'experiment_name': '${experiment.name}', 'seed': 42, 'device': 'cuda'},\n",
       "  'config_hash': 'ae68613e0026280d6c1750f5f240b0e4',\n",
       "  'encoder': 'WormholeEncoder',\n",
       "  'generator': 'WormholeGenerator',\n",
       "  'model': 'NoneType',\n",
       "  'dataset': 'MultivariateNormalDistributionDataset',\n",
       "  'latent_dim': 'N/A',\n",
       "  'training_epochs': 'N/A',\n",
       "  'batch_size': 'N/A',\n",
       "  'checkpoints': [],\n",
       "  'best_model': None,\n",
       "  'best_loss': 'N/A',\n",
       "  'best_epoch': 'N/A'},\n",
       " {'name': 'gmm_systematic_exp_442891f2fd3acfa594826ee12a74dc05',\n",
       "  'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/gmm_systematic_exp_442891f2fd3acfa594826ee12a74dc05',\n",
       "  'config': {'dataset': {'_target_': 'datasets.distribution_datasets.MultivariateNormalDistributionDataset', 'n_sets': 50000, 'set_size': '${experiment.set_size}', 'data_shape': [2], 'seed': '${seed}', 'prior_mu': [0, 5], 'prior_cov_df': 10, 'prior_cov_scale': 1}, 'encoder': {'_target_': 'encoder.encoders.DistributionEncoderGNN', 'in_dim': '${dataset.data_shape[0]}', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'set_size': '${experiment.set_size}', 'layers': 2, 'fc_layers': 2}, 'model': {'_target_': 'layers.MLP', 'in_dims': [32, 32], 'hidden_dim': 128, 'out_dim': 2, 'layers': 4}, 'generator': {'_target_': 'generator.direct.DirectGenerator', 'model': '${model}', 'loss_type': 'mmd', 'noise_dim': '${experiment.latent_dim}'}, 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': '${experiment.lr}', 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ConstantLR', '_partial_': True}, 'training': {'_target_': 'training.Trainer', 'num_epochs': 200, 'log_interval': 100, 'save_interval': 20, 'eval_interval': 100, 'early_stopping': False, 'patience': 5, 'use_tqdm': False}, 'mixer': {'_target_': 'mixer.mixer.SetMixer', 'k': '${experiment.n_mix}', 'alpha': '${experiment.alpha}'}, 'wandb': {'_target_': 'wandb_utils.setup_wandb', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'mode': 'online', 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'experiment': {'wandb': {'_target_': 'wandb_utils.setup_wandb', 'mode': 'disabled', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'name': 'gmm_systematic_exp', 'description': 'Gaussian Mixture Model experiment with distribution embeddings', 'latent_dim': 32, 'hidden_dim': 128, 'set_size': 300, 'batch_size': 256, 'n_mix': 3, 'lr': 0.0002, 'alpha': 1.0}, 'loss': {'_target_': 'loss.default.LossManager', 'mask_context_prob': 0.0}, 'experiment_name': '${experiment.name}', 'seed': 42, 'device': 'cuda'},\n",
       "  'config_hash': '442891f2fd3acfa594826ee12a74dc05',\n",
       "  'encoder': 'DistributionEncoderGNN',\n",
       "  'generator': 'DirectGenerator',\n",
       "  'model': 'MLP',\n",
       "  'dataset': 'MultivariateNormalDistributionDataset',\n",
       "  'latent_dim': 'N/A',\n",
       "  'training_epochs': 'N/A',\n",
       "  'batch_size': 'N/A',\n",
       "  'checkpoints': [],\n",
       "  'best_model': None,\n",
       "  'best_loss': 'N/A',\n",
       "  'best_epoch': 'N/A'},\n",
       " {'name': 'gmm_systematic_exp_41712136656aab7a65da6d8fdddcd9cd',\n",
       "  'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/gmm_systematic_exp_41712136656aab7a65da6d8fdddcd9cd',\n",
       "  'config': {'dataset': {'_target_': 'datasets.distribution_datasets.MultivariateNormalDistributionDataset', 'n_sets': 50000, 'set_size': '${experiment.set_size}', 'data_shape': [2], 'seed': '${seed}', 'prior_mu': [0, 5], 'prior_cov_df': 10, 'prior_cov_scale': 1}, 'encoder': {'_target_': 'encoder.encoders.DistributionEncoderTx', 'in_dim': '${dataset.data_shape[0]}', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'set_size': '${experiment.set_size}', 'layers': 2, 'heads': 4}, 'model': {'_target_': 'model.cvae.SimpleVAE', 'in_dim': 2, 'latent_dim': 32, 'hidden_dim': 128, 'vae_latent_dim': 32, 'fc_layers': 4}, 'generator': {'_target_': 'generator.cvae.CVAE', 'model': '${model}', 'latent_dim': '${experiment.latent_dim}', 'noise_shape': '${experiment.latent_dim}', 'beta': 1.0, 'kl_anneal_steps': 0, 'kl_anneal_start': 0.0}, 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': '${experiment.lr}', 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ConstantLR', '_partial_': True}, 'training': {'_target_': 'training.Trainer', 'num_epochs': 200, 'log_interval': 100, 'save_interval': 20, 'eval_interval': 100, 'early_stopping': False, 'patience': 5, 'use_tqdm': False}, 'mixer': {'_target_': 'mixer.mixer.SetMixer', 'k': '${experiment.n_mix}', 'alpha': '${experiment.alpha}'}, 'wandb': {'_target_': 'wandb_utils.setup_wandb', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'mode': 'online', 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'experiment': {'wandb': {'_target_': 'wandb_utils.setup_wandb', 'mode': 'disabled', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'name': 'gmm_systematic_exp', 'description': 'Gaussian Mixture Model experiment with distribution embeddings', 'latent_dim': 32, 'hidden_dim': 128, 'set_size': 300, 'batch_size': 256, 'n_mix': 3, 'lr': 0.0002, 'alpha': 1.0}, 'loss': {'_target_': 'loss.default.LossManager', 'mask_context_prob': 0.0}, 'experiment_name': '${experiment.name}', 'seed': 42, 'device': 'cuda'},\n",
       "  'config_hash': '41712136656aab7a65da6d8fdddcd9cd',\n",
       "  'encoder': 'DistributionEncoderTx',\n",
       "  'generator': 'CVAE',\n",
       "  'model': 'SimpleVAE',\n",
       "  'dataset': 'MultivariateNormalDistributionDataset',\n",
       "  'latent_dim': 'N/A',\n",
       "  'training_epochs': 'N/A',\n",
       "  'batch_size': 'N/A',\n",
       "  'checkpoints': [],\n",
       "  'best_model': None,\n",
       "  'best_loss': 'N/A',\n",
       "  'best_epoch': 'N/A'},\n",
       " {'name': 'gmm_systematic_exp_f3bda8eed18a81a01ae4ae4fe4abb110',\n",
       "  'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/gmm_systematic_exp_f3bda8eed18a81a01ae4ae4fe4abb110',\n",
       "  'config': {'dataset': {'_target_': 'datasets.distribution_datasets.MultivariateNormalDistributionDataset', 'n_sets': 50000, 'set_size': '${experiment.set_size}', 'data_shape': [2], 'seed': '${seed}', 'prior_mu': [0, 5], 'prior_cov_df': 10, 'prior_cov_scale': 1}, 'encoder': {'_target_': 'encoder.encoders.DistributionEncoderGNN', 'in_dim': '${dataset.data_shape[0]}', 'latent_dim': '${experiment.latent_dim}', 'hidden_dim': '${experiment.hidden_dim}', 'set_size': '${experiment.set_size}', 'layers': 2, 'fc_layers': 2}, 'model': {'_target_': 'layers.MLP', 'in_dims': [2, 32, 1], 'hidden_dim': 128, 'out_dim': 2, 'layers': 4}, 'generator': {'_target_': 'generator.ddpm.DDPM', 'model': '${model}', 'betas': [0.0001, 0.02], 'n_T': 400, 'drop_prob': 0.1, 'noise_shape': '${dataset.data_shape}'}, 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': '${experiment.lr}', 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ConstantLR', '_partial_': True}, 'training': {'_target_': 'training.Trainer', 'num_epochs': 200, 'log_interval': 100, 'save_interval': 20, 'eval_interval': 100, 'early_stopping': False, 'patience': 5, 'use_tqdm': False}, 'mixer': {'_target_': 'mixer.mixer.SetMixer', 'k': '${experiment.n_mix}', 'alpha': '${experiment.alpha}'}, 'wandb': {'_target_': 'wandb_utils.setup_wandb', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'mode': 'online', 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'experiment': {'wandb': {'_target_': 'wandb_utils.setup_wandb', 'mode': 'disabled', 'project': 'distribution-embeddings', 'entity': None, 'name': None, 'tags': [], 'notes': None, 'group': None, 'save_code': True}, 'name': 'gmm_systematic_exp', 'description': 'Gaussian Mixture Model experiment with distribution embeddings', 'latent_dim': 32, 'hidden_dim': 128, 'set_size': 300, 'batch_size': 256, 'n_mix': 3, 'lr': 0.0002, 'alpha': 1.0}, 'loss': {'_target_': 'loss.default.LossManager', 'mask_context_prob': 0.0}, 'experiment_name': '${experiment.name}', 'seed': 42, 'device': 'cuda'},\n",
       "  'config_hash': 'f3bda8eed18a81a01ae4ae4fe4abb110',\n",
       "  'encoder': 'DistributionEncoderGNN',\n",
       "  'generator': 'DDPM',\n",
       "  'model': 'MLP',\n",
       "  'dataset': 'MultivariateNormalDistributionDataset',\n",
       "  'latent_dim': 'N/A',\n",
       "  'training_epochs': 'N/A',\n",
       "  'batch_size': 'N/A',\n",
       "  'checkpoints': [],\n",
       "  'best_model': None,\n",
       "  'best_loss': 'N/A',\n",
       "  'best_epoch': 'N/A'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf0bd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5aa8d0035504545a076988e5c85e8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: DistributionEncoderTx, Generator: DDPM, OT error: 17.922968574795117, data shape: 2\n",
      "Encoder: DistributionEncoderGNN, Generator: CVAE, OT error: 18.602895371552073, data shape: 2\n",
      "Encoder: WormholeEncoder, Generator: WormholeGenerator, OT error: 17.918656918880316, data shape: 2\n",
      "Encoder: DistributionEncoderGNN, Generator: DirectGenerator, OT error: 14.822601921486296, data shape: 2\n",
      "Encoder: DistributionEncoderTx, Generator: CVAE, OT error: 16.623401321714727, data shape: 2\n",
      "Encoder: DistributionEncoderGNN, Generator: DDPM, OT error: 16.34861585789228, data shape: 2\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    \"Encoder\" : [],\n",
    "    \"Generator\" : [],\n",
    "    \"N dims\" : [],\n",
    "    \"OT reconstruction error\" : []\n",
    "}\n",
    "\n",
    "N_sets = 40\n",
    "set_size = 10**3\n",
    "\n",
    "\n",
    "for c in tqdm(cfg):\n",
    "    encoder_name = c['encoder']\n",
    "    generator_name = c['generator']\n",
    "    data_shape = c['config']['dataset']['data_shape']\n",
    "    num_epochs = c['config']['training']['num_epochs']\n",
    "\n",
    "    if num_epochs != 200:\n",
    "        continue\n",
    "\n",
    "    if 'KME' in encoder_name or 'Mean' in encoder_name:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        enc, gen = load_model(c['config'], c['dir'], device=device)\n",
    "    except:\n",
    "        print(encoder_name)\n",
    "        continue\n",
    "\n",
    "    if data_shape[0] != 2:\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    if 'Tx' not in encoder_name and 'Wormhole' not in encoder_name:\n",
    "        set_size = 10**5\n",
    "    else:\n",
    "        set_size=10**3\n",
    "\n",
    "    dataset = GaussianMixtureModelDataset(\n",
    "        n_sets=N_sets,\n",
    "        set_size=set_size,\n",
    "        prior_mu=(0,5),\n",
    "        data_shape=data_shape\n",
    "    )\n",
    "    \n",
    "    ot_errors = []\n",
    "    for idx in range(len(dataset)//8):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(dataset.data[idx*8:(idx+1)*8], dtype=torch.float).cuda()\n",
    "            z = enc(x)\n",
    "            x_hat = gen.sample(z, num_samples=10**5)\n",
    "\n",
    "        mus = dataset.mu[idx*8:(idx+1)*8]\n",
    "        covs = dataset.cov[idx*8:(idx+1)*8]\n",
    "        weights = dataset.weights[idx*8:(idx+1)*8]\n",
    "\n",
    "\n",
    "        r_means, r_covs, r_weights = fit_gmm_batch(\n",
    "            x_hat.detach().cpu().numpy(), \n",
    "            0.5 * mus,\n",
    "            covs,\n",
    "            weights,\n",
    "            # use_kmeans_init=True\n",
    "        )\n",
    "\n",
    "        ot_dists = [\n",
    "            gmm_ot_loss(r_m, m, r_c, c, r_w, w) \n",
    "            for r_m, m, r_c, c, r_w, w in zip(r_means, mus, r_covs, covs, r_weights, weights)\n",
    "        ]\n",
    "        ot_errors += ot_dists\n",
    "\n",
    "    d['Encoder'].append(encoder_name)\n",
    "    d['Generator'].append(generator_name)\n",
    "    d['N dims'].append(data_shape[0])\n",
    "    d['OT reconstruction error'].append(np.mean(ot_errors))\n",
    "    print(f\"Encoder: {encoder_name}, Generator: {generator_name}, OT error: {np.mean(ot_errors)}, data shape: {data_shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd47b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Generator</th>\n",
       "      <th>N dims</th>\n",
       "      <th>OT reconstruction error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DistributionEncoderTx</td>\n",
       "      <td>DirectGenerator</td>\n",
       "      <td>10</td>\n",
       "      <td>8.776316e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DistributionEncoderTx</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>10</td>\n",
       "      <td>1.126318e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DistributionEncoderTx</td>\n",
       "      <td>DirectGenerator</td>\n",
       "      <td>10</td>\n",
       "      <td>6.721938e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WormholeEncoder</td>\n",
       "      <td>WormholeGenerator</td>\n",
       "      <td>10</td>\n",
       "      <td>8.771564e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DistributionEncoderGNN</td>\n",
       "      <td>CVAE</td>\n",
       "      <td>10</td>\n",
       "      <td>3.744157e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DistributionEncoderTx</td>\n",
       "      <td>DDPM</td>\n",
       "      <td>10</td>\n",
       "      <td>7.753786e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DistributionEncoderGNN</td>\n",
       "      <td>DirectGenerator</td>\n",
       "      <td>10</td>\n",
       "      <td>3.222186e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DistributionEncoderGNN</td>\n",
       "      <td>DirectGenerator</td>\n",
       "      <td>10</td>\n",
       "      <td>8.801756e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Encoder          Generator  N dims  OT reconstruction error\n",
       "1   DistributionEncoderTx    DirectGenerator      10             8.776316e+02\n",
       "0   DistributionEncoderTx               CVAE      10             1.126318e+03\n",
       "3   DistributionEncoderTx    DirectGenerator      10             6.721938e+03\n",
       "4         WormholeEncoder  WormholeGenerator      10             8.771564e+03\n",
       "5  DistributionEncoderGNN               CVAE      10             3.744157e+04\n",
       "2   DistributionEncoderTx               DDPM      10             7.753786e+04\n",
       "6  DistributionEncoderGNN    DirectGenerator      10             3.222186e+06\n",
       "7  DistributionEncoderGNN    DirectGenerator      10             8.801756e+07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d).sort_values(by='OT reconstruction error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2884e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-types",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
