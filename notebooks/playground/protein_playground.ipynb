{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec09cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acefbeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da768df1d06451cb01f9583d6eeac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f3fe308e874392bcd0bcc8cf3794fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_progen.py:   0%|          | 0.00/2.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/hugohrban/progen2-medium:\n",
      "- configuration_progen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c0193d180049c1b62a5082d3b237af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_progen.py:   0%|          | 0.00/24.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/hugohrban/progen2-medium:\n",
      "- modeling_progen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1756dc7ebce4e1d8b6e52006e9c1b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a621c708db414c9f9b40a073e801ecd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bea08b30f74291bfb6fb87e8c2daaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"hugohrban/progen2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e12f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "class ZToPrefix(nn.Module):\n",
    "    def __init__(self, input_dim, prefix_length, d_model):\n",
    "        super().__init__()\n",
    "        # A simple linear projection that outputs prefix_length * d_model values\n",
    "        self.fc = nn.Linear(input_dim, prefix_length * d_model)\n",
    "        self.prefix_length = prefix_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        prefix = self.fc(x)            # Shape: (batch_size, prefix_length * d_model)\n",
    "        prefix = prefix.view(batch_size, self.prefix_length, self.d_model)  # Reshape to (batch_size, prefix_length, d_model)\n",
    "        return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26db2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "esm_dim = 768  # Adjust this to match the dimension of your ESM representations\n",
    "generated_esm_embedding = torch.randn(batch_size, esm_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa11c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 32])\n",
      "torch.Size([1, 32])\n",
      "<|pad|> 9.670131449063257e-13\n",
      "<|bos|> 9.261969446602847e-13\n",
      "<|eos|> 1.0327710041335703e-11\n",
      "1 0.1502617597579956\n",
      "2 0.31953683495521545\n",
      "A 0.018371088430285454\n",
      "B 5.238836138232728e-07\n",
      "C 0.0053237020038068295\n",
      "D 0.01670844480395317\n",
      "E 0.028254467993974686\n",
      "F 0.009900936856865883\n",
      "G 0.03264149650931358\n",
      "H 0.01732638105750084\n",
      "I 0.015535769984126091\n",
      "K 0.023146817460656166\n",
      "L 0.01947997882962227\n",
      "M 0.15795548260211945\n",
      "N 0.024909188970923424\n",
      "O 6.143419284398988e-08\n",
      "P 0.011583761312067509\n",
      "Q 0.02621517702937126\n",
      "R 0.02283564582467079\n",
      "S 0.026994973421096802\n",
      "T 0.02846738137304783\n",
      "U 6.175801559038518e-07\n",
      "V 0.020205963402986526\n",
      "W 0.004934186115860939\n",
      "X 0.003107278374955058\n",
      "Y 0.016301607713103294\n",
      "Z 5.45181023881014e-07\n"
     ]
    }
   ],
   "source": [
    "prefix_length = 10\n",
    "d_model = model.config.embed_dim  # e.g., 1024 or whatever the model uses\n",
    "\n",
    "prefix_module = ESMToPrefix(input_dim=esm_dim, prefix_length=prefix_length, d_model=d_model)\n",
    "\n",
    "# Map the generated ESM embedding to prefix embeddings.\n",
    "prefix_embeddings = prefix_module(generated_esm_embedding)  # shape: (batch_size, prefix_length, d_model)\n",
    "\n",
    "\n",
    "\n",
    "# forward pass\n",
    "logits = model(inputs_embeds=prefix_embeddings).logits\n",
    "\n",
    "print(logits.shape)\n",
    "# print output probabilities\n",
    "next_token_logits = logits[:, -1, :]\n",
    "next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "print(next_token_logits.shape)\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    print(tokenizer.decode(i), next_token_probs[0, i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1bca340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"1MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt)).to(model.device)\n",
    "\n",
    "# forward pass\n",
    "logits = model(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c2fb83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(prompt)):\n",
    "\n",
    "#     print(tokenizer.decode(F.softmax(logits[i, :], dim=-1).argmax()), prompt[i+1])\n",
    "tokenizer.encode('<|bos|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZToPrefix(nn.Module):\n",
    "    def __init__(self, input_dim, prefix_length, d_model):\n",
    "        super().__init__()\n",
    "        # A simple linear projection that outputs prefix_length * d_model values\n",
    "        self.fc = nn.Linear(input_dim, prefix_length * d_model)\n",
    "        self.prefix_length = prefix_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        prefix = self.fc(x)            # Shape: (batch_size, prefix_length * d_model)\n",
    "        prefix = prefix.view(batch_size, self.prefix_length, self.d_model)  # Reshape to (batch_size, prefix_length, d_model)\n",
    "        return prefix\n",
    "\n",
    "class ConditionedProgen2(nn.Module):\n",
    "    def __init__(self, progen2_name='hugohrban/progen2-medium', \n",
    "                 latent_dim=128, prefix_length=10, ):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(progen2_name, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(progen2_name, trust_remote_code=True)\n",
    "        self.prefix_module = ZToPrefix(latent_dim, prefix_length, self.model.config.embed_dim)\n",
    "\n",
    "    def forward(self, input_ids, esm_embedding):\n",
    "        # Get the prefix embeddings from the ESM embedding\n",
    "        prefix_embeddings = self.prefix_module(esm_embedding)\n",
    "        \n",
    "        # Concatenate the prefix embeddings with the input embeddings\n",
    "        input_embeddings = self.model.transformer.wte(input_ids)  # Get the token embeddings\n",
    "\n",
    "        combined_embeddings = torch.cat((prefix_embeddings, input_embeddings), dim=1)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs = self.model(inputs_embeds=combined_embeddings)\n",
    "        return outputs.logits[:, 1:, :]\n",
    "        \n",
    "class Progen2Generator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        progen2_name=\"hugohrban/progen2-medium\",\n",
    "        latent_dim=128,\n",
    "        prefix_length=10,\n",
    "        temperature=1.0,\n",
    "        max_length=1024,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        self.model = ConditionedProgen2(\n",
    "            progen2_name=progen2_name,\n",
    "            latent_dim=latent_dim,\n",
    "            prefix_length=prefix_length,\n",
    "        ).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(progen2_name, trust_remote_code=True)\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "    def loss(self, x, latent):\n",
    "        input_ids = self.tokenizer.encode(x.to(self.device))\n",
    "        shift_logits = self.model(input_ids, latent)[:, :-1, :]\n",
    "        shift_labels = input_ids[:, 1:]\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.reshape(-1, shift_logits.size(-1)),\n",
    "            shift_labels.reshape(-1)\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def sample(self, latent, num_samples=1, return_seq=False):\n",
    "        device = latent.device\n",
    "        batch_size = latent.size(0)\n",
    "        start_ids = torch.tensor(\n",
    "            [self.tokenizer.encode('1')] * batch_size,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        all_samples = []\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                out = self._generate(start_ids, latent)\n",
    "            all_samples.append(out)\n",
    "\n",
    "        out = torch.stack(all_samples, dim=1)\n",
    "\n",
    "        if return_seq:\n",
    "            texts = [\n",
    "                [self.tokenizer.decode(out[b, n], skip_special_tokens=True)\n",
    "                 for n in range(num_samples)]\n",
    "                for b in range(batch_size)\n",
    "            ]\n",
    "            return out, texts\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _generate(self, input_ids, latent):\n",
    "        cur_ids = input_ids\n",
    "        for _ in range(self.max_length - 2):\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(cur_ids, latent)\n",
    "            next_logits = logits[:, -1, :] / self.temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            cur_ids = torch.cat([cur_ids, next_token], dim=1)\n",
    "        return cur_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c4ddc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg2 = Progen2Generator(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6617d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 3, 12, 23, 25, 15, 10, 11, 11, 14, 23, 21, 28, 20, 11, 22, 25, 20,\n",
       "           15, 19, 28, 11, 10, 28, 23, 14, 22, 20, 21, 25,  5, 10, 21, 10,  8,\n",
       "           17,  9, 15, 20, 21, 26, 14,  5, 11, 11, 14,  5,  5, 20,  9, 25, 21,\n",
       "           16, 25,  9, 10, 11,  7,  5,  8, 12,  9, 19,  9,  5, 26, 23, 14, 25,\n",
       "            5, 11,  5,  9, 11, 12,  9, 21, 25, 20, 15,  7, 22,  8, 21, 21, 22,\n",
       "           12, 25, 10, 16,  8, 16, 23, 11, 11, 25, 17, 15, 25, 28,  5, 25, 25,\n",
       "           11,  5, 17,  9, 23, 23, 28, 22, 15, 28, 25, 11, 15,  9, 14, 25, 28,\n",
       "            5, 12,  5,  8,  5, 26, 17, 23, 21, 28, 15, 21, 11,  5, 14, 20, 11,\n",
       "           25, 22, 15, 21,  8, 25, 28,  8,  8, 14, 21, 21, 23, 28, 17, 25, 22,\n",
       "           13, 12, 21, 15, 22, 25, 21, 28,  7, 14,  8,  5, 23,  5,  5,  9,  7,\n",
       "           14, 23, 25, 21, 10, 16,  5, 13,  5, 13, 23, 17, 11, 21, 12, 12, 19,\n",
       "           21, 13, 12, 15, 12,  9, 28, 12, 13, 10, 17, 22, 26, 15, 26, 21, 17,\n",
       "            8, 15, 20, 21, 15,  5, 14,  5, 11, 22, 14, 13, 22, 11, 14, 22,  9,\n",
       "           13, 14, 20, 13, 14, 15, 17, 19, 14,  5, 21,  7, 14, 14,  7, 14, 26,\n",
       "           19,  9, 13, 22, 17, 21, 22, 20, 11,  8, 19,  8, 14, 13, 13, 11, 13,\n",
       "           11, 28, 21, 11, 19, 28,  8,  5,  4,  2,  4, 27, 23, 22,  5, 11, 22,\n",
       "           15, 13, 11, 20, 13, 21, 22, 15,  9, 21,  9, 15, 21, 17, 11, 15, 10,\n",
       "           15,  8,  8, 15, 17, 16,  5, 15,  8,  5, 15,  5, 11, 21, 13, 25, 23,\n",
       "            9,  9, 28, 21,  5,  9, 21,  9, 15, 22, 22, 15, 15, 15, 23, 25, 13,\n",
       "            9,  8, 21, 25, 14,  9, 12, 22, 21, 15, 21,  8, 25, 19, 20,  9,  5,\n",
       "            5, 15, 16,  9, 15, 28, 22,  5, 15,  5, 10, 16, 11,  7,  5, 22, 21,\n",
       "           21, 16,  5, 11, 11, 23, 17, 22,  9, 21, 15, 15, 21, 11,  5, 10, 14,\n",
       "           21, 12, 10,  5, 15,  5, 21,  8, 23,  5, 14, 19, 20, 25, 23,  8, 14,\n",
       "           19, 14,  5, 19,  8,  8, 23, 16, 11, 15, 13, 21, 25,  5, 26, 28,  8,\n",
       "            9, 20, 28,  8, 11, 17, 28,  5, 21, 25, 21,  7, 14, 22, 25, 25, 23,\n",
       "            8, 25, 28, 19, 11, 10,  5, 13,  5, 20, 15, 28, 22, 16,  5, 25, 13,\n",
       "           21, 23, 17, 15, 22, 28, 14,  9, 15, 16, 15,  5, 23, 20, 15, 21, 10,\n",
       "           26, 11, 11, 10, 25,  5, 15,  5, 28,  5, 25, 22, 21,  5,  5, 22, 21,\n",
       "           23, 14, 25,  5,  8, 15,  8, 21, 15,  5,  5, 13, 21, 21,  5, 14, 17,\n",
       "           10, 25, 15,  8, 14, 21, 22, 23, 19,  8, 25, 19,  9, 23, 15, 25,  5,\n",
       "           15]]], device='cuda:0'),\n",
       " [['1HTVLFGGKTRYQGSVQLPYGFYTKSQRVAFRFDNELQRWKAGGKAAQEVRMVEFGCADHEPEAWTKVAGAEGHERVQLCSDRRSHVFMDMTGGVNLVYAVVGANETTYSLYVGLEKVYAHADAWNTRYLRGAKQGVSLRDVYDDKRRTYNVSIHRLSVRYCKDATAAECKTVRFMAIAITNGRHHPRIHLHEYHIFNSWLWRNDLQRLAKAGSKISGKSEIKQIKLNPKARCKKCKWPEISNRSQGDPDKIIGIGYRGPYDA22XTSAGSLIGQIRSLERELRNGLFLDDLNMALDALAGRIVTEEYRAERELSSLLLTVIEDRVKEHSRLRDVPQEAALMELYSALAFMGCASRRMAGGTNSERLLRGAFKRHFALARDTAKPQVTDKPKAPDDTMGLIRVAWYDEQYDGNYARVRCKSVVTDVYPGFAIAQLYSMAVIRTNLSYKELMLATQLRFWGGFVALAYAVSRAASRTKVADLDRLAAIRRAKNFVLDKRSTPDVPETLVAL']])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg2.sample(torch.rand(1, 128).to('cuda'), num_samples=1, return_seq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab6ff919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmModel, EsmConfig\n",
    "from encoder.encoders import DistributionEncoder\n",
    "\n",
    "class ESMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, esm_model_name=\"facebook/esm2_t6_8M_UR50D\", output_dim=320, pooling=\"mean\", freeze=False):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(esm_model_name)\n",
    "        if freeze:\n",
    "            for p in self.esm.parameters(): p.requires_grad = False\n",
    "        self.pooling = pooling\n",
    "        h = self.esm.config.hidden_size\n",
    "        self.proj = nn.Linear(h, output_dim) if output_dim != h else nn.Identity()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.esm(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        if self.pooling == \"cls\":\n",
    "            pooled = x[:, 0]\n",
    "        elif self.pooling == \"mean\":\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            pooled = (x * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        elif self.pooling == \"max\":\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            x[mask == 0] = -1e9\n",
    "            pooled = x.max(1).values\n",
    "        else:\n",
    "            raise ValueError(\"bad pooling :(\")\n",
    "        return self.proj(pooled)\n",
    "\n",
    "class ProteinSetEncoder(nn.Module):\n",
    "    def __init__(self, esm_model_name=\"facebook/esm2_t6_8M_UR50D\", esm_dim=320, latent_dim=32, hidden_dim=128,\n",
    "                 pooling=\"cls\", freeze=False, dist_type=\"tx\", layers=2, heads=4):\n",
    "        super().__init__()\n",
    "        self.esm_extractor = ESMFeatureExtractor(esm_model_name, esm_dim, pooling, freeze)\n",
    "        if dist_type == \"tx\":\n",
    "            from encoder.encoders import DistributionEncoderTx as DE\n",
    "            self.dist = DE(esm_dim, latent_dim, hidden_dim, None, layers, heads)\n",
    "        elif dist_type == \"gnn\":\n",
    "            from encoder.encoders import DistributionEncoderGNN as DE\n",
    "            self.dist = DE(esm_dim, latent_dim, hidden_dim, None, layers, fc_layers=2)\n",
    "        elif dist_type == \"median_gnn\":\n",
    "            from encoder.encoders import DistributionEncoderMedianGNN as DE\n",
    "            self.dist = DE(esm_dim, latent_dim, hidden_dim, None, layers, fc_layers=2)\n",
    "        else:\n",
    "            raise ValueError(\"bad dist type :(\")\n",
    "\n",
    "    def forward(self, samples):\n",
    "        b, s = samples['input_ids'].shape[:2]\n",
    "        ids = samples['input_ids'].view(b * s, -1)\n",
    "        mask = samples['attention_mask'].view(b * s, -1)\n",
    "        feats = self.esm_extractor(ids, mask).view(b, s, -1)\n",
    "        return self.dist(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20cc7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "enc = ProteinSetEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6827a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ca84fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = '1MSKGEELF2'\n",
    "seq2 = '1TGVVPILV2'\n",
    "\n",
    "seqs= [seq2]*10\n",
    "\n",
    "samples = esm_tokenizer(seqs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "samples['input_ids'] = samples['input_ids'].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
