{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# add parent directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# initialize hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config: config\n",
      "dataset:\n",
      "  _target_: datasets.perturbseq_dataset.PerturbseqDataset\n",
      "  adata_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/essential_gene_knockouts_raw.h5ad\n",
      "  pert_embedding_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/pert_embeddings/GenePT.pt\n",
      "  control_pert: non-targeting\n",
      "  pert_key: gene\n",
      "  cell_key: cell_type\n",
      "  split_mode: iid\n",
      "  pca_components: ${experiment.pert_embedding_dim}\n",
      "  seed: 42\n",
      "  set_size: 100\n",
      "  data_shape:\n",
      "  - 11907\n",
      "  heldout_perts:\n",
      "  - SUPT5H\n",
      "  - ATF5\n",
      "  - SRSF1\n",
      "  - PSMA3\n",
      "  - SNRPD3\n",
      "  - RPL30\n",
      "  - EXOSC2\n",
      "  - CDC73\n",
      "  - NUP54\n",
      "  - PRIM2\n",
      "  - TSR2\n",
      "  - RPS11\n",
      "  - KPNB1\n",
      "  - NACA\n",
      "  - CSE1L\n",
      "  - SF3B2\n",
      "  - PHAX\n",
      "  - POLR2G\n",
      "  - RPS15A\n",
      "  - SF3A2\n",
      "  heldout_cell_types:\n",
      "  - k562\n",
      "encoder:\n",
      "  _target_: encoder.perturbseq_encoders.DistributionEncoderResNetPertPredictor\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  set_size: ${experiment.set_size}\n",
      "  pert_embedding_dim: ${experiment.pert_embedding_dim}\n",
      "  layers: ${experiment.layers}\n",
      "  fc_layers: 2\n",
      "model:\n",
      "  _target_: model.cvae.SimpleVAE\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  vae_latent_dim: ${experiment.latent_dim}\n",
      "  fc_layers: 4\n",
      "generator:\n",
      "  _target_: generator.cvae.CVAE\n",
      "  model: ${model}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  noise_shape: ${experiment.latent_dim}\n",
      "  beta: 1.0\n",
      "  kl_anneal_steps: 0\n",
      "  kl_anneal_start: 0.0\n",
      "optimizer:\n",
      "  _target_: torch.optim.Adam\n",
      "  _partial_: true\n",
      "  lr: ${experiment.lr}\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 0\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.CosineAnnealingLR\n",
      "  _partial_: true\n",
      "  T_max: 250\n",
      "  eta_min: 1.0e-05\n",
      "training:\n",
      "  _target_: training.Trainer\n",
      "  num_epochs: 200\n",
      "  log_interval: 100\n",
      "  save_interval: 20\n",
      "  eval_interval: 100\n",
      "  early_stopping: false\n",
      "  patience: 5\n",
      "  use_tqdm: false\n",
      "mixer:\n",
      "  _target_: types.NoneType\n",
      "wandb:\n",
      "  _target_: wandb_utils.setup_wandb\n",
      "  project: distribution-embeddings\n",
      "  entity: null\n",
      "  name: null\n",
      "  mode: online\n",
      "  tags: []\n",
      "  notes: null\n",
      "  group: null\n",
      "  save_code: true\n",
      "experiment:\n",
      "  wandb:\n",
      "    _target_: wandb_utils.setup_wandb\n",
      "    project: distribution-embeddings\n",
      "    entity: null\n",
      "    name: null\n",
      "    mode: online\n",
      "    tags: []\n",
      "    notes: null\n",
      "    group: null\n",
      "    save_code: true\n",
      "  name: essential_genes_exp\n",
      "  description: Essential genes analysis using distribution embeddings\n",
      "  latent_dim: 128\n",
      "  layers: 4\n",
      "  hidden_dim: 1024\n",
      "  noise_dim: 64\n",
      "  set_size: 100\n",
      "  batch_size: 96\n",
      "  lr: 0.0001\n",
      "  pert_embedding_dim: 16\n",
      "  mean_loss_weight: 1.0\n",
      "  pert_pred_loss_weight: 0.5\n",
      "loss:\n",
      "  _target_: loss.perturbseq.PerturbSeqLossManager\n",
      "  mean_loss_weight: ${experiment.mean_loss_weight}\n",
      "  pert_pred_loss_weight: ${experiment.pert_pred_loss_weight}\n",
      "  mask_context_prob: 0.0\n",
      "experiment_name: ${experiment.name}\n",
      "seed: 42\n",
      "device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hydra.initialize(config_path=\"../config\", version_base=\"1.1\")\n",
    "\n",
    "# Choose which config to load\n",
    "config_name = \"config\"  # Change this to use a different config\n",
    "print(f\"Loading config: {config_name}\")\n",
    "\n",
    "# Load the config\n",
    "cfg = hydra.compose(\n",
    "    config_name=config_name, \n",
    "    overrides=[\"experiment=essential_genes\", \"loss=perturbseq\"]\n",
    ")\n",
    "\n",
    "# Display the loaded config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with 16 components explains 0.3266 of variance\n",
      "Loaded 9220 sets (cell_type x gene combinations)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=cfg.experiment.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = hydra.utils.instantiate(cfg.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator (with model already instantiated)\n",
    "generator = hydra.utils.instantiate(cfg.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "model_parameters = list(encoder.parameters()) + list(generator.model.parameters())\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer)(params=model_parameters)\n",
    "scheduler = hydra.utils.instantiate(cfg.scheduler)(optimizer=optimizer)\n",
    "\n",
    "loss_manager = hydra.utils.instantiate(cfg.loss)\n",
    "\n",
    "# Create trainer\n",
    "trainer = hydra.utils.instantiate(cfg.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.use_tqdm = True\n",
    "output_dir, stats = trainer.train(\n",
    "    encoder=encoder,\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_manager=loss_manager,\n",
    "    output_dir=os.path.abspath('../outputs'),\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model.pt\t\t checkpoint_epoch_160.pt  checkpoint_epoch_40.pt\n",
      "checkpoint_epoch_100.pt  checkpoint_epoch_180.pt  checkpoint_epoch_60.pt\n",
      "checkpoint_epoch_120.pt  checkpoint_epoch_200.pt  checkpoint_epoch_80.pt\n",
      "checkpoint_epoch_140.pt  checkpoint_epoch_20.pt   config.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls /orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33'}\n",
    "config['config'] = OmegaConf.load('/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "\n",
    "def instantiate_and_load_model(config) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Instantiate and load a model from a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        model_path: The path to the model checkpoint file\n",
    "        device: The device to load the model on\n",
    "\n",
    "    Returns:\n",
    "        The loaded model\n",
    "    \"\"\"\n",
    "    cfg = config['config']\n",
    "    encoder = hydra.utils.instantiate(cfg.encoder)\n",
    "    generator = hydra.utils.instantiate(cfg.generator)\n",
    "\n",
    "    checkpoint = torch.load(config['dir'] + '/checkpoint_epoch_100.pt', weights_only=False)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    generator.model.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    return encoder, generator\n",
    "\n",
    "encoder, generator = instantiate_and_load_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def generate_set_mean_predictions(encoder, sets, X, ctrl_key, pert_keys):\n",
    "    encoder = encoder.to('cuda') \n",
    "\n",
    "    ctrl_X = torch.tensor(X[sets[ctrl_key]]).to('cuda')\n",
    "    pert_X = {k: torch.tensor(X[sets[k]]).to('cuda') for k in pert_keys}\n",
    "\n",
    "    ctrl_S = encoder(ctrl_X.unsqueeze(0))\n",
    "    \n",
    "    pert_S = {k: encoder(pert_X[k].unsqueeze(0)) for k in pert_keys}\n",
    "    pert_S_delta = {k: pert_S[k] - ctrl_S for k in pert_keys}\n",
    "\n",
    "    pert_S = torch.cat([pert_S[k] for k in pert_keys], dim=0)\n",
    "    pert_S_delta = torch.cat([pert_S_delta[k] for k in pert_keys], dim=0)\n",
    "\n",
    "    ctrl_X_mean = torch.mean(ctrl_X, dim=0)\n",
    "    pert_X_mean = {k: torch.mean(pert_X[k], dim=0) for k in pert_keys}\n",
    "    pert_X_mean = torch.cat([pert_X_mean[k].unsqueeze(0) for k in pert_keys], dim=0)\n",
    "    pert_X_delta = pert_X_mean - ctrl_X_mean.unsqueeze(0)\n",
    "\n",
    "    pert_X_delta_recon = encoder.mean_predictor(pert_S) - ctrl_X_mean\n",
    "    return ctrl_X_mean.cpu().detach().numpy(), ctrl_S.cpu().detach().numpy(), pert_X_delta.cpu().detach().numpy(), pert_S_delta.cpu().detach().numpy(), pert_X_delta_recon.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Calculate R² using Pearson correlation.\"\"\"\n",
    "    r = pearsonr(y_true, y_pred, axis=1)\n",
    "    return (r[0]**2).mean()\n",
    "\n",
    "# solve optimal linear predictor\n",
    "def solve_optimal_linear_predictor(Y, X, bias=True):\n",
    "    if bias:\n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "    if bias:\n",
    "        return beta[:-1], beta[-1]\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'k562'  \n",
    "ctrl_key = dataset.control_pert\n",
    "pert_keys = [k for k in dataset.sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "eval_pert_keys = [k for k in dataset.eval_sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctrl_X, ctrl_S, X_delta, S_delta, X_delta_recon = generate_set_mean_predictions(\n",
    "        encoder, dataset.sets[cell_type], dataset.X, ctrl_key, pert_keys\n",
    "    )\n",
    "    _, _, X_delta_eval, S_delta_eval, X_delta_recon_eval = generate_set_mean_predictions(\n",
    "        encoder, dataset.eval_sets[cell_type], dataset.X, ctrl_key, eval_pert_keys\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.953576576358714, 0.06776411691334215)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "beta, bias = solve_optimal_linear_predictor(X_delta, S_delta)\n",
    "X_delta_pred_full = S_delta_eval @ beta + bias\n",
    "r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Best CV score: 0.183\n",
      "Best alpha: 1.0\n",
      "Best CV score: 0.327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4888247643207899, 2.1124798343225892, 0.48821092, 2.1111455)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "Z = np.vstack([dataset.pert_embeddings[k] for k in pert_keys])\n",
    "Z_eval = np.vstack([dataset.pert_embeddings[k] for k in eval_pert_keys])\n",
    "\n",
    "# compute all interactions of Z\n",
    "Zi = np.einsum('bi,bj->bij', Z, Z).reshape(Z.shape[0], -1)\n",
    "Zi_eval = np.einsum('bi,bj->bij', Z_eval, Z_eval).reshape(Z_eval.shape[0], -1)\n",
    "\n",
    "\n",
    "# search over alpha\n",
    "# search over alpha values using cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "ridge = Ridge()\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(Zi, S_delta)\n",
    "\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "\n",
    "reg = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "# reg = KernelRidge(kernel='polynomial', degree=3, alpha=0.1)\n",
    "# reg = RandomForestRegressor()\n",
    "reg.fit(Zi, S_delta)\n",
    "S_delta_pred_eval_kr = reg.predict(Zi_eval).astype(np.float32)\n",
    "X_delta_pred_gde = S_delta_pred_eval_kr @ beta + bias\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(Zi, X_delta)\n",
    "\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# mean predict the delta\n",
    "reg = Ridge(alpha=0.1)# grid_search.best_params_['alpha'])\n",
    "reg.fit(Zi, X_delta)\n",
    "X_delta_pred_full = reg.predict(Zi_eval).astype(np.float32)\n",
    "\n",
    "r2_score(X_delta_eval, X_delta_pred_gde), mean_squared_error(X_delta_eval, X_delta_pred_gde), r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-types",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
