{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# add parent directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# initialize hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config: config\n",
      "dataset:\n",
      "  _target_: datasets.perturbseq_dataset.PerturbseqDataset\n",
      "  adata_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/essential_gene_knockouts_raw.h5ad\n",
      "  pert_embedding_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/pert_embeddings/GenePT.pt\n",
      "  control_pert: non-targeting\n",
      "  pert_key: gene\n",
      "  cell_key: cell_type\n",
      "  split_mode: iid\n",
      "  pca_components: ${experiment.pert_embedding_dim}\n",
      "  seed: 42\n",
      "  set_size: 100\n",
      "  data_shape:\n",
      "  - 11907\n",
      "  heldout_perts:\n",
      "  - SUPT5H\n",
      "  - ATF5\n",
      "  - SRSF1\n",
      "  - PSMA3\n",
      "  - SNRPD3\n",
      "  - RPL30\n",
      "  - EXOSC2\n",
      "  - CDC73\n",
      "  - NUP54\n",
      "  - PRIM2\n",
      "  - TSR2\n",
      "  - RPS11\n",
      "  - KPNB1\n",
      "  - NACA\n",
      "  - CSE1L\n",
      "  - SF3B2\n",
      "  - PHAX\n",
      "  - POLR2G\n",
      "  - RPS15A\n",
      "  - SF3A2\n",
      "  heldout_cell_types:\n",
      "  - k562\n",
      "encoder:\n",
      "  _target_: encoder.perturbseq_encoders.DistributionEncoderResNetPertPredictor\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  set_size: ${experiment.set_size}\n",
      "  pert_embedding_dim: ${experiment.pert_embedding_dim}\n",
      "  layers: ${experiment.layers}\n",
      "  fc_layers: 2\n",
      "model:\n",
      "  _target_: model.cvae.SimpleVAE\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  vae_latent_dim: ${experiment.latent_dim}\n",
      "  fc_layers: 4\n",
      "generator:\n",
      "  _target_: generator.cvae.CVAE\n",
      "  model: ${model}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  noise_shape: ${experiment.latent_dim}\n",
      "  beta: 1.0\n",
      "  kl_anneal_steps: 0\n",
      "  kl_anneal_start: 0.0\n",
      "optimizer:\n",
      "  _target_: torch.optim.Adam\n",
      "  _partial_: true\n",
      "  lr: ${experiment.lr}\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 0\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.ConstantLR\n",
      "  _partial_: true\n",
      "training:\n",
      "  _target_: training.Trainer\n",
      "  num_epochs: 100\n",
      "  log_interval: 100\n",
      "  save_interval: 20\n",
      "  eval_interval: 100\n",
      "  early_stopping: false\n",
      "  patience: 5\n",
      "  use_tqdm: false\n",
      "mixer:\n",
      "  _target_: types.NoneType\n",
      "wandb:\n",
      "  _target_: wandb_utils.setup_wandb\n",
      "  project: distribution-embeddings\n",
      "  entity: null\n",
      "  name: null\n",
      "  mode: online\n",
      "  tags: []\n",
      "  notes: null\n",
      "  group: null\n",
      "  save_code: true\n",
      "experiment:\n",
      "  wandb:\n",
      "    _target_: wandb_utils.setup_wandb\n",
      "    project: distribution-embeddings\n",
      "    entity: null\n",
      "    name: null\n",
      "    mode: online\n",
      "    tags: []\n",
      "    notes: null\n",
      "    group: null\n",
      "    save_code: true\n",
      "  name: essential_genes_exp\n",
      "  description: Essential genes analysis using distribution embeddings\n",
      "  latent_dim: 128\n",
      "  layers: 4\n",
      "  hidden_dim: 1024\n",
      "  noise_dim: 64\n",
      "  set_size: 100\n",
      "  batch_size: 96\n",
      "  lr: 5.0e-05\n",
      "  pert_embedding_dim: 3072\n",
      "  mean_loss_weight: 1.0\n",
      "  pert_pred_loss_weight: 0.5\n",
      "loss:\n",
      "  _target_: loss.perturbseq.PerturbSeqLossManager\n",
      "  mean_loss_weight: ${experiment.mean_loss_weight}\n",
      "  pert_pred_loss_weight: ${experiment.pert_pred_loss_weight}\n",
      "  mask_context_prob: 0.0\n",
      "experiment_name: ${experiment.name}\n",
      "seed: 42\n",
      "device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hydra.initialize(config_path=\"../config\", version_base=\"1.1\")\n",
    "\n",
    "# Choose which config to load\n",
    "config_name = \"config\"  # Change this to use a different config\n",
    "print(f\"Loading config: {config_name}\")\n",
    "\n",
    "# Load the config\n",
    "cfg = hydra.compose(\n",
    "    config_name=config_name, \n",
    "    overrides=[\"experiment=essential_genes\", \"loss=perturbseq\"]\n",
    ")\n",
    "\n",
    "# Display the loaded config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PCA applied, using 3072 components\n",
      "Loaded 9220 sets (cell_type x gene combinations)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=cfg.experiment.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = hydra.utils.instantiate(cfg.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator (with model already instantiated)\n",
    "generator = hydra.utils.instantiate(cfg.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "model_parameters = list(encoder.parameters()) + list(generator.model.parameters())\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer)(params=model_parameters)\n",
    "scheduler = hydra.utils.instantiate(cfg.scheduler)(optimizer=optimizer)\n",
    "\n",
    "loss_manager = hydra.utils.instantiate(cfg.loss)\n",
    "\n",
    "# Create trainer\n",
    "trainer = hydra.utils.instantiate(cfg.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar_experiments []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/101 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 438.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 150.69 MiB is free. Process 763708 has 75.74 GiB memory in use. Including non-PyTorch memory, this process has 3.20 GiB memory in use. Of the allocated memory 2.63 GiB is allocated by PyTorch, and 59.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39muse_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m output_dir, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../outputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/training.py:279\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, encoder, generator, dataloader, optimizer, loss_manager, scheduler, device, output_dir, config)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# Handle samples which can be either a tensor or a dictionary\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     loss, losses \u001b[38;5;241m=\u001b[39m \u001b[43mloss_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/loss/perturbseq.py:31\u001b[0m, in \u001b[0;36mPerturbSeqLossManager.loss\u001b[0;34m(self, encoder, generator, batch, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 31\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msamples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m latent \u001b[38;5;241m=\u001b[39m encoder(samples)  \u001b[38;5;66;03m# latent is num samples x num sets x latent dim\u001b[39;00m\n\u001b[1;32m     34\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mloss(samples\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39msamples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]), latent)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 438.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 150.69 MiB is free. Process 763708 has 75.74 GiB memory in use. Including non-PyTorch memory, this process has 3.20 GiB memory in use. Of the allocated memory 2.63 GiB is allocated by PyTorch, and 59.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.use_tqdm = True\n",
    "trainer.log_interval = 10\n",
    "output_dir, stats = trainer.train(\n",
    "    encoder=encoder,\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_manager=loss_manager,\n",
    "    output_dir=os.path.abspath('../outputs'),\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model.pt\t\t checkpoint_epoch_160.pt  checkpoint_epoch_40.pt\n",
      "checkpoint_epoch_100.pt  checkpoint_epoch_180.pt  checkpoint_epoch_60.pt\n",
      "checkpoint_epoch_120.pt  checkpoint_epoch_200.pt  checkpoint_epoch_80.pt\n",
      "checkpoint_epoch_140.pt  checkpoint_epoch_20.pt   config.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls /orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dir': '/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33'}\n",
    "config['config'] = OmegaConf.load('/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/outputs/essential_genes_exp_7febae6e4ed11221715d4828439f4f33/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "\n",
    "def instantiate_and_load_model(config) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Instantiate and load a model from a checkpoint file.\n",
    "    \n",
    "    Args:\n",
    "        model_path: The path to the model checkpoint file\n",
    "        device: The device to load the model on\n",
    "\n",
    "    Returns:\n",
    "        The loaded model\n",
    "    \"\"\"\n",
    "    cfg = config['config']\n",
    "    encoder = hydra.utils.instantiate(cfg.encoder)\n",
    "    generator = hydra.utils.instantiate(cfg.generator)\n",
    "\n",
    "    checkpoint = torch.load(config['dir'] + '/checkpoint_epoch_100.pt', weights_only=False)\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    generator.model.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    return encoder, generator\n",
    "\n",
    "encoder, generator = instantiate_and_load_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def generate_set_mean_predictions(encoder, sets, X, ctrl_key, pert_keys):\n",
    "    encoder = encoder.to('cuda') \n",
    "\n",
    "    ctrl_X = torch.tensor(X[sets[ctrl_key]]).to('cuda')\n",
    "    pert_X = {k: torch.tensor(X[sets[k]]).to('cuda') for k in pert_keys}\n",
    "\n",
    "    ctrl_S = encoder(ctrl_X.unsqueeze(0))\n",
    "    \n",
    "    pert_S = {k: encoder(pert_X[k].unsqueeze(0)) for k in pert_keys}\n",
    "    pert_S_delta = {k: pert_S[k] - ctrl_S for k in pert_keys}\n",
    "\n",
    "    pert_S = torch.cat([pert_S[k] for k in pert_keys], dim=0)\n",
    "    pert_S_delta = torch.cat([pert_S_delta[k] for k in pert_keys], dim=0)\n",
    "\n",
    "    ctrl_X_mean = torch.mean(ctrl_X, dim=0)\n",
    "    pert_X_mean = {k: torch.mean(pert_X[k], dim=0) for k in pert_keys}\n",
    "    pert_X_mean = torch.cat([pert_X_mean[k].unsqueeze(0) for k in pert_keys], dim=0)\n",
    "    pert_X_delta = pert_X_mean - ctrl_X_mean.unsqueeze(0)\n",
    "\n",
    "    pert_X_delta_recon = encoder.mean_predictor(pert_S) - ctrl_X_mean\n",
    "    return ctrl_X_mean.cpu().detach().numpy(), ctrl_S.cpu().detach().numpy(), pert_X_delta.cpu().detach().numpy(), pert_S_delta.cpu().detach().numpy(), pert_X_delta_recon.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Calculate R² using Pearson correlation.\"\"\"\n",
    "    r = pearsonr(y_true, y_pred, axis=1)\n",
    "    return (r[0]**2).mean()\n",
    "\n",
    "# solve optimal linear predictor\n",
    "def solve_optimal_linear_predictor(Y, X, bias=True):\n",
    "    if bias:\n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "    if bias:\n",
    "        return beta[:-1], beta[-1]\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'k562'  \n",
    "ctrl_key = dataset.control_pert\n",
    "pert_keys = [k for k in dataset.sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "eval_pert_keys = [k for k in dataset.eval_sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctrl_X, ctrl_S, X_delta, S_delta, X_delta_recon = generate_set_mean_predictions(\n",
    "        encoder, dataset.sets[cell_type], dataset.X, ctrl_key, pert_keys\n",
    "    )\n",
    "    _, _, X_delta_eval, S_delta_eval, X_delta_recon_eval = generate_set_mean_predictions(\n",
    "        encoder, dataset.eval_sets[cell_type], dataset.X, ctrl_key, eval_pert_keys\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9514674027501666, 0.08505320212344723)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "beta, bias = solve_optimal_linear_predictor(X_delta, S_delta)\n",
    "X_delta_pred_full = S_delta_eval @ beta + bias\n",
    "r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "Z = np.vstack([dataset.pert_embeddings[k] for k in pert_keys])\n",
    "Z_eval = np.vstack([dataset.pert_embeddings[k] for k in eval_pert_keys])\n",
    "\n",
    "# compute all interactions of Z\n",
    "Zi = np.einsum('bi,bj->bij', Z, Z).reshape(Z.shape[0], -1)\n",
    "Zi_eval = np.einsum('bi,bj->bij', Z_eval, Z_eval).reshape(Z_eval.shape[0], -1)\n",
    "\n",
    "\n",
    "\n",
    "reg = Ridge(alpha=1.)\n",
    "# reg = KernelRidge(kernel='polynomial', degree=3, alpha=0.1)\n",
    "# reg = RandomForestRegressor()\n",
    "reg.fit(Zi, S_delta)\n",
    "S_delta_pred_eval_kr = reg.predict(Zi_eval).astype(np.float32)\n",
    "X_delta_pred_gde = S_delta_pred_eval_kr @ beta + bias\n",
    "\n",
    "\n",
    "# mean predict the delta\n",
    "reg = Ridge(alpha=1.)# grid_search.best_params_['alpha'])\n",
    "reg.fit(Zi, X_delta)\n",
    "X_delta_pred_full = reg.predict(Zi_eval).astype(np.float32)\n",
    "\n",
    "r2_score(X_delta_eval, X_delta_pred_gde), mean_squared_error(X_delta_eval, X_delta_pred_gde), r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "Z = np.vstack([dataset.pert_embeddings[k] for k in pert_keys])\n",
    "Z_eval = np.vstack([dataset.pert_embeddings[k] for k in eval_pert_keys])\n",
    "\n",
    "# compute all interactions of Z\n",
    "Zi = np.einsum('bi,bj->bij', Z, Z).reshape(Z.shape[0], -1)\n",
    "Zi_eval = np.einsum('bi,bj->bij', Z_eval, Z_eval).reshape(Z_eval.shape[0], -1)\n",
    "\n",
    "\n",
    "# search over alpha\n",
    "# search over alpha values using cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "ridge = Ridge()\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(Zi, S_delta)\n",
    "\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "\n",
    "reg = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "# reg = KernelRidge(kernel='polynomial', degree=3, alpha=0.1)\n",
    "# reg = RandomForestRegressor()\n",
    "reg.fit(Zi, S_delta)\n",
    "S_delta_pred_eval_kr = reg.predict(Zi_eval).astype(np.float32)\n",
    "X_delta_pred_gde = S_delta_pred_eval_kr @ beta + bias\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(Zi, X_delta)\n",
    "\n",
    "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# mean predict the delta\n",
    "reg = Ridge(alpha=0.1)# grid_search.best_params_['alpha'])\n",
    "reg.fit(Zi, X_delta)\n",
    "X_delta_pred_full = reg.predict(Zi_eval).astype(np.float32)\n",
    "\n",
    "r2_score(X_delta_eval, X_delta_pred_gde), mean_squared_error(X_delta_eval, X_delta_pred_gde), r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-types",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
