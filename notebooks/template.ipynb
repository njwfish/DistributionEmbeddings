{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# add parent directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# initialize hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config: config\n",
      "dataset:\n",
      "  _target_: datasets.perturbseq_dataset.PerturbseqDataset\n",
      "  adata_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/essential_gene_knockouts_raw.h5ad\n",
      "  pert_embedding_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/pert_embeddings/GenePT.pt\n",
      "  control_pert: non-targeting\n",
      "  pert_key: gene\n",
      "  cell_key: cell_type\n",
      "  split_mode: iid\n",
      "  pca_components: ${experiment.pert_embedding_dim}\n",
      "  seed: 42\n",
      "  set_size: 100\n",
      "  data_shape:\n",
      "  - 11907\n",
      "  heldout_perts:\n",
      "  - SUPT5H\n",
      "  - ATF5\n",
      "  - SRSF1\n",
      "  - PSMA3\n",
      "  - SNRPD3\n",
      "  - RPL30\n",
      "  - EXOSC2\n",
      "  - CDC73\n",
      "  - NUP54\n",
      "  - PRIM2\n",
      "  - TSR2\n",
      "  - RPS11\n",
      "  - KPNB1\n",
      "  - NACA\n",
      "  - CSE1L\n",
      "  - SF3B2\n",
      "  - PHAX\n",
      "  - POLR2G\n",
      "  - RPS15A\n",
      "  - SF3A2\n",
      "  heldout_cell_types:\n",
      "  - k562\n",
      "encoder:\n",
      "  _target_: encoder.perturbseq_encoders.DistributionEncoderResNetPertPredictor\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  set_size: ${experiment.set_size}\n",
      "  pert_embedding_dim: ${experiment.pert_embedding_dim}\n",
      "  layers: 2\n",
      "  fc_layers: 2\n",
      "model:\n",
      "  _target_: model.cvae.SimpleVAE\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  vae_latent_dim: ${experiment.latent_dim}\n",
      "  fc_layers: 4\n",
      "generator:\n",
      "  _target_: generator.cvae.CVAE\n",
      "  model: ${model}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  noise_shape: ${experiment.latent_dim}\n",
      "  beta: 1.0\n",
      "  kl_anneal_steps: 0\n",
      "  kl_anneal_start: 0.0\n",
      "optimizer:\n",
      "  _target_: torch.optim.Adam\n",
      "  _partial_: true\n",
      "  lr: ${experiment.lr}\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 0\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.CosineAnnealingLR\n",
      "  _partial_: true\n",
      "  T_max: 250\n",
      "  eta_min: 1.0e-05\n",
      "training:\n",
      "  _target_: training.Trainer\n",
      "  num_epochs: 200\n",
      "  log_interval: 100\n",
      "  save_interval: 20\n",
      "  eval_interval: 100\n",
      "  early_stopping: false\n",
      "  patience: 5\n",
      "  use_tqdm: false\n",
      "mixer:\n",
      "  _target_: types.NoneType\n",
      "wandb:\n",
      "  _target_: wandb_utils.setup_wandb\n",
      "  project: distribution-embeddings\n",
      "  entity: null\n",
      "  name: null\n",
      "  mode: online\n",
      "  tags: []\n",
      "  notes: null\n",
      "  group: null\n",
      "  save_code: true\n",
      "experiment:\n",
      "  wandb:\n",
      "    _target_: wandb_utils.setup_wandb\n",
      "    project: distribution-embeddings\n",
      "    entity: null\n",
      "    name: null\n",
      "    mode: online\n",
      "    tags: []\n",
      "    notes: null\n",
      "    group: null\n",
      "    save_code: true\n",
      "  name: essential_genes_exp\n",
      "  description: Essential genes analysis using distribution embeddings\n",
      "  latent_dim: 128\n",
      "  layers: 4\n",
      "  hidden_dim: 256\n",
      "  noise_dim: 64\n",
      "  set_size: 100\n",
      "  batch_size: 96\n",
      "  lr: 0.0001\n",
      "  pert_embedding_dim: 16\n",
      "  mean_loss_weight: 1.0\n",
      "  pert_pred_loss_weight: 0.5\n",
      "loss:\n",
      "  _target_: loss.perturbseq.PerturbSeqLossManager\n",
      "  mean_loss_weight: ${experiment.mean_loss_weight}\n",
      "  pert_pred_loss_weight: ${experiment.pert_pred_loss_weight}\n",
      "  mask_context_prob: 0.0\n",
      "experiment_name: ${experiment.name}\n",
      "seed: 42\n",
      "device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hydra.initialize(config_path=\"../config\", version_base=\"1.1\")\n",
    "\n",
    "# Choose which config to load\n",
    "config_name = \"config\"  # Change this to use a different config\n",
    "print(f\"Loading config: {config_name}\")\n",
    "\n",
    "# Load the config\n",
    "cfg = hydra.compose(\n",
    "    config_name=config_name, \n",
    "    overrides=[\"experiment=essential_genes\", \"loss=perturbseq\"]\n",
    ")\n",
    "\n",
    "# Display the loaded config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with 16 components explains 0.3266 of variance\n",
      "Loaded 9220 sets (cell_type x gene combinations)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=cfg.experiment.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = hydra.utils.instantiate(cfg.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator (with model already instantiated)\n",
    "generator = hydra.utils.instantiate(cfg.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "model_parameters = list(encoder.parameters()) + list(generator.model.parameters())\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer)(params=model_parameters)\n",
    "scheduler = hydra.utils.instantiate(cfg.scheduler)(optimizer=optimizer)\n",
    "\n",
    "loss_manager = hydra.utils.instantiate(cfg.loss)\n",
    "\n",
    "# Create trainer\n",
    "trainer = hydra.utils.instantiate(cfg.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_loss_weight': 1.0,\n",
       " 'pert_pred_loss_weight': 0.5,\n",
       " 'mask_context_prob': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_manager.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar_experiments []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 101/101 [00:36<00:00,  2.78it/s, loss=22.667112]\n",
      "Epoch 2/200: 100%|██████████| 101/101 [00:40<00:00,  2.47it/s, loss=13.782756]\n",
      "Epoch 3/200: 100%|██████████| 101/101 [00:39<00:00,  2.53it/s, loss=12.846636]\n",
      "Epoch 4/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=10.414200]\n",
      "Epoch 5/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=12.282897]\n",
      "Epoch 6/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=11.091025]\n",
      "Epoch 7/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=10.304071]\n",
      "Epoch 8/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=9.508952] \n",
      "Epoch 9/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=9.722234]\n",
      "Epoch 10/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=8.673017]\n",
      "Epoch 11/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=8.057650]\n",
      "Epoch 12/200: 100%|██████████| 101/101 [00:43<00:00,  2.35it/s, loss=7.333703]\n",
      "Epoch 13/200: 100%|██████████| 101/101 [01:02<00:00,  1.63it/s, loss=9.850603]\n",
      "Epoch 14/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=8.358371]\n",
      "Epoch 15/200: 100%|██████████| 101/101 [00:44<00:00,  2.29it/s, loss=7.567790]\n",
      "Epoch 16/200: 100%|██████████| 101/101 [00:43<00:00,  2.33it/s, loss=8.071248]\n",
      "Epoch 17/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=8.740870]\n",
      "Epoch 18/200: 100%|██████████| 101/101 [00:42<00:00,  2.38it/s, loss=7.350140] \n",
      "Epoch 19/200: 100%|██████████| 101/101 [00:41<00:00,  2.44it/s, loss=7.937321]\n",
      "Epoch 20/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=6.858327]\n",
      "Epoch 21/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=7.606313]\n",
      "Epoch 22/200: 100%|██████████| 101/101 [00:41<00:00,  2.44it/s, loss=8.504443]\n",
      "Epoch 23/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=7.112463]\n",
      "Epoch 24/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=6.559834]\n",
      "Epoch 25/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=6.698214]\n",
      "Epoch 26/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=7.446722]\n",
      "Epoch 27/200: 100%|██████████| 101/101 [00:41<00:00,  2.44it/s, loss=6.898898]\n",
      "Epoch 28/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=6.131093]\n",
      "Epoch 29/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=7.464874]\n",
      "Epoch 30/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=7.353591]\n",
      "Epoch 31/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=7.442766]\n",
      "Epoch 32/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=7.595881]\n",
      "Epoch 33/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=6.064254]\n",
      "Epoch 34/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=6.077032]\n",
      "Epoch 35/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.899099]\n",
      "Epoch 36/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.901707]\n",
      "Epoch 37/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.448113]\n",
      "Epoch 38/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.500568]\n",
      "Epoch 39/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=7.968579]\n",
      "Epoch 40/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.524218]\n",
      "Epoch 41/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=6.735566]\n",
      "Epoch 42/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.929479]\n",
      "Epoch 43/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.759932]\n",
      "Epoch 44/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.717326]\n",
      "Epoch 45/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.713453]\n",
      "Epoch 46/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.694312]\n",
      "Epoch 47/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.355928]\n",
      "Epoch 48/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.727918]\n",
      "Epoch 49/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.147448]\n",
      "Epoch 50/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.201116]\n",
      "Epoch 51/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.181142]\n",
      "Epoch 52/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.177944]\n",
      "Epoch 53/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=6.052377]\n",
      "Epoch 54/200: 100%|██████████| 101/101 [00:41<00:00,  2.40it/s, loss=5.069542]\n",
      "Epoch 55/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.063062]\n",
      "Epoch 56/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=4.985696]\n",
      "Epoch 57/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=5.655715]\n",
      "Epoch 58/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=5.216974]\n",
      "Epoch 59/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=4.668917]\n",
      "Epoch 60/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=4.604935]\n",
      "Epoch 61/200: 100%|██████████| 101/101 [00:42<00:00,  2.36it/s, loss=4.959971]\n",
      "Epoch 62/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=4.443876]\n",
      "Epoch 63/200: 100%|██████████| 101/101 [00:42<00:00,  2.38it/s, loss=4.544691]\n",
      "Epoch 64/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=4.599499]\n",
      "Epoch 65/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=4.515225]\n",
      "Epoch 66/200: 100%|██████████| 101/101 [00:42<00:00,  2.38it/s, loss=5.108874]\n",
      "Epoch 67/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=4.709645]\n",
      "Epoch 68/200: 100%|██████████| 101/101 [00:42<00:00,  2.38it/s, loss=4.632973]\n",
      "Epoch 69/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=5.295529]\n",
      "Epoch 70/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=5.140278]\n",
      "Epoch 71/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=4.422294]\n",
      "Epoch 72/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.979710]\n",
      "Epoch 73/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.311598]\n",
      "Epoch 74/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.309241]\n",
      "Epoch 75/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.576481]\n",
      "Epoch 76/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.922148]\n",
      "Epoch 77/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.145258]\n",
      "Epoch 78/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.553695]\n",
      "Epoch 79/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.955901]\n",
      "Epoch 80/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.678911]\n",
      "Epoch 81/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.423809]\n",
      "Epoch 82/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.003386]\n",
      "Epoch 83/200: 100%|██████████| 101/101 [00:44<00:00,  2.29it/s, loss=4.197903]\n",
      "Epoch 84/200: 100%|██████████| 101/101 [00:42<00:00,  2.40it/s, loss=4.890521]\n",
      "Epoch 85/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.291050]\n",
      "Epoch 86/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=4.770886]\n",
      "Epoch 87/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.875349]\n",
      "Epoch 88/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=5.789021]\n",
      "Epoch 89/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.351832]\n",
      "Epoch 90/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=4.766242]\n",
      "Epoch 91/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.556118]\n",
      "Epoch 92/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.411781]\n",
      "Epoch 93/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.428628]\n",
      "Epoch 94/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.825501]\n",
      "Epoch 95/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=4.949309]\n",
      "Epoch 96/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.980545]\n",
      "Epoch 97/200: 100%|██████████| 101/101 [00:42<00:00,  2.39it/s, loss=4.260417]\n",
      "Epoch 98/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.618313]\n",
      "Epoch 99/200: 100%|██████████| 101/101 [00:41<00:00,  2.41it/s, loss=5.244156]\n",
      "Epoch 100/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.913709]\n",
      "Epoch 101/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.631824]\n",
      "Epoch 102/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=4.572620]\n",
      "Epoch 103/200: 100%|██████████| 101/101 [00:41<00:00,  2.42it/s, loss=4.175859]\n",
      "Epoch 104/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.026449]\n",
      "Epoch 105/200: 100%|██████████| 101/101 [00:41<00:00,  2.43it/s, loss=5.010657]\n",
      "Epoch 106/200:  66%|██████▋   | 67/101 [00:28<00:14,  2.38it/s, loss=4.525578]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39muse_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_dir, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../outputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/training.py:277\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, encoder, generator, dataloader, optimizer, loss_manager, scheduler, device, output_dir, config)\u001b[0m\n\u001b[1;32m    274\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m dataloader\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# Handle samples which can be either a tensor or a dictionary\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     loss, losses \u001b[38;5;241m=\u001b[39m loss_manager\u001b[38;5;241m.\u001b[39mloss(encoder, generator, batch, device)\n\u001b[1;32m    281\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/cell-types/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cell-types/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cell-types/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cell-types/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cell-types/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/datasets/perturbseq_dataset.py:141\u001b[0m, in \u001b[0;36mPerturbseqDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Get the expression vectors for the sampled cells\u001b[39;00m\n\u001b[1;32m    138\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[sampled_indices]\n\u001b[1;32m    140\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_type\u001b[39m\u001b[38;5;124m'\u001b[39m: cell_type,\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperturbation\u001b[39m\u001b[38;5;124m'\u001b[39m: perturbation,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_control\u001b[39m\u001b[38;5;124m'\u001b[39m: perturbation \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol_pert,\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctrl_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[ctrl_indices], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    146\u001b[0m }\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Add perturbation embedding if available\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpert_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.use_tqdm = True\n",
    "output_dir, stats = trainer.train(\n",
    "    encoder=encoder,\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_manager=loss_manager,\n",
    "    output_dir=os.path.abspath('../outputs'),\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def generate_set_mean_predictions(encoder, sets, X, ctrl_key, pert_keys):\n",
    "    encoder = encoder.to('cuda') \n",
    "\n",
    "    ctrl_X = torch.tensor(X[sets[ctrl_key]]).to('cuda')\n",
    "    pert_X = {k: torch.tensor(X[sets[k]]).to('cuda') for k in pert_keys}\n",
    "\n",
    "    ctrl_S = encoder(ctrl_X.unsqueeze(0))\n",
    "    \n",
    "    pert_S = {k: encoder(pert_X[k].unsqueeze(0)) for k in pert_keys}\n",
    "    pert_S_delta = {k: pert_S[k] - ctrl_S for k in pert_keys}\n",
    "\n",
    "    pert_S = torch.cat([pert_S[k] for k in pert_keys], dim=0)\n",
    "    pert_S_delta = torch.cat([pert_S_delta[k] for k in pert_keys], dim=0)\n",
    "\n",
    "    ctrl_X_mean = torch.mean(ctrl_X, dim=0)\n",
    "    pert_X_mean = {k: torch.mean(pert_X[k], dim=0) for k in pert_keys}\n",
    "    pert_X_mean = torch.cat([pert_X_mean[k].unsqueeze(0) for k in pert_keys], dim=0)\n",
    "    pert_X_delta = pert_X_mean - ctrl_X_mean.unsqueeze(0)\n",
    "\n",
    "    pert_X_delta_recon = encoder.mean_predictor(pert_S) - ctrl_X_mean\n",
    "    return ctrl_X_mean.cpu().detach().numpy(), ctrl_S.cpu().detach().numpy(), pert_X_delta.cpu().detach().numpy(), pert_S_delta.cpu().detach().numpy(), pert_X_delta_recon.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Calculate R² using Pearson correlation.\"\"\"\n",
    "    r = pearsonr(y_true, y_pred, axis=1)\n",
    "    return (r[0]**2).mean()\n",
    "\n",
    "# solve optimal linear predictor\n",
    "def solve_optimal_linear_predictor(Y, X, bias=True):\n",
    "    if bias:\n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "    if bias:\n",
    "        return beta[:-1], beta[-1]\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'k562'  \n",
    "ctrl_key = dataset.control_pert\n",
    "pert_keys = [k for k in dataset.sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "eval_pert_keys = [k for k in dataset.eval_sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctrl_X, ctrl_S, X_delta, S_delta, X_delta_recon = generate_set_mean_predictions(\n",
    "        encoder, dataset.sets[cell_type], dataset.X, ctrl_key, pert_keys\n",
    "    )\n",
    "    _, _, X_delta_eval, S_delta_eval, X_delta_recon_eval = generate_set_mean_predictions(\n",
    "        encoder, dataset.eval_sets[cell_type], dataset.X, ctrl_key, eval_pert_keys\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.951844455631036, 0.06866181666741762)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "beta, bias = solve_optimal_linear_predictor(X_delta, S_delta)\n",
    "X_delta_pred_full = S_delta_eval @ beta + bias\n",
    "r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.489849635092809, 2.2195290778759107, 0.4904286, 2.2179322)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "Z = np.vstack([dataset.pert_embeddings[k] for k in pert_keys])\n",
    "Z_eval = np.vstack([dataset.pert_embeddings[k] for k in eval_pert_keys])\n",
    "\n",
    "# compute all interactions of Z\n",
    "Zi = np.einsum('bi,bj->bij', Z, Z).reshape(Z.shape[0], -1)\n",
    "Zi_eval = np.einsum('bi,bj->bij', Z_eval, Z_eval).reshape(Z_eval.shape[0], -1)\n",
    "\n",
    "\n",
    "reg = Ridge()\n",
    "\n",
    "# reg = KernelRidge(kernel='polynomial', degree=3, alpha=0.1)\n",
    "# reg = RandomForestRegressor()\n",
    "reg.fit(Zi, S_delta)\n",
    "S_delta_pred_eval_kr = reg.predict(Zi_eval).astype(np.float32)\n",
    "\n",
    "X_delta_pred_gde = S_delta_pred_eval_kr @ beta + bias\n",
    "\n",
    "# mean predict the delta\n",
    "reg = Ridge()\n",
    "reg.fit(Zi, X_delta)\n",
    "X_delta_pred_full = reg.predict(Zi_eval).astype(np.float32)\n",
    "\n",
    "r2_score(X_delta_eval, X_delta_pred_gde), mean_squared_error(X_delta_eval, X_delta_pred_gde), r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-types",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
