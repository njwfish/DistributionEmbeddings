{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# add parent directory to path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# initialize hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config: config\n",
      "dataset:\n",
      "  _target_: datasets.perturbseq_dataset.PerturbseqDataset\n",
      "  adata_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/essential_gene_knockouts_raw.h5ad\n",
      "  pert_embedding_path: /orcd/data/omarabu/001/Omnicell_datasets/essential_gene_knockouts_raw/pert_embeddings/GenePT.pt\n",
      "  control_pert: non-targeting\n",
      "  pert_key: gene\n",
      "  cell_key: cell_type\n",
      "  split_mode: iid\n",
      "  pca_components: ${experiment.pert_embedding_dim}\n",
      "  seed: 42\n",
      "  set_size: 100\n",
      "  data_shape:\n",
      "  - 11907\n",
      "  heldout_perts:\n",
      "  - SUPT5H\n",
      "  - ATF5\n",
      "  - SRSF1\n",
      "  - PSMA3\n",
      "  - SNRPD3\n",
      "  - RPL30\n",
      "  - EXOSC2\n",
      "  - CDC73\n",
      "  - NUP54\n",
      "  - PRIM2\n",
      "  - TSR2\n",
      "  - RPS11\n",
      "  - KPNB1\n",
      "  - NACA\n",
      "  - CSE1L\n",
      "  - SF3B2\n",
      "  - PHAX\n",
      "  - POLR2G\n",
      "  - RPS15A\n",
      "  - SF3A2\n",
      "  heldout_cell_types:\n",
      "  - k562\n",
      "encoder:\n",
      "  _target_: encoder.perturbseq_encoders.DistributionEncoderResNetPertPredictor\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  set_size: ${experiment.set_size}\n",
      "  pert_embedding_dim: ${experiment.pert_embedding_dim}\n",
      "  layers: 2\n",
      "  fc_layers: 2\n",
      "model:\n",
      "  _target_: model.cvae.SimpleVAE\n",
      "  in_dim: ${dataset.data_shape[0]}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  hidden_dim: ${experiment.hidden_dim}\n",
      "  vae_latent_dim: ${experiment.latent_dim}\n",
      "  fc_layers: 4\n",
      "generator:\n",
      "  _target_: generator.cvae.CVAE\n",
      "  model: ${model}\n",
      "  latent_dim: ${experiment.latent_dim}\n",
      "  noise_shape: ${experiment.latent_dim}\n",
      "  beta: 1.0\n",
      "  kl_anneal_steps: 0\n",
      "  kl_anneal_start: 0.0\n",
      "optimizer:\n",
      "  _target_: torch.optim.Adam\n",
      "  _partial_: true\n",
      "  lr: ${experiment.lr}\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  eps: 1.0e-08\n",
      "  weight_decay: 0\n",
      "scheduler:\n",
      "  _target_: torch.optim.lr_scheduler.CosineAnnealingLR\n",
      "  _partial_: true\n",
      "  T_max: 250\n",
      "  eta_min: 1.0e-05\n",
      "training:\n",
      "  _target_: training.Trainer\n",
      "  num_epochs: 100\n",
      "  log_interval: 100\n",
      "  save_interval: 20\n",
      "  eval_interval: 100\n",
      "  early_stopping: false\n",
      "  patience: 5\n",
      "  use_tqdm: false\n",
      "mixer:\n",
      "  _target_: types.NoneType\n",
      "wandb:\n",
      "  _target_: wandb_utils.setup_wandb\n",
      "  project: distribution-embeddings\n",
      "  entity: null\n",
      "  name: null\n",
      "  mode: online\n",
      "  tags: []\n",
      "  notes: null\n",
      "  group: null\n",
      "  save_code: true\n",
      "experiment:\n",
      "  wandb:\n",
      "    _target_: wandb_utils.setup_wandb\n",
      "    project: distribution-embeddings\n",
      "    entity: null\n",
      "    name: null\n",
      "    mode: online\n",
      "    tags: []\n",
      "    notes: null\n",
      "    group: null\n",
      "    save_code: true\n",
      "  name: essential_genes_exp\n",
      "  description: Essential genes analysis using distribution embeddings\n",
      "  latent_dim: 128\n",
      "  layers: 4\n",
      "  hidden_dim: 256\n",
      "  noise_dim: 64\n",
      "  set_size: 100\n",
      "  batch_size: 96\n",
      "  lr: 0.0001\n",
      "  pert_embedding_dim: 16\n",
      "  mean_loss_weight: 1.0\n",
      "  pert_pred_loss_weight: 0.5\n",
      "loss:\n",
      "  _target_: loss.perturbseq.PerturbSeqLossManager\n",
      "  mean_loss_weight: ${experiment.mean_loss_weight}\n",
      "  pert_pred_loss_weight: ${experiment.pert_pred_loss_weight}\n",
      "  mask_context_prob: 0.0\n",
      "experiment_name: ${experiment.name}\n",
      "seed: 42\n",
      "device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hydra.initialize(config_path=\"../config\", version_base=\"1.1\")\n",
    "\n",
    "# Choose which config to load\n",
    "config_name = \"config\"  # Change this to use a different config\n",
    "print(f\"Loading config: {config_name}\")\n",
    "\n",
    "# Load the config\n",
    "cfg = hydra.compose(\n",
    "    config_name=config_name, \n",
    "    overrides=[\"experiment=essential_genes\", \"loss=perturbseq\"]\n",
    ")\n",
    "\n",
    "# Display the loaded config\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with 16 components explains 0.3266 of variance\n",
      "Loaded 9220 sets (cell_type x gene combinations)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = hydra.utils.instantiate(cfg.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = hydra.utils.instantiate(cfg.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator (with model already instantiated)\n",
    "generator = hydra.utils.instantiate(cfg.generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "model_parameters = list(encoder.parameters()) + list(generator.model.parameters())\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer)(params=model_parameters)\n",
    "scheduler = hydra.utils.instantiate(cfg.scheduler)(optimizer=optimizer)\n",
    "\n",
    "loss_manager = hydra.utils.instantiate(cfg.loss)\n",
    "\n",
    "# Create trainer\n",
    "trainer = hydra.utils.instantiate(cfg.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<loss.perturbseq.PerturbSeqLossManager at 0x1506bdb6bf40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/1209 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 36.69 MiB is free. Process 281029 has 78.18 GiB memory in use. Including non-PyTorch memory, this process has 882.00 MiB memory in use. Of the allocated memory 203.48 MiB is allocated by PyTorch, and 8.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39muse_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_dir, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../outputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/training.py:148\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, encoder, generator, dataloader, optimizer, loss_manager, scheduler, device, output_dir, config)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Handle samples which can be either a tensor or a dictionary\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     loss, losses \u001b[38;5;241m=\u001b[39m \u001b[43mloss_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/loss/perturbseq.py:35\u001b[0m, in \u001b[0;36mPerturbSeqLossManager.loss\u001b[0;34m(self, encoder, generator, batch, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m latent \u001b[38;5;241m=\u001b[39m encoder(samples)  \u001b[38;5;66;03m# latent is num samples x num sets x latent dim\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(latent\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 35\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\n\u001b[1;32m     37\u001b[0m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m recon_loss\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/generator/cvae.py:83\u001b[0m, in \u001b[0;36mCVAE.loss\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/generator/cvae.py:62\u001b[0m, in \u001b[0;36mCVAE.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     59\u001b[0m c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Get mean and logvar from encoder part of the model\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Sample latent using reparameterization\u001b[39;00m\n\u001b[1;32m     65\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n",
      "File \u001b[0;32m/orcd/data/omarabu/001/njwfish/DistributionEmbeddings/model/cvae.py:18\u001b[0m, in \u001b[0;36mSimpleVAE.encode\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Concatenate input and context\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_hidden(h)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Get mu and logvar\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 36.69 MiB is free. Process 281029 has 78.18 GiB memory in use. Including non-PyTorch memory, this process has 882.00 MiB memory in use. Of the allocated memory 203.48 MiB is allocated by PyTorch, and 8.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.use_tqdm = True\n",
    "output_dir, stats = trainer.train(\n",
    "    encoder=encoder,\n",
    "    generator=generator,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_manager=loss_manager,\n",
    "    output_dir=os.path.abspath('../outputs'),\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def generate_set_mean_predictions(encoder, sets, X, ctrl_key, pert_keys):\n",
    "    encoder = encoder.to('cuda') \n",
    "\n",
    "    ctrl_X = torch.tensor(X[sets[ctrl_key]]).to('cuda')\n",
    "    pert_X = {k: torch.tensor(X[sets[k]]).to('cuda') for k in pert_keys}\n",
    "\n",
    "    ctrl_S = encoder(ctrl_X.unsqueeze(0))\n",
    "    \n",
    "    pert_S = {k: encoder(pert_X[k].unsqueeze(0)) for k in pert_keys}\n",
    "    pert_S_delta = {k: pert_S[k] - ctrl_S for k in pert_keys}\n",
    "\n",
    "    pert_S = torch.cat([pert_S[k] for k in pert_keys], dim=0)\n",
    "    pert_S_delta = torch.cat([pert_S_delta[k] for k in pert_keys], dim=0)\n",
    "\n",
    "    ctrl_X_mean = torch.mean(ctrl_X, dim=0)\n",
    "    pert_X_mean = {k: torch.mean(pert_X[k], dim=0) for k in pert_keys}\n",
    "    pert_X_mean = torch.cat([pert_X_mean[k].unsqueeze(0) for k in pert_keys], dim=0)\n",
    "    pert_X_delta = pert_X_mean - ctrl_X_mean.unsqueeze(0)\n",
    "\n",
    "    pert_X_delta_recon = encoder.mean_predictor(pert_S) - ctrl_X_mean\n",
    "    return ctrl_X_mean.cpu().detach().numpy(), ctrl_S.cpu().detach().numpy(), pert_X_delta.cpu().detach().numpy(), pert_S_delta.cpu().detach().numpy(), pert_X_delta_recon.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Calculate RÂ² using Pearson correlation.\"\"\"\n",
    "    r = pearsonr(y_true, y_pred, axis=1)\n",
    "    return (r[0]**2).mean()\n",
    "\n",
    "# solve optimal linear predictor\n",
    "def solve_optimal_linear_predictor(Y, X, bias=True):\n",
    "    if bias:\n",
    "        X = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "    if bias:\n",
    "        return beta[:-1], beta[-1]\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'k562'  \n",
    "ctrl_key = dataset.control_pert\n",
    "pert_keys = [k for k in dataset.sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "eval_pert_keys = [k for k in dataset.eval_sets[cell_type] if k != ctrl_key and k in dataset.pert_embeddings]\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctrl_X, ctrl_S, X_delta, S_delta, X_delta_recon = generate_set_mean_predictions(\n",
    "        encoder, dataset.sets[cell_type], dataset.X, ctrl_key, pert_keys\n",
    "    )\n",
    "    _, _, X_delta_eval, S_delta_eval, X_delta_recon_eval = generate_set_mean_predictions(\n",
    "        encoder, dataset.eval_sets[cell_type], dataset.X, ctrl_key, eval_pert_keys\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.942771173643357, 0.10214500899233878)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "beta, bias = solve_optimal_linear_predictor(X_delta, S_delta)\n",
    "X_delta_pred_full = S_delta_eval @ beta + bias\n",
    "r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43110166928780036, 2.2951323160105894, 0.4315087, 2.2952776)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression   \n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import numpy as np\n",
    "\n",
    "Z = np.vstack([dataset.pert_embeddings[k] for k in pert_keys])\n",
    "Z_eval = np.vstack([dataset.pert_embeddings[k] for k in eval_pert_keys])\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "# reg = KernelRidge(kernel='polynomial', degree=2, alpha=0.1)\n",
    "reg.fit(Z, S_delta)\n",
    "S_delta_pred_eval_kr = reg.predict(Z_eval).astype(np.float32)\n",
    "\n",
    "X_delta_pred_gde = S_delta_pred_eval_kr @ beta + bias\n",
    "\n",
    "# mean predict the delta\n",
    "reg = LinearRegression()\n",
    "# reg = KernelRidge(kernel='polynomial', degree=2, alpha=0.1)\n",
    "reg.fit(Z, X_delta)\n",
    "X_delta_pred_full = reg.predict(Z_eval).astype(np.float32)\n",
    "\n",
    "r2_score(X_delta_eval, X_delta_pred_gde), mean_squared_error(X_delta_eval, X_delta_pred_gde), r2_score(X_delta_eval, X_delta_pred_full), mean_squared_error(X_delta_eval, X_delta_pred_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-types",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
