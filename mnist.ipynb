{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from dist_ae import SetAutoencoderGNN, SetAutoencoderTx, train_w_stop\n",
    "from losses import sliced_wasserstein_distance\n",
    "\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mnist_sets(n_sets, set_size):\n",
    "    # load mnist dataset\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    \n",
    "    # group indices by label\n",
    "    label_to_indices = {i: torch.where(torch.tensor(dataset.targets) == i)[0] for i in range(10)}\n",
    "    \n",
    "    sets = []\n",
    "    metadata = []\n",
    "    for _ in range(n_sets):\n",
    "        label = torch.randint(0, 10, (1,)).item()  # random label\n",
    "        indices = torch.randperm(len(label_to_indices[label]))[:set_size]  # sample set_size images\n",
    "        sets.append(dataset.data[label_to_indices[label][indices]].float() / 255.0)\n",
    "        metadata.append(label)  # store label info\n",
    "    \n",
    "    return torch.stack(sets).float(), metadata\n",
    "\n",
    "\n",
    "mnist_sets, _ = sample_mnist_sets(N_sets, set_size)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_sets, batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.sum(b[0][0] - b.reshape(-1, 1, 28, 28)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
       "          0.5647, 0.8941, 0.9961, 0.6510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2275, 0.8078,\n",
       "          0.9922, 0.8235, 0.6431, 0.9922, 0.0706, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.5412, 0.9529, 0.5725,\n",
       "          0.0588, 0.0196, 0.0627, 0.9176, 0.2941, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.5569, 0.9608, 0.4039, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.8784, 0.3490, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.2980, 0.9843, 0.5569, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0549, 0.9098, 0.3294, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0510, 0.8941, 0.7804, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.2118, 0.9608, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.6392, 0.4980, 0.9922, 0.1961, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.7098, 0.5451, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.7412, 0.9961, 0.9922, 0.4588, 0.0314, 0.0000, 0.0000,\n",
       "          0.0000, 0.3529, 0.9176, 0.0941, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.5804, 0.9922, 0.9922, 0.8824, 0.4196, 0.0549,\n",
       "          0.1137, 0.8941, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0510, 0.8902, 0.3725, 0.8392, 0.9922, 0.7922,\n",
       "          0.7686, 0.8706, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.4000, 0.0353, 0.0000, 0.5529, 1.0000,\n",
       "          0.9961, 0.1451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3647, 0.9961,\n",
       "          0.9922, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2902, 0.9529, 0.5098,\n",
       "          0.9922, 0.3490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.9373, 0.4275, 0.0510,\n",
       "          0.8902, 0.3490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.6941, 0.5765, 0.0157, 0.2039,\n",
       "          0.9922, 0.3490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.3255, 0.8863, 0.0000, 0.0000, 0.4157,\n",
       "          0.9961, 0.2314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0314, 0.8588, 0.4902, 0.0000, 0.0000, 0.8078,\n",
       "          0.8000, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1176, 0.9922, 0.2627, 0.0118, 0.4941, 0.9882,\n",
       "          0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0275, 0.8588, 0.6353, 0.4078, 0.9922, 0.6000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.1647, 0.7882, 0.9412, 0.6118, 0.0353,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/y7jr3zbd39b6fvmxt9lwswl00000gn/T/ipykernel_75811/2935216422.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_to_indices = {i: torch.where(torch.tensor(dataset.targets) == i)[0] for i in range(10)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for b in train_loader:\n",
    "    print(b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/y7jr3zbd39b6fvmxt9lwswl00000gn/T/ipykernel_75811/319726545.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_to_indices = {i: torch.where(torch.tensor(dataset.targets) == i)[0] for i in range(10)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.2806 | Val Loss: 0.2181\n",
      "Epoch 2 | Train Loss: 0.1832 | Val Loss: 0.1611\n",
      "Epoch 3 | Train Loss: 0.1586 | Val Loss: 0.1531\n",
      "Epoch 4 | Train Loss: 0.1509 | Val Loss: 0.1467\n",
      "Epoch 5 | Train Loss: 0.1472 | Val Loss: 0.1453\n",
      "Epoch 6 | Train Loss: 0.1460 | Val Loss: 0.1443\n",
      "Epoch 7 | Train Loss: 0.1438 | Val Loss: 0.1403\n",
      "Epoch 8 | Train Loss: 0.1390 | Val Loss: 0.1366\n",
      "Epoch 9 | Train Loss: 0.1356 | Val Loss: 0.1338\n",
      "Epoch 10 | Train Loss: 0.1340 | Val Loss: 0.1332\n",
      "Epoch 11 | Train Loss: 0.1327 | Val Loss: 0.1329\n",
      "Epoch 12 | Train Loss: 0.1317 | Val Loss: 0.1306\n",
      "Epoch 13 | Train Loss: 0.1283 | Val Loss: 0.1267\n",
      "Epoch 14 | Train Loss: 0.1244 | Val Loss: 0.1230\n",
      "Epoch 15 | Train Loss: 0.1223 | Val Loss: 0.1213\n",
      "Epoch 16 | Train Loss: 0.1208 | Val Loss: 0.1203\n",
      "Epoch 17 | Train Loss: 0.1199 | Val Loss: 0.1196\n",
      "Epoch 18 | Train Loss: 0.1195 | Val Loss: 0.1190\n",
      "Epoch 19 | Train Loss: 0.1193 | Val Loss: 0.1183\n",
      "Epoch 20 | Train Loss: 0.1168 | Val Loss: 0.1161\n",
      "Epoch 21 | Train Loss: 0.1154 | Val Loss: 0.1142\n",
      "Epoch 22 | Train Loss: 0.1142 | Val Loss: 0.1137\n",
      "Epoch 23 | Train Loss: 0.1131 | Val Loss: 0.1124\n",
      "Epoch 24 | Train Loss: 0.1118 | Val Loss: 0.1111\n",
      "Epoch 25 | Train Loss: 0.1108 | Val Loss: 0.1103\n",
      "Epoch 26 | Train Loss: 0.1096 | Val Loss: 0.1093\n",
      "Epoch 27 | Train Loss: 0.1089 | Val Loss: 0.1091\n",
      "Epoch 28 | Train Loss: 0.1082 | Val Loss: 0.1082\n",
      "Epoch 29 | Train Loss: 0.1077 | Val Loss: 0.1072\n",
      "Epoch 30 | Train Loss: 0.1067 | Val Loss: 0.1061\n",
      "Epoch 31 | Train Loss: 0.1055 | Val Loss: 0.1050\n",
      "Epoch 32 | Train Loss: 0.1042 | Val Loss: 0.1036\n",
      "Epoch 33 | Train Loss: 0.1027 | Val Loss: 0.1018\n",
      "Epoch 34 | Train Loss: 0.1003 | Val Loss: 0.0986\n",
      "Epoch 35 | Train Loss: 0.0976 | Val Loss: 0.0962\n",
      "Epoch 36 | Train Loss: 0.0949 | Val Loss: 0.0931\n",
      "Epoch 37 | Train Loss: 0.0929 | Val Loss: 0.0919\n",
      "Epoch 38 | Train Loss: 0.0917 | Val Loss: 0.0911\n",
      "Epoch 39 | Train Loss: 0.0911 | Val Loss: 0.0907\n",
      "Epoch 40 | Train Loss: 0.0906 | Val Loss: 0.0900\n",
      "Epoch 41 | Train Loss: 0.0900 | Val Loss: 0.0890\n",
      "Epoch 42 | Train Loss: 0.0882 | Val Loss: 0.0873\n",
      "Epoch 43 | Train Loss: 0.0867 | Val Loss: 0.0858\n",
      "Epoch 44 | Train Loss: 0.0859 | Val Loss: 0.0858\n",
      "Epoch 45 | Train Loss: 0.0862 | Val Loss: 0.0857\n",
      "Epoch 46 | Train Loss: 0.0856 | Val Loss: 0.0851\n",
      "Epoch 47 | Train Loss: 0.0849 | Val Loss: 0.0847\n",
      "Epoch 48 | Train Loss: 0.0848 | Val Loss: 0.0850\n",
      "Epoch 49 | Train Loss: 0.0850 | Val Loss: 0.0840\n",
      "Epoch 50 | Train Loss: 0.0838 | Val Loss: 0.0837\n",
      "Epoch 51 | Train Loss: 0.0836 | Val Loss: 0.0831\n",
      "Epoch 52 | Train Loss: 0.0832 | Val Loss: 0.0826\n",
      "Epoch 53 | Train Loss: 0.0824 | Val Loss: 0.0820\n",
      "Epoch 54 | Train Loss: 0.0817 | Val Loss: 0.0811\n",
      "Epoch 55 | Train Loss: 0.0813 | Val Loss: 0.0807\n",
      "Epoch 56 | Train Loss: 0.0808 | Val Loss: 0.0800\n",
      "Epoch 57 | Train Loss: 0.0804 | Val Loss: 0.0794\n",
      "Epoch 58 | Train Loss: 0.0797 | Val Loss: 0.0795\n",
      "Epoch 59 | Train Loss: 0.0795 | Val Loss: 0.0788\n",
      "Epoch 60 | Train Loss: 0.0790 | Val Loss: 0.0789\n",
      "Epoch 61 | Train Loss: 0.0788 | Val Loss: 0.0787\n",
      "Epoch 62 | Train Loss: 0.0784 | Val Loss: 0.0785\n",
      "Epoch 63 | Train Loss: 0.0783 | Val Loss: 0.0776\n",
      "Epoch 64 | Train Loss: 0.0768 | Val Loss: 0.0762\n",
      "Epoch 65 | Train Loss: 0.0759 | Val Loss: 0.0756\n",
      "Epoch 66 | Train Loss: 0.0754 | Val Loss: 0.0753\n",
      "Epoch 67 | Train Loss: 0.0753 | Val Loss: 0.0754\n",
      "Epoch 68 | Train Loss: 0.0757 | Val Loss: 0.0758\n",
      "Epoch 69 | Train Loss: 0.0758 | Val Loss: 0.0752\n",
      "Epoch 70 | Train Loss: 0.0752 | Val Loss: 0.0749\n",
      "Epoch 71 | Train Loss: 0.0752 | Val Loss: 0.0751\n",
      "Epoch 72 | Train Loss: 0.0749 | Val Loss: 0.0747\n",
      "Epoch 73 | Train Loss: 0.0747 | Val Loss: 0.0746\n",
      "Epoch 74 | Train Loss: 0.0748 | Val Loss: 0.0742\n",
      "Epoch 75 | Train Loss: 0.0745 | Val Loss: 0.0744\n",
      "Epoch 76 | Train Loss: 0.0745 | Val Loss: 0.0744\n",
      "Epoch 77 | Train Loss: 0.0744 | Val Loss: 0.0741\n",
      "Epoch 78 | Train Loss: 0.0744 | Val Loss: 0.0745\n",
      "Epoch 79 | Train Loss: 0.0743 | Val Loss: 0.0740\n",
      "Epoch 80 | Train Loss: 0.0740 | Val Loss: 0.0740\n",
      "Epoch 81 | Train Loss: 0.0740 | Val Loss: 0.0740\n",
      "Epoch 82 | Train Loss: 0.0738 | Val Loss: 0.0735\n",
      "Epoch 83 | Train Loss: 0.0736 | Val Loss: 0.0736\n",
      "Epoch 84 | Train Loss: 0.0737 | Val Loss: 0.0734\n",
      "Epoch 85 | Train Loss: 0.0736 | Val Loss: 0.0735\n",
      "Epoch 86 | Train Loss: 0.0736 | Val Loss: 0.0734\n",
      "Epoch 87 | Train Loss: 0.0734 | Val Loss: 0.0731\n",
      "Epoch 88 | Train Loss: 0.0729 | Val Loss: 0.0729\n",
      "Epoch 89 | Train Loss: 0.0733 | Val Loss: 0.0730\n",
      "Epoch 90 | Train Loss: 0.0731 | Val Loss: 0.0728\n",
      "Epoch 91 | Train Loss: 0.0730 | Val Loss: 0.0736\n",
      "Epoch 92 | Train Loss: 0.0731 | Val Loss: 0.0727\n",
      "Epoch 93 | Train Loss: 0.0724 | Val Loss: 0.0725\n",
      "Epoch 94 | Train Loss: 0.0728 | Val Loss: 0.0725\n",
      "Epoch 95 | Train Loss: 0.0725 | Val Loss: 0.0723\n",
      "Epoch 96 | Train Loss: 0.0722 | Val Loss: 0.0718\n",
      "Epoch 97 | Train Loss: 0.0723 | Val Loss: 0.0718\n",
      "Epoch 98 | Train Loss: 0.0720 | Val Loss: 0.0717\n",
      "Epoch 99 | Train Loss: 0.0718 | Val Loss: 0.0719\n",
      "Epoch 100 | Train Loss: 0.0718 | Val Loss: 0.0715\n",
      "Epoch 101 | Train Loss: 0.0717 | Val Loss: 0.0717\n",
      "Epoch 102 | Train Loss: 0.0716 | Val Loss: 0.0711\n",
      "Epoch 103 | Train Loss: 0.0715 | Val Loss: 0.0714\n",
      "Epoch 104 | Train Loss: 0.0713 | Val Loss: 0.0708\n",
      "Epoch 105 | Train Loss: 0.0712 | Val Loss: 0.0716\n",
      "Epoch 106 | Train Loss: 0.0712 | Val Loss: 0.0712\n",
      "Epoch 107 | Train Loss: 0.0709 | Val Loss: 0.0705\n",
      "Epoch 108 | Train Loss: 0.0700 | Val Loss: 0.0695\n",
      "Epoch 109 | Train Loss: 0.0696 | Val Loss: 0.0691\n",
      "Epoch 110 | Train Loss: 0.0692 | Val Loss: 0.0692\n",
      "Epoch 111 | Train Loss: 0.0693 | Val Loss: 0.0694\n",
      "Epoch 112 | Train Loss: 0.0693 | Val Loss: 0.0693\n",
      "Epoch 113 | Train Loss: 0.0690 | Val Loss: 0.0687\n",
      "Epoch 114 | Train Loss: 0.0688 | Val Loss: 0.0689\n",
      "Epoch 115 | Train Loss: 0.0685 | Val Loss: 0.0682\n",
      "Epoch 116 | Train Loss: 0.0686 | Val Loss: 0.0685\n",
      "Epoch 117 | Train Loss: 0.0683 | Val Loss: 0.0684\n",
      "Epoch 118 | Train Loss: 0.0683 | Val Loss: 0.0681\n",
      "Epoch 119 | Train Loss: 0.0684 | Val Loss: 0.0678\n",
      "Epoch 120 | Train Loss: 0.0684 | Val Loss: 0.0680\n",
      "Epoch 121 | Train Loss: 0.0682 | Val Loss: 0.0679\n",
      "Epoch 122 | Train Loss: 0.0678 | Val Loss: 0.0681\n",
      "Epoch 123 | Train Loss: 0.0682 | Val Loss: 0.0679\n",
      "Epoch 124 | Train Loss: 0.0682 | Val Loss: 0.0677\n",
      "Epoch 125 | Train Loss: 0.0681 | Val Loss: 0.0678\n",
      "Epoch 126 | Train Loss: 0.0680 | Val Loss: 0.0675\n",
      "Epoch 127 | Train Loss: 0.0676 | Val Loss: 0.0675\n",
      "Epoch 128 | Train Loss: 0.0675 | Val Loss: 0.0675\n",
      "Epoch 129 | Train Loss: 0.0676 | Val Loss: 0.0674\n",
      "Epoch 130 | Train Loss: 0.0678 | Val Loss: 0.0674\n",
      "Epoch 131 | Train Loss: 0.0673 | Val Loss: 0.0677\n",
      "Epoch 132 | Train Loss: 0.0676 | Val Loss: 0.0675\n",
      "Epoch 133 | Train Loss: 0.0675 | Val Loss: 0.0671\n",
      "Epoch 134 | Train Loss: 0.0674 | Val Loss: 0.0672\n",
      "Epoch 135 | Train Loss: 0.0672 | Val Loss: 0.0670\n",
      "Epoch 136 | Train Loss: 0.0675 | Val Loss: 0.0673\n",
      "Epoch 137 | Train Loss: 0.0670 | Val Loss: 0.0659\n",
      "Epoch 138 | Train Loss: 0.0661 | Val Loss: 0.0657\n",
      "Epoch 139 | Train Loss: 0.0659 | Val Loss: 0.0658\n",
      "Epoch 140 | Train Loss: 0.0657 | Val Loss: 0.0657\n",
      "Epoch 141 | Train Loss: 0.0658 | Val Loss: 0.0659\n",
      "Epoch 142 | Train Loss: 0.0657 | Val Loss: 0.0659\n",
      "Epoch 143 | Train Loss: 0.0656 | Val Loss: 0.0656\n",
      "Epoch 144 | Train Loss: 0.0654 | Val Loss: 0.0657\n",
      "Epoch 145 | Train Loss: 0.0657 | Val Loss: 0.0655\n",
      "Epoch 146 | Train Loss: 0.0656 | Val Loss: 0.0651\n",
      "Epoch 147 | Train Loss: 0.0650 | Val Loss: 0.0647\n",
      "Epoch 148 | Train Loss: 0.0646 | Val Loss: 0.0644\n",
      "Epoch 149 | Train Loss: 0.0644 | Val Loss: 0.0646\n",
      "Epoch 150 | Train Loss: 0.0644 | Val Loss: 0.0643\n",
      "Epoch 151 | Train Loss: 0.0643 | Val Loss: 0.0643\n",
      "Epoch 152 | Train Loss: 0.0643 | Val Loss: 0.0643\n",
      "Epoch 153 | Train Loss: 0.0643 | Val Loss: 0.0644\n",
      "Epoch 154 | Train Loss: 0.0643 | Val Loss: 0.0642\n",
      "Epoch 155 | Train Loss: 0.0644 | Val Loss: 0.0639\n",
      "Epoch 156 | Train Loss: 0.0643 | Val Loss: 0.0646\n",
      "Epoch 157 | Train Loss: 0.0645 | Val Loss: 0.0639\n",
      "Epoch 158 | Train Loss: 0.0643 | Val Loss: 0.0637\n",
      "Epoch 159 | Train Loss: 0.0641 | Val Loss: 0.0641\n",
      "Epoch 160 | Train Loss: 0.0640 | Val Loss: 0.0637\n",
      "Epoch 161 | Train Loss: 0.0638 | Val Loss: 0.0642\n",
      "Epoch 162 | Train Loss: 0.0640 | Val Loss: 0.0640\n",
      "Epoch 163 | Train Loss: 0.0640 | Val Loss: 0.0639\n",
      "Epoch 164 | Train Loss: 0.0641 | Val Loss: 0.0637\n",
      "Epoch 165 | Train Loss: 0.0638 | Val Loss: 0.0636\n",
      "Epoch 166 | Train Loss: 0.0635 | Val Loss: 0.0638\n",
      "Epoch 167 | Train Loss: 0.0636 | Val Loss: 0.0635\n",
      "Epoch 168 | Train Loss: 0.0636 | Val Loss: 0.0635\n",
      "Epoch 169 | Train Loss: 0.0636 | Val Loss: 0.0633\n",
      "Epoch 170 | Train Loss: 0.0637 | Val Loss: 0.0634\n",
      "Epoch 171 | Train Loss: 0.0633 | Val Loss: 0.0638\n",
      "Epoch 172 | Train Loss: 0.0637 | Val Loss: 0.0634\n",
      "Epoch 173 | Train Loss: 0.0636 | Val Loss: 0.0635\n",
      "Epoch 174 | Train Loss: 0.0632 | Val Loss: 0.0632\n",
      "Epoch 175 | Train Loss: 0.0634 | Val Loss: 0.0629\n",
      "Epoch 176 | Train Loss: 0.0630 | Val Loss: 0.0631\n",
      "Epoch 177 | Train Loss: 0.0633 | Val Loss: 0.0631\n",
      "Epoch 178 | Train Loss: 0.0631 | Val Loss: 0.0631\n",
      "Epoch 179 | Train Loss: 0.0629 | Val Loss: 0.0629\n",
      "Epoch 180 | Train Loss: 0.0630 | Val Loss: 0.0629\n",
      "Epoch 181 | Train Loss: 0.0628 | Val Loss: 0.0630\n",
      "Epoch 182 | Train Loss: 0.0630 | Val Loss: 0.0629\n",
      "Epoch 183 | Train Loss: 0.0629 | Val Loss: 0.0630\n",
      "Epoch 184 | Train Loss: 0.0634 | Val Loss: 0.0637\n",
      "Epoch 185 | Train Loss: 0.0635 | Val Loss: 0.0629\n",
      "Epoch 186 | Train Loss: 0.0630 | Val Loss: 0.0630\n",
      "Epoch 187 | Train Loss: 0.0627 | Val Loss: 0.0624\n",
      "Epoch 188 | Train Loss: 0.0624 | Val Loss: 0.0624\n",
      "Epoch 189 | Train Loss: 0.0628 | Val Loss: 0.0627\n",
      "Epoch 190 | Train Loss: 0.0626 | Val Loss: 0.0626\n",
      "Epoch 191 | Train Loss: 0.0625 | Val Loss: 0.0629\n",
      "Epoch 192 | Train Loss: 0.0629 | Val Loss: 0.0624\n",
      "Epoch 193 | Train Loss: 0.0624 | Val Loss: 0.0621\n",
      "Epoch 194 | Train Loss: 0.0622 | Val Loss: 0.0620\n",
      "Epoch 195 | Train Loss: 0.0622 | Val Loss: 0.0619\n",
      "Epoch 196 | Train Loss: 0.0621 | Val Loss: 0.0624\n",
      "Epoch 197 | Train Loss: 0.0623 | Val Loss: 0.0625\n",
      "Epoch 198 | Train Loss: 0.0620 | Val Loss: 0.0624\n",
      "Epoch 199 | Train Loss: 0.0624 | Val Loss: 0.0619\n",
      "Epoch 200 | Train Loss: 0.0619 | Val Loss: 0.0619\n"
     ]
    }
   ],
   "source": [
    "set_size = 100\n",
    "N_sets = 10**3\n",
    "\n",
    "mnist_sets, _ = sample_mnist_sets(N_sets, set_size)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_sets, \n",
    "                                           batch_size=32, shuffle=True)\n",
    "\n",
    "mnist_sets, _ = sample_mnist_sets(N_sets, set_size)\n",
    "val_loader = torch.utils.data.DataLoader(mnist_sets, \n",
    "                                           batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(X, Y):\n",
    "    return torch.vmap(sliced_wasserstein_distance, randomness='different')(X, Y).mean()\n",
    "\n",
    "dist_ae = SetAutoencoderGNN(mnist_sets.shape[2], 16, 64, set_size)\n",
    "optimizer = torch.optim.Adam(dist_ae.parameters(), lr=5e-4)\n",
    "\n",
    "dist_ae = train_w_stop(dist_ae, optimizer, train_loader, val_loader, \n",
    "                       loss_fn, max_epochs=200, patience=10, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/y7jr3zbd39b6fvmxt9lwswl00000gn/T/ipykernel_75811/319726545.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_to_indices = {i: torch.where(torch.tensor(dataset.targets) == i)[0] for i in range(10)}\n"
     ]
    }
   ],
   "source": [
    "test_sets, metadata = sample_mnist_sets(1000, set_size)\n",
    "with torch.no_grad():\n",
    "    z, rec = dist_ae((test_sets).float().to('mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "umap = UMAP(n_components=2)\n",
    "vis = umap.fit_transform(z.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(2,2), dpi=150)\n",
    "sns.scatterplot(x=vis[:, 0], y=vis[:, 1], hue=metadata, s=3, alpha=0.7,\n",
    "                palette='tab10', legend=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_means(mnist_sets, n_components=16):\n",
    "    n_sets, set_size, dims = mnist_sets.shape\n",
    "    flat_data = mnist_sets.view(-1, dims)  # flatten sets into a single batch\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(flat_data)  # get principal components\n",
    "\n",
    "    pcs = pcs.reshape(n_sets, set_size, -1)  # restore set structure\n",
    "    return pcs.mean(axis=1)  # mean over each set\n",
    "\n",
    "pca_means_res = pca_means(test_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokulg/.conda/envs/metaMI/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vis = umap.fit_transform(pca_means_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(2,2), dpi=150)\n",
    "sns.scatterplot(x=vis[:, 0], y=vis[:, 1], hue=metadata, s=5, alpha=0.7,\n",
    "                palette='tab10', legend=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
